 2/1: x = 10
 2/2: x
 2/3: mylist = ["a", "b", "c"]
 2/4: mylist
 2/5: mylist.append("d")
 2/6: mylist
 2/7: print(mylist)
 2/8: mylist[2:]
 2/9: mylist[:2]
2/10: mylist = []
 3/1:
for i in range(4):
    print i
 3/2:
for i in range(4):
    print (i)
 3/3:
for i in range(4, 8):
    print (i)
 3/4:
for i in range(4, 8,2):
    print (i)
 3/5:
i = numbers
While i > 0:
    a = input("How many Numbers?  ")
 3/6:
i = numbers
While i == range(i:i+1)
    a = input("How many Numbers?  ")
 3/7:
i = number
While a == range(i:i+1):
    i = input("How many Numbers?  ")
 3/8:
i = number
While i in range(n)
    n = input("How many Numbers?  ")
 3/9:
i = number
for i in range(n)
    n = input("How many Numbers?  ")
3/10:
i = number
for i in range(n):
    n = input("How many Numbers?  ")
3/11:
i = number
for i in range(number):
    n = input("How many Numbers?  ")
3/12:
i = number
for i in range(number):
    n = input("How many Numbers?  ")
3/13: print("How many Numbers?")
3/14: print input("How many Numbers?")
3/15: print("How many Numbers?")
3/16: i = input("How many Numbers?  ")
3/17:
While i >= 0:
    print(i)
3/18:
While i >= 0:
    print(range[0:i])
3/19:
While i == range[0:i]
    print(i)
3/20:
While x == range[0:i]
    print(i)
3/21:
While i == range[0:x]
    print(i)
3/22:
While x < int(i)
    print(x)
3/23:
i = input("How many Numbers?  ")
While x < int(i):
    print(x)
    x = x+1
3/24:
i = int(input("How many Numbers?  "))
While x < int(i):
    print(x)
    x = x+1
3/25:
x = "y"
While x == "y":
    num = int(input("How many Numbers?  "))
    print (num)
    for i in range(num):
        print(i)
    x = input("Continue?:  ")
3/26:
random = "y"
While random == "y":
    num = int(input("How many Numbers?  "))
    print (num)
    for i in range(num):
        print(i)
    x = input("Continue?:  ")
3/27:
random = "y"
While random == "y":
    num = int(input("How many Numbers?  "))
    for i in range(num):
        print(i)
    x = input("Continue?:  ")
3/28:
run = "y"
While run == "y":
    num = int(input("How many Numbers?  "))
    for i in range(num):
        print(i)
    x = input("Continue?:  ")
3/29: filepath = "../Resources/input.txt"
3/30:
with open(filepath, "r") as textfile:
    lines = textfile.read()
    print(lines)
3/31:
with open(filepath, "r") as textfile:
    lines = textfile.read()
    print(type(lines))
3/32: import random
3/33:
for x in range(5):
    print(random.randint(1, 10))
 4/1:
import os
import csv
 4/2:
import os
import csv
3/34:
import os
import csv
from pathlib import path
filepath = Path("../Resources/netflix_ratings.csv")
csv_file = os.path.join("folder_name", "file.csv")
data_output = os.path.join("folder_name", "data.csv")
with open(data_output, "w", newline="") as csvfile:
   writer = csv.writer(csvfile)
3/35:
output_path = Path("./output.csv)
                ")
3/36: output_path = Path("./output.csv")
 5/1: print (a)
 5/2: print (a)
 5/3: x
 5/4: x = 10
 5/5: x
 5/6: a = 3
 5/7: a
 5/8: x
 5/9: tear = ["abc", "def","ghi"]
5/10: tear
5/11: tear.append('jkl')
5/12: tear
5/13: tear[0,1]
5/14: tear[0:1]
5/15: tear[0:4]
5/16: tear[2:]
5/17: x = []
 6/1:
print("Hello User!")
name = input("What is your name?  ")
 6/2:
print("Hello User!")
name = input("What is your name?  ")
print("Hello" + name + "!")
 6/3:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
 6/4:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = input("What is your age?  ")
if age <=10 then:
    print("Aww, you are just a baby")
else
    print("ah, a well travelled soul you are!")
 6/5:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = input("What is your age?  ")
if age <=10 then
    print("Aww, you are just a baby")
else
    print("ah, a well travelled soul you are!")
 6/6:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = input("What is your age?  ")
if age <=10:
    print("Aww, you are just a baby")
else
    print("ah, a well travelled soul you are!")
 6/7:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = input("What is your age?  ")
if age <=10:
    print("Aww, you are just a baby")
else:
    print("ah, a well travelled soul you are!")
 6/8:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = int(input("What is your age?  "))
if age <=10:
    print("Aww, you are just a baby")
else:
    print("ah, a well travelled soul you are!")
 6/9:
print("Hello User!")
name = input("What is your name?  ")
print("Hello " + name + "!")
age = int(input("What is your age?  "))
if age <=10:
    print("Aww, you are just a baby")
else:
    print("ah, a well travelled soul you are!")
6/10:
for x in range(5)
x
6/11:
for x in range(5):
x
6/12:
for x in range(5):
    x
6/13:
for x in range(5):
    print x
6/14:
for x in range(5):
    print (x)
6/15:
for x in range(5,10):
    print (x)
6/16:
for x in range(25, 35):
    print (x)
6/17:
stationery = ["pen","paper","scissor","notebook","bag","calculator"]
for i in range(stationery):
    print(i)
6/18:
stationery = ["pen","paper","scissor","notebook","bag","calculator"]
for i in stationery:
    print(i)
6/19:
x = "Yes"
while x == "Yes":
    print("Whee! Merry-Go-Rounds are great!")
    x = input("Would you like to go on the Merry-Go-Round again? ")
6/20:
x = "Yes"
while x == "Yes":
    print("Whee! Merry-Go-Rounds are great!")
    x = input("Would you like to go on the Merry-Go-Round again? ")
6/21:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print (i)
6/22:
candyList = ["Snickers", "Kit Kat", "Sour Patch Kids", "Juicy Fruit", "Swedish Fish",
             "Skittles", "Hershey Bar", "Skittles", "Starbursts", "M&Ms"]

# The amount of candy the user will be allowed to choose
allowance = 5

# The list used to store all of the candies selected inside of
candyCart = []

for candy in candyList:
    print("[" + str(candyList.index(candy)) + "]" + candy)
6/23:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ candies_name.index(i)+"]")
6/24:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]")
6/25:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]" + i)
6/26:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]" + i)
allowance = 5
candycart=[]
for x in range(allowance):
    choose = input("what candies do you want to take home?   ")
    candycart.append(candies_name[int(choose))
6/27:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]" + i)
allowance = 5
candycart=[]
for x in range(allowance):
    choose = input("what candies do you want to take home?   ")
    candycart.append(candies_name[int(choose)]
6/28:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]" + i)
allowance = 5
candycart=[]
for x in range(allowance):
    choose = input("what candies do you want to take home?   ")
    candycart.append(candies_name[int(choose)])
6/29:
candies_name = ["snickers", "oreos", "gumballs", "hershey's", "M&M's", "Skittles", "Swedish fish", "Starburst", "kit-kat", "Juicy fruit"]
for i in candies_name:
    print ("["+ str(candies_name.index(i))+"]" + i)
allowance = 5
candycart=[]
for x in range(allowance):
    choose = input("what candies do you want to take home?   ")
    candycart.append(candies_name[int(choose)]) 
    print("I brought home with me...")
for candy in candycart:
    print(i)
 7/1:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak]
print (---------------------------------------------------------------------------------------------------------)
print (pies)
 7/2:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
print (---------------------------------------------------------------------------------------------------------)
print (pies)
 7/3:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
print ("---------------------------------------------------------------------------------------------------------")
print (pies)
 7/4:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
shopping = "y"
While shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
 7/5:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
shopping = "y"
while shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
 8/1:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
pie_purchase = []
shopping = "y"
while shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
pie_choice = input("Which pie do you choose?   ")
pie_purchase.append(pie_choice)
print("------------------------------------------------------------------------")
print("Great! We'll have that " + pie_list[int(pie_choice) - 1] + " right out for you.")
 9/1:
# Initial variable to track shopping status
shopping = 'y'

# List to track pie purchases
pie_purchases = []

# Pie List
pie_list = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun",
            "Blueberry", "Buko", "Burek", "Tamale", "Steak"]

# Display initial message
print("Welcome to the House of Pies! Here are our pies:")

# While we are still shopping...
while shopping == "y":

    # Show pie selection prompt
    print("---------------------------------------------------------------------")
    print("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee, " +
          " (5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek, " +
          " (9) Tamale, (10) Steak ")

    pie_choice = input("Which would you like? ")

    # Add pie to the pie list
    pie_purchases.append(pie_choice)

    print("------------------------------------------------------------------------")

    # Inform the customer of the pie purchase
    print("Great! We'll have that " + pie_list[int(pie_choice) - 1] + " right out for you.")

    # Provide exit option
    shopping = input("Would you like to make another purchase: (y)es or (n)o? ")

# Once the pie list is complete
print("------------------------------------------------------------------------")
print("You purchased a total of " + str(len(pie_purchases)) + ".")
 9/2:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
pie_purchase = []
shopping = "y"
while shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
pie_choice = input("Which pie do you choose?   ")
pie_purchase.append(pie_choice)
print("------------------------------------------------------------------------")
print("Great! We'll have that " + pies[int(pie_choice) - 1] + " right out for you.")
shopping = input("Would you like to make another purchase: (y)es or (n)o?   ")
10/1:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
pie_purchase = []
shopping = "y"
while shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
    pie_choice = input("Which pie do you choose?   ")
    pie_purchase.append(pie_choice)
    print("------------------------------------------------------------------------")
    print("Great! We'll have that " + pies[int(pie_choice) - 1] + " right out for you.")
    shopping = input("Would you like to make another purchase: (y)es or (n)o?   ")
print("------------------------------------------------------------------------")
print("You purchased a total of " + str(len(pie_purchases)) + ".")
10/2:
print ("Welcome to the house of pies! Here are our pies:")
pies = ["Pecan", "Apple Crisp", "Bean", "Banoffee", "Black Bun", "Blueberry", "Buko", "Burek", "Tamale", "Steak"]
pie_purchase = []
shopping = "y"
while shopping == "y":
    print ("---------------------------------------------------------------------------------------------------------")
    print ("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee," + "(5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek," + "(9) Tamale, (10) Steak")
    pie_choice = input("Which pie do you choose?   ")
    pie_purchase.append(pie_choice)
    print("------------------------------------------------------------------------")
    print("Great! We'll have that " + pies[int(pie_choice) - 1] + " right out for you.")
    shopping = input("Would you like to make another purchase: (y)es or (n)o?   ")
print("------------------------------------------------------------------------")
print("You purchased a total of " + str(len(pie_purchase)) + ".")
10/3:
# Import the random and string Module
import random
import string

# Utilize the string module's custom method: ".ascii_letters"
print(string.ascii_letters)

# Utilize the random module's custom method randint
for x in range(10):
    print(random.randint(1, 10))
10/4:
# Import the random and string Module
import random
import string

# Utilize the string module's custom method: ".ascii_letters"
print(string.ascii_letters)

# Utilize the random module's custom method randint
for x in range(10):
    print(random.randint(1.10))
for x in range(10):
    print(random.randint(1, 10))
10/5:
# Import the random and string Module
import random
import string

# Utilize the string module's custom method: ".ascii_letters"
print(string.ascii_letters)

# Utilize the random module's custom method randint
for x in range(10):
    print(random.randint(1.10))
10/6:
# Import the random and string Module
import random
import string

# Utilize the string module's custom method: ".ascii_letters"
print(string.ascii_letters)

# Utilize the random module's custom method randint
for x in range(10):
    print(random.randint(1,10))
10/7:
import os
import csv
csvpath = os.path.join('..', 'Resources', 'accounting.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(accounting.csv, delimiter=',')
    print(csvreader)
    csv_header = next(csvreader)
    print("CSV Header: {csv_header}")

    # Read each row of data after the header
    for row in csvreader:
        print(row)
10/8:
import os
import csv
csvpath = os.path.join('..', 'Desktop', 'Resources', 'accounting.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(accounting.csv, delimiter=',')
    print(csvreader)
    csv_header = next(csvreader)
    print("CSV Header: {csv_header}")

    # Read each row of data after the header
    for row in csvreader:
        print(row)
10/9:
import os
import csv
csvpath = os.path.join('..', 'Desktop', 'Resources', 'accounting.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    print(csvreader)
    csv_header = next(csvreader)
    print("CSV Header: {csv_header}")

    # Read each row of data after the header
    for row in csvreader:
        print(row)
10/10:
import os
import csv
csvpath = os.path.join('..', 'Desktop', 'Resources', 'accounting.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    print(csvreader)
    # Read each row of data after the header
    for row in csvreader:
        print(row)
10/11:
# Modules
import os
import csv

# Prompt user for video lookup
video = input("What show or movie are you looking for? ")

# Set path for file
csvpath = os.path.join("..", "Resources", "netflix_ratings.csv")

# Bonus
# ------------------------------------------
# Set variable to check if we found the video
found = False

# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[5])

            # BONUS: Set variable to confirm we have found the video
            found = True

            # BONUS: Stop at first results to avoid duplicates
            break

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
11/1:
# Modules
import os
import csv

# Prompt user for video lookup
video = input("What show or movie are you looking for? ")

# Set path for file
csvpath = os.path.join("..", "Resources", "netflix_ratings.csv")

# Bonus
# ------------------------------------------
# Set variable to check if we found the video
found = False

# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[5])

            # BONUS: Set variable to confirm we have found the video
            found = True

            # BONUS: Stop at first results to avoid duplicates
            break

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
11/2:
# Modules
import os
import csv

# Prompt user for video lookup
video = input("What show or movie are you looking for? ")

# Set path for file
csvpath = os.path.join("..", "08-Stu_ReadNetFlix", "Resources", "netflix_ratings.csv")

# Bonus
# ------------------------------------------
# Set variable to check if we found the video
found = False

# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[5])

            # BONUS: Set variable to confirm we have found the video
            found = True

            # BONUS: Stop at first results to avoid duplicates
            break

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
11/3:
# Modules
import os
import csv

# Prompt user for video lookup
video = input("What show or movie are you looking for? ")

# Set path for file
csvpath = os.path.join("Desktop", "08-Stu_ReadNetFlix", "Resources", "netflix_ratings.csv")

# Bonus
# ------------------------------------------
# Set variable to check if we found the video
found = False

# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[5])

            # BONUS: Set variable to confirm we have found the video
            found = True

            # BONUS: Stop at first results to avoid duplicates
            break

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
11/4:
# Modules
import os
import csv

# Prompt user for video lookup
video = input("What show or movie are you looking for? ")

# Set path for file
csvpath = os.path.join("..", "Resources", "netflix_ratings.csv")

# Bonus
# ------------------------------------------
# Set variable to check if we found the video
found = False

# Open the CSV
with open(csvpath, r, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[5])

            # BONUS: Set variable to confirm we have found the video
            found = True

            # BONUS: Stop at first results to avoid duplicates
            break

    # If the video is never found, alert the user
    if found is False:
        print("Sorry about this, we don't seem to have what you are looking for!")
11/5:
import os
import csv
from pathlib import Path
filepath = Path("Desktop/web_starter.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   title = []
   Price = []
   Subscriber_count = []
   Number_of_Reviews = []
   Course_Length = []
   
   for row in csvreader:
        title.append(row[1])
        Price.append(row[4])
        Subscriber_count.append(row[5])
        Number_of_Reviews.append(row[6])
        Course_Length.append(row[9])

   #print(title)
   #print(Price)
   #print(Subscriber_count)
   #print(Number_of_Reviews)
   #print(Course_Length)
   
   zipped_list = zip(title, Price, Subscriber_count, Number_of_Reviews,  Course_Length )
   print (zipped_list)
   
   import csv
with open("output.csv", "w") as csvfile:
   header = ['title', 'Price', 'Subscriber_count', ' Number_of_Reviews', 'Course_Length']
   writer = csv.writer(csvfile, delimiter=',')
   writer.writerow(header)
   writer.writerows(zipped_list)
11/6:
import os
import csv
from pathlib import Path
filepath = Path("web_starter.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   title = []
   Price = []
   Subscriber_count = []
   Number_of_Reviews = []
   Course_Length = []
   
   for row in csvreader:
        title.append(row[1])
        Price.append(row[4])
        Subscriber_count.append(row[5])
        Number_of_Reviews.append(row[6])
        Course_Length.append(row[9])

   #print(title)
   #print(Price)
   #print(Subscriber_count)
   #print(Number_of_Reviews)
   #print(Course_Length)
   
   zipped_list = zip(title, Price, Subscriber_count, Number_of_Reviews,  Course_Length )
   print (zipped_list)
   
   import csv
with open("output.csv", "w") as csvfile:
   header = ['title', 'Price', 'Subscriber_count', ' Number_of_Reviews', 'Course_Length']
   writer = csv.writer(csvfile, delimiter=',')
   writer.writerow(header)
   writer.writerows(zipped_list)
11/7:
import os
import csv
from pathlib import Path
filepath = Path("web_starter.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   title = []
   Price = []
   Subscriber_count = []
   Number_of_Reviews = []
   Course_Length = []
   
   for row in csvreader:
        title.append(row[1])
        Price.append(row[4])
        Subscriber_count.append(row[5])
        Number_of_Reviews.append(row[6])
        Course_Length.append(row[9])

   #print(title)
   #print(Price)
   #print(Subscriber_count)
   #print(Number_of_Reviews)
   #print(Course_Length)
   
   zipped_list = zip(title, Price, Subscriber_count, Number_of_Reviews,  Course_Length )
   print (zipped_list)
   
   import csv
with open("output.csv", "w") as csvfile:
   header = ['title', 'Price', 'Subscriber_count', ' Number_of_Reviews', 'Course_Length']
   writer = csv.writer(csvfile, delimiter=',')
   writer.writerow(header)
   writer.writerows(zipped_list)
12/1:
import os
import csv
from pathlib import Path
filepath = Path("web_starter.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   title = []
   Price = []
   Subscriber_count = []
   Number_of_Reviews = []
   Course_Length = []
   
   for row in csvreader:
        title.append(row[1])
        Price.append(row[4])
        Subscriber_count.append(row[5])
        Number_of_Reviews.append(row[6])
        Course_Length.append(row[9])

   #print(title)
   #print(Price)
   #print(Subscriber_count)
   #print(Number_of_Reviews)
   #print(Course_Length)
   
   zipped_list = zip(title, Price, Subscriber_count, Number_of_Reviews,  Course_Length )
   print (zipped_list)
   
   import csv
with open("output.csv", "w") as csvfile:
   header = ['title', 'Price', 'Subscriber_count', ' Number_of_Reviews', 'Course_Length']
   writer = csv.writer(csvfile, delimiter=',')
   writer.writerow(header)
   writer.writerows(zipped_list)
36/1:
import os
import csv
from pathlib import Path
filepath = Path("cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/2:
import os
import csv
from pathlib import Path
filepath = Path("cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/3:
import os
import csv
from pathlib import Path
filepath = Path("../"Resources","cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/4:
import os
import csv
from pathlib import Path
filepath = Path("../Resources","cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/5:
import os
import csv
from pathlib import Path
filepath = Path("Desktop/Resources","cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/6: !pwd
36/7:
import os
import csv
from pathlib import Path
filepath = Path("Desktop/,"cereal.csv")
cereal_csv = os.path.join("../Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/8:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join(".."Resources", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/9:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join(".."Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
36/10:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        print(row)
34/1:
import os
import csv
from pathlib import Path
filepath = Path("Desktop",""web_starter.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   title = []
   Price = []
   Subscriber_count = []
   Number_of_Reviews = []
   Course_Length = []
   
   for row in csvreader:
        title.append(row[1])
        Price.append(row[4])
        Subscriber_count.append(row[5])
        Number_of_Reviews.append(row[6])
        Course_Length.append(row[9])

   #print(title)
   #print(Price)
   #print(Subscriber_count)
   #print(Number_of_Reviews)
   #print(Course_Length)
   
   zipped_list = zip(title, Price, Subscriber_count, Number_of_Reviews,  Course_Length )
   print (zipped_list)
   
   import csv
with open("output.csv", "w") as csvfile:
   header = ['title', 'Price', 'Subscriber_count', ' Number_of_Reviews', 'Course_Length']
   writer = csv.writer(csvfile, delimiter=',')
   writer.writerow(header)
   writer.writerows(zipped_list)
36/11:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
        title.append(column[1])
        Price.append(column[8])
        zipped_list = zip(name, fiber)
   print (zipped_list)
36/12:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
name = []
fiber = []
for row in csvreader:
    title.append(column[1])
    Price.append(column[8])
    zipped_list = zip(name, fiber)
    print (zipped_list)
36/13:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as cereal.csv:
   csvreader = csv.reader(cereal.csv, delimiter=",")
name = []
fiber = []
for row in csvreader:
    title.append(column[1])
    Price.append(column[8])
    zipped_list = zip(name, fiber)
    print (zipped_list)
36/14:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as cereal_csv:
   csvreader = csv.reader(cereal_csv, delimiter=",")
name = []
fiber = []
for row in csvreader:
    title.append(column[1])
    Price.append(column[8])
    zipped_list = zip(name, fiber)
    print (zipped_list)
36/15:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as cereal_csv:
   csvreader = csv.reader(cereal_csv, delimiter=",")
name = []
fiber = []
for row in csvreader:
    print (name)
36/16:
import os
import csv
from pathlib import Path
filepath = Path("Desktop","cereal.csv")
cereal_csv = os.path.join("Desktop", "cereal.csv")
with open(filepath, newline="", encoding='utf-8' ) as cereal_csv:
   csvreader = csv.reader(cereal_csv, delimiter=",")

for row in csvreader:
    print (name)
36/17:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        print(row)
36/18:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
name = []
fiber = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fiber.append[7]
    print (fiber)
36/19:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
name = []
fiber = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fiber.append(row[7])
    print (fiber)
36/20:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
name = []
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
36/21:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        fiber>=5
        print(fiber)
36/22:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        fiber>=5
        print int(fiber)
36/23:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        fiber>=str("5")
        print (fiber)
36/24:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        fiber>= int("5")
        print (fiber)
36/25:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        fiber >= str("5")
        print (fiber)
36/26:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= str("5"):
        print (fiber)
36/27:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= str("5"):
            print (fiber)
36/28:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= float("5"):
            print (fiber)
36/29:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= float(5):
            print (fiber)
36/30:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= float(5):
            print (fiber)
36/31:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber > float(5):
            print (fiber)
36/32:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber > float(5.0):
            print (fiber)
36/33:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber > float(5.0):
            print (fiber)
36/34:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= float(5.0):
            print (fiber)
36/35:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= str(5.0):
            print (fiber)
36/36:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= str(5):
            print (fiber)
36/37:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= float(5):
            print (fiber)
36/38:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= 5:
            print float(fiber)
36/39:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str(5):
            print float(fiber)
36/40:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str(5):
            print (fiber)
36/41:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str(5):
            print float(fiber)
36/42:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str("5"):
            print float(fiber)
36/43:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str("5"):
            print float("fiber")
36/44:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >=str("5"):
            print float(fiber)
36/45:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    for row in csvreader:
        fibers.append(row[7])
    print (fibers)
    for fiber in fibers:
        if fiber >= 5:
            print float(fiber)
36/46:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    if float(row[7]) >=5:
    print (row[0], row[7])
36/47:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    if float(row[7]) >=5:
        print (row[0], row[7])
36/48:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    if float(row[7]) >=5:
        print (row[0], row[7])
36/49:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    if float(row[7]) >=5:
        print (row[0], row[7])
36/50:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(cereal_csv)
    if float(row[7]) >= 5:
        print (row[0], row[7])
36/51:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(csvreader)
    for row in csvreader:
        if float(row[7]) >= 5:
        print (row[0], row[7])
36/52:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=",")
    next(csvreader)
    for row in csvreader:
        if float(row[7]) >= 5:
            print (row[0], row[7])
36/53:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
'open the file
with open(cereal.csv, 'r') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        print(row)
36/54:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(cereal.csv, 'r') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        print(row)
36/55:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
fibers = []
with open(cereal.csv, 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    for row in csvreader:
        print(row)
36/56:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(cereal.csv, 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    for row in csvreader:
        print(row)
36/57:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(cereal.csv, 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    for row in csvreader:
        print(row[0])
36/58:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open('cereal.csv', 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    for row in csvreader:
        print(row[0])
36/59:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(cereal.csv, 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    for row in csvreader:
        print(row[0])
36/60:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(cereal.csv, 'r') as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    header =next(cereal_csv)
    for row in csvreader:
        print(row[0])
36/61:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    header =next(cereal_csv)
    for row in csvreader:
        print(row[0])
36/62:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    header =next(cereal_csv)
    for row in csvreader:
        print(row)
36/63:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    #header =next(cereal_csv)
    header = next(csvreader)
    #print(header)
    for row in csvreader:
        print(row)
36/64:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    #header =next(cereal_csv)
    header = next(csvreader)
    #print(header)
    for row in csvreader:
        print(row)
36/65:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    header =next(cereal_csv)
    #header = next(csvreader)
    #print(header)
    for row in csvreader:
        print(row)
36/66:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    header =next(cereal_csv)
    #header = next(csvreader)
    #print(header)
    #for row in csvreader:
     #   print(row)
    print(list(csvreader))
36/67:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    header =next(cereal_csv)
    #header = next(csvreader)
    #print(header)
    #for row in csvreader:
     #   print(row)
    print(list(csvreader)[0])
36/68:
import os
import csv
from pathlib import Path

filepath = Path("Desktop","cereal.csv")
#open csv file
with open(filepath, newline="") as cereal_csv:
    csvreader = csv.reader(cereal_csv, delimiter=',')
    #csv reader gives first line of text
    header =next(cereal_csv)
    #this print gives only the header names
    #header = next(csvreader)
    #print(header)
    for row in csvreader:
        print(row)
    #this print statement gives the inner list data, list within list
    #print(list(csvreader)[0])
37/1: name = {Shubha}
37/2: Profile = {"name", "age", "hobbies","wake-up_time"}
37/3:
name = ["shubha"]
age = [38]
hobbies = [cooking, painting]
wake-up_time = ["5 AM"]
Profile
37/4:
name = ["shubha"]
age = [38]
hobbies = [cooking, painting]
wake-up_time = [5]
Profile
37/5:
name = ["shubha"]
age = [38]
hobbies = [cooking, painting]
wake-up_time = [5]
Profile
37/6:
name = ["shubha"]
age = [38]
hobbies = ["cooking"]
wake-up_time = [5]
Profile
38/1:
def my_function():
    #code goes here
    return "value"
38/2:
def show():
    return print("hi")
38/3:
def show():
    return print("hi")
show()
38/4:
def show():
    print("hi")
show()
38/5:
#variable used only with function and/or print statement, not outside of it
def show(message):
    print(message)
show("Hey you!")
38/6:
def square(num):
    squared = num*num
    return squared
print(3)
38/7:
def square(num):
    squared = num*num
    return squared
print(squared)
38/8:
def square(num):
    squared = num*num
    return squared
print(square)
38/9:
numbers = [1,2,3,4,5]
def mysum(mynumbers):
    total = 0
    for number in mynumbers:
        total+= number
        return(total)
    mysum(numbers)
38/10: 10
37/7:
Profile = {"name": "shubha"
"age": 38
"hobbies": "cooking"
"wake-up_time": 5}
Profile = {"name", "age", "hobbies","wake-up_time"}
37/8:
Profile = {"name", "age", "hobbies","wake-up_time"}
Profile = {"name": "shubha"
"age" 38
"hobbies": "cooking"
"wake-up_time": 5}
print {"name", "age", "hobbies","wake-up_time"}
37/9:
Profile = {"name", "age", "hobbies","wake-up_time"}
Profile = {"name": "shubha","age" 38,"hobbies": "cooking","wake-up_time": 5}
print {"name", "age", "hobbies","wake-up_time"}
37/10:
Profile = {"name", "age", "hobbies","wake-up_time"}
Profile_personal= {"name": "shubha","age" 38,"hobbies": "cooking","wake-up_time": 5}
print {"name", "age", "hobbies","wake-up_time"}
37/11:
Profile = {"name", "age", "hobbies","wake-up_time"}
Profile_personal= {"name": "shubha","age" 38,"hobbies": "cooking","wake-up_time": 5}
print ({"name", "age", "hobbies","wake-up_time"})
46/1:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("Desktop","budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile, delimiter=',')
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/2:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("Desktop","budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfileS)
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/3:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("Desktop","budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/4:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("Desktop","budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile, delimiter=',')
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/5:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile, delimiter=',')
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/6:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath, newline="") as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile, delimiter=',')
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/7:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/8:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        print(row)
46/9:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#Total_Months = 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        total_months = total_months +1
        print(row)
46/10:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        total_months = total_months + 1
        print(row)
46/11:
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        total_months = total_months + 1
        print(total_months)
46/12:
import os
import csv
from pathlib import Path
!ls
# Link file path to Python
#filepath = Path("budget_data.csv")
# Declare variables
#total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    #for row in csvreader:
        #total_months = total_months + 1
        #print(total_months)
46/13:
import os
import csv
#from pathlib import Path
!ls
# Link file path to Python
#filepath = Path("budget_data.csv")
# Declare variables
#total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    #for row in csvreader:
        #total_months = total_months + 1
        #print(total_months)
46/14:
import os
import csv
#from pathlib import Path
ls
# Link file path to Python
#filepath = Path("budget_data.csv")
# Declare variables
#total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    #for row in csvreader:
        #total_months = total_months + 1
        #print(total_months)
46/15:
import os
import csv
#from pathlib import Path
!ls
# Link file path to Python
#filepath = Path("budget_data.csv")
# Declare variables
#total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    #for row in csvreader:
        #total_months = total_months + 1
        #print(total_months)
46/16:
import os
import csv
pwd
#from pathlib import Path
# Link file path to Python
#filepath = Path("budget_data.csv")
# Declare variables
#total_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    #for row in csvreader:
        #total_months = total_months + 1
        #print(total_months)
46/17:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csvreader = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csvreader:
        sum_months = sum_months + 1
        print(sum_months)
46/18:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Loop through all the datas we collect 
    for row in csv_data:
        sum_months = sum_months + 1
        print(sum_months)
46/19:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months = sum_months + x
        print(sum_months)
46/20:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[0])
        sum_months = sum_months + x
        print(sum_months)
46/21:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months = sum_months + x
        print(sum_months)
46/22:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months = sum_months + x
        print(sum_months)
46/23:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[2])
        sum_months = sum_months + x
        print(sum_months)
46/24:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[2])
        sum_months = sum_months + x
        print(sum_months)
46/25:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.Dictreader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[2])
        sum_months = sum_months + x
        print(sum_months)
46/26:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months = sum_months + x
        print(sum_months)
46/27:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months += x
        print(sum_months)
46/28:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months += x
        print(sum_months)
46/29:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(filepath) as budgetfile:
   # Read files
    #csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    #for row in csv_data:
        #x = int(row[1])
        #sum_months += x
        #print(sum_months)
sum = 0
with open(file_path) as file:
    data = csv.reader(file,delimiter=',')
    next(data)
    for row in data:
        x = int(row[1])
        sum += x
        
print(sum)
46/30:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
#with open(file_path) as budget_file:
   # Read files
    #csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loop through all the datas we collect 
    #for row in csv_data:
        #x = int(row[1])
        #sum_months += x
        #print(sum_months)
sum = 0
with open(file_path) as budget_file:
    csv_data = csv.reader(budget_file)
    next(csv_data)
    for row in csv_data:
        x = int(row[1])
        sum += x
        
print(sum)
46/31:
import os
import csv

#from pathlib import Path
# Link file path to Python
filepath = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(filepath) as budgetfile:
   # Read files
    csv_data = csv.reader(budgetfile)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        x = int(row[1])
        sum_months += x
    print(sum_months)
46/32:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        sum_months = sum_months + (row) 
    print(sum_months)
#import inspect
46/33:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        sum_months = sum_months + (row[1]) 
    print(sum_months)
#import inspect
46/34:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
sum_months = 0 
#net_amount = 
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        sum_months = sum_months + int(row[1]) 
    print(sum_months)
#import inspect
46/35:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
revenue_total = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        revenue_total = revenue_total + int(row[1]) 
    print(revenue_total)
#import inspect
46/36:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
revenue_total = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        revenue_total = revenue_total + (row[0]) 
    print(revenue_total)
        average_change = 
#import inspect
46/37:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
revenue_total = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        revenue_total = revenue_total + (row[0]) 
    print(revenue_total)
        #average_change = 
#import inspect
46/38:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
revenue_total = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        revenue_total = revenue_total + str(row[0]) 
    print(revenue_total)
        #average_change = 
#import inspect
46/39:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
revenue_total = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        revenue_total = revenue_total + str(row[0]) 
    print(revenue_total)
        #average_change = 
#import inspect
46/40:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[0])
        net_amount = net_amount + x 
    print(revenue_total)
        #average_change = 
#import inspect
46/41:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(revenue_total)
        #average_change = 
#import inspect
46/42:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loop through all the datas we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(revenue_total)
        #average_change = 
#import inspect
46/43:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/44:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[Revenue])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/45:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    #next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row["Revenue"])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/46:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/47:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = str(row["Revenue"])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/48:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = str(row[1])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/49:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
#Average_Change = 
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
        #average_change = 
#import inspect
46/50:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
        average_change = x -(x+1)
    print(average_change)
#import inspect
46/51:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
        average_change = x -(x+1)
    print(net_amount)
    print(average_change)
#import inspect
46/52:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
#sum_months = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
        average_change = net_amount/41
    print(net_amount)
    print(average_change)
#import inspect
46/53:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
        months_total = months_total + 1
    print(net_amount)
    print(months_total)
    
#import inspect
46/54:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        months_total = months_total + 1
        print(months_total)
        # calculate net amount over entire period.
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
    
    
#import inspect
46/55:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
    print(months_total)
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
    
    
#import inspect
46/56:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
    print(months_total)
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
    print(net_amount)
    
    
#import inspect
46/57:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
    print(months_total)
    print(net_amount)
    
    
#import inspect
46/58:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" total_months   )
print("Total Net Amount:" +  "$"+net_amount)
#import inspect
46/59:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" months_total)
print("Total Net Amount:" +  "$"+net_amount)
#import inspect
46/60:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" +  "$"+net_amount)
#import inspect
46/61:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" +  "$"+(net_amount))
#import inspect
46/62:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" +  "$"+(net_amount))
#import inspect
46/63:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" + "$"+(net_amount))
#import inspect
46/64:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" + "$" + (net_amount))
#import inspect
46/65:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
#print("Total Net Amount:" + "$" + (net_amount))
#import inspect
46/66:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
#print("Total Net Amount:" + "$" + (net_amount))
#import inspect
46/67:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
#print("Total Net Amount:" + "$" + (net_amount))
46/68:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" + "$" + (net_amount))
46/69:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" (net_amount))
46/70:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" + (net_amount))
46/71:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print("Total Net Amount:" (net_amount))
46/72:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print(net_amount)
46/73:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
print(net_amount)
46/74:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" (months_total)
46/75:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print(months_total)
46/76:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        x = int(row[1])
        net_amount = net_amount + x 
print(months_total)
print(net_amount)
46/77:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (previous_net_mount - net_amount)
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/78:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
average_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (previous_net_mount - net_amount)
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/79:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (previous_net_mount - net_amount)
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/80:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (previous_net_amount - net_amount)
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/81:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (previous_net_amount - net_amount)
        previous_net_amount = net_amount
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/82:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        net_amount = int(row[1])
        total_net_amount = total_net_amount + net_amount
        total_change = total_change + (net_amount-previous_net_amount)
        previous_net_amount = net_amount
print(months_total)
print(total_net_amount)
print(total_change/months_total)
46/83:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_amount = 0
total_net_amount = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        ind_revenue = int(row[1])
        net_revenue = net_revenue + ind_revenue
        total_change = total_change + (net_amount - previous_net_amount)
        previous_net_amount = net_amount
print(months_total)
print(net_revenue)
print(total_change/months_total)
46/84:
import os
import csv

#from pathlib import Path
# Link file path to Python
file_path = Path("budget_data.csv")
# Declare variables
months_total = 0 
net_revenue = 0
#Greatest_Increase = ("", (0))
#Greatest_Decrease = ("", (0))
total_change = 0
# open file which is linked from the path 
with open(file_path) as budget_file:
   # Read files
    csv_data = csv.reader(budget_file)
    # Skips the headers of the csv file
    next(csv_data)
    # Loops through all the data we collect 
    previous_net_amount = 0
    for row in csv_data:
        # Calculate total no. of months for the entire period
        months_total = months_total + 1
        # calculate net amount over entire period
        ind_revenue = int(row[1])
        net_revenue = net_revenue + ind_revenue
        total_change = total_change + (net_amount - previous_net_amount)
        previous_net_amount = net_amount
print(months_total)
print(net_revenue)
print(total_change/months_total)
48/1: import numpy as np
48/2: S = list(Range(10))
48/3: S = list(range(10))
48/4: np._version_
48/5: np.__version__
48/6: L=list(range(10))
48/7: L=list(range(10))
48/8:
L=list(range(10))
Print(L)
48/9:
L=list(range(10))
print(L)
49/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
months = 0
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=" ")
    # Skips the headers of the csv file
    next(csvreader)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # Calculate total no. of months for the entire period
        months += 1 
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
        

    print("Financial Analysis")
    print("----------------------------------")
    print("Total Months:" + str(months))
    print("Total_net_amount:" + str(sum(revenue)))
    print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
50/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path
50/2:
# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
months = 0
revenue = []
avgchange = []
50/3:
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=" ")
    # Skips the headers of the csv file
    next(csvreader)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # Calculate total no. of months for the entire period
        months += 1 
        # Calculate net amount over entire period
        revenue.append(int(row[1]))
50/4:
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # Calculate total no. of months for the entire period
        months += 1 
        # Calculate net amount over entire period
        revenue.append(int(row[1]))
50/5:
 for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
        

    print("Financial Analysis")
    print("----------------------------------")
    print("Total Months:" + str(months))
    print("Total_net_amount:" + str(sum(revenue)))
    print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
50/6:
 for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
        

print("Financial Analysis")
print("----------------------------------")
print("Total Months:" + str(months))
print("Total_net_amount:" + str(sum(revenue)))
print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
50/7:
# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
months = 0
revenue = []
avgchange = []
 next(csvfile)
50/8:
# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
months = 0
revenue = []
avgchange = []
next(csvfile)
50/9:
 for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
        

print("Financial Analysis")
print("----------------------------------")
print("Total Months:" + str(months))
print("Total_net_amount:" + str(sum(revenue)))
print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
50/10:
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    next(csvreader)
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # Calculate total no. of months for the entire period
        months += 1 
        # Calculate net amount over entire period
        revenue.append(int(row[1]))
50/11:
 for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
        

print("Financial Analysis")
print("----------------------------------")
print("Total Months:" + str(months))
print("Total_net_amount:" + str(sum(revenue)))
print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
50/12:
 for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
print("Financial Analysis")
print("----------------------------------")
print("Total Months:" + str(months))
print("Total_net_amount:" + str(sum(revenue)))
print("Average Change:" + str(sum(avgchange)/(str(months)-1)))
52/1: import pandas as pd
52/2: pd.__version__
53/1:
# Modules
import os
import csv
from pathlib as import path
53/2:
# Modules
import os
import csv
from pathlib import path
53/3:
# Modules
import os
import csv
53/4:
# Modules
import os
import csv
from pathlib import Path
53/5: directory = 'Resources/netflix_ratings.csv'
53/6: directory = Path('Resources/netflix_ratings.csv')
53/7: print(directory)
53/8: video = input("What movie are you looking for?"    )
53/9: video = input("What movie are you looking for?"    )
53/10:
found = False
with open(file_path) as csvfile:
    csvreader = csv.reader(csvfile,delimiter = ",")
    for row in csvreader:
        if row[0]==video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6]))
            found = True
        else:
            print("sorry, we do not have that video now!")
53/11:
found = False
with open(file_path) as csvfile:
    csvreader = csv.reader(csvfile,delimiter = ",")
    for row in csvreader:
        if row[0]==video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
            found = True
        else:
            print("sorry, we do not have that video now!")
53/12:
found = False
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile,delimiter = ",")
    for row in csvreader:
        if row[0]==video:
            print(row[0] + " is rated " + row[1] + " with a rating of " + row[6])
            found = True
        else:
            print("sorry, we do not have that video now!")
54/1:
# Import Dependencies
import pandas as pd
54/2:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {["Frame": "Ornate", "Classical", "Modern", "Wood", "Cardboard"]
             ["Price": "15.0", "12.5", "10.0", "5.0", "1.0"]
             ["Sales": "100", "200", "150", "300", "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/3:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {["Frame": "Ornate", "Classical", "Modern", "Wood", "Cardboard"]
             ["Price": 15.0, 12.5, 10.0, 5.0, 1.0]
             ["Sales": 100, 200, 150, 300, "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/4:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {["Frame": "Ornate", "Classical", "Modern", "Wood", "Cardboard"],
             ["Price": 15.0, 12.5, 10.0, 5.0, 1.0],
             ["Sales": 100, 200, 150, 300, "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/5:
# Import Dependencies
import pandas as pd
54/6:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {["Frame": "Ornate", "Classical", "Modern", "Wood", "Cardboard"],
             ["Price": 15.0, 12.5, 10.0, 5.0, 1.0],
             ["Sales": 100, 200, 150, 300, "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/7:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {"Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
             ["Price ": [15.0, 12.5, 10.0, 5.0, 1.0],
             ["Sales": [100, 200, 150, 300, "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/8:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
dictionary = {"Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
             "Price ": [15.0, 12.5, 10.0, 5.0, 1.0],
             "Sales": [100, 200, 150, 300, "N/A"]}
solution = pd.DataFrame(dictionary)
solution
54/9:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
List =[{"Painting": "Mona Lisa (Knockoff)", "Popularity": "Very Popular", "Price":25},
      {"Painting": "Van Gogh (Knockoff)", "Popularity": "Popular", "Price":20}
      {"Painting": "Starving Artist", "Popularity": "Average", "Price":10}]
54/10:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
List =[{"Painting": "Mona Lisa (Knockoff)", "Popularity": "Very Popular", "Price":25},
      {"Painting": "Van Gogh (Knockoff)", "Popularity": "Popular", "Price":20},
      {"Painting": "Starving Artist", "Popularity": "Average", "Price":10}]
54/11:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
List =[{"Painting": "Mona Lisa (Knockoff)", "Popularity": "Very Popular", "Price":25},
      {"Painting": "Van Gogh (Knockoff)", "Popularity": "Popular", "Price":20},
      {"Painting": "Starving Artist", "Popularity": "Average", "Price":10}]
54/12:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
List =[{"Painting": "Mona Lisa (Knockoff)", "Popularity": "Very Popular", "Price":25},
      {"Painting": "Van Gogh (Knockoff)", "Popularity": "Popular", "Price":20},
      {"Painting": "Starving Artist", "Popularity": "Average", "Price":10}]
54/13:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
List =[{"Painting": "Mona Lisa (Knockoff)", "Popularity": "Very Popular", "Price":25},
      {"Painting": "Van Gogh (Knockoff)", "Popularity": "Popular", "Price":20},
      {"Painting": "Starving Artist", "Popularity": "Average", "Price":10}]
solution = pd.DataFrame(List)
solution
55/1:
# Collecting a summary of all numeric data
training_data[["Weight", "Membership(Days)"].sum()
55/2:
# Collecting a summary of all numeric data
training_data[["Weight","Membership(Days)"].sum()
55/3:
# Collecting a summary of all numeric data
training_data[["Weight","Membership(Days)"]].sum()
55/4:
# Collecting a summary of all numeric data
training_data[["Weight","Membership(Days)"]].sum()
training_data
55/5:
# Collecting a summary of all numeric data
training_data[["Weight","Membership(Days)"]].describe()
training_data
55/6:
# Collecting a summary of all numeric data
training_data["Weight"].describe()
training_data
55/7:
# Collecting a summary of all numeric data
training_data["Weight"].describe()
training_data
55/8:
# Collecting a summary of all numeric data
training_data["Weight"].describe()
training_data
55/9:
# Collecting a summary of all numeric data
training_data.describe()
training_data
55/10:
# Finding the names of the trainers
training_data["Names"].
55/11:
# Import Dependencies
import pandas as pd
import random
55/12:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head(10)
55/13:
# Collecting a summary of all numeric data
training_data.describe()
training_data
55/14:
# Collecting a summary of all numeric data
training_data.describe()
55/15:
# Collecting a summary of all numeric data
training_data.describe(include ='all')
55/16:
# Finding the names of the trainers
training_data.'Name'
55/17:
# Finding the names of the trainers
training_data.'Name'
55/18:
# Finding the names of the trainers
training_data.Name
55/19:
# Finding the names of the trainers
training_data.Trainer
55/20:
# Finding the average weight of all students
training_data[Weight].average()
55/21:
# Finding the average weight of all students
training_data["Weight"].average()
55/22:
# Converting the membership days into weeks and then adding a column to the Data
#create a new column
training_data["Age"] = weeks
training data.head()
55/23:
# Converting the membership days into weeks and then adding a column to the Data
#create a new column
training_data["Age"] = weeks
training data.head()
55/24:
# Converting the membership days into weeks and then adding a column to the Data
#create a new column
training_data["Age"] = weeks
training data.head()
55/25:
# Converting the membership days into weeks and then adding a column to the Data
#create a new column
training_data["Age"] = weeks
training_data.head()
55/26:
#reorganize columns using double brackets
org_df = training_data[["Name","Trainer", "Weight","Membership(Days)"]]
od = org_df
od.head(0)
od.columns
56/1:
hello = "Hello world"
print (hello)
56/2: 4+4
56/3:
w=3
a
56/4:
w=3
w
56/5:
hello = "Hello world"
hello
56/6:
w=10
w
56/7: import pandas as pd
56/8: dsL"UCLA", "UCD", "UC Irving", "Florida University","UTA"])
56/9: ds[("UCLA","UCD", "UC Irving", "Florida University","UTA"])
56/10:
ds(["UCLA","UCD", "UC Irving", "Florida University","UTA"])
ds
56/11:
ds=pd.Series(["UCLA","UCD", "UC Irving", "Florida University","UTA"])
ds
56/12:
ds = pd.Series(["UCLA","UCD", "UC Irving", "Florida University","UTA"], index=["i","ii","iii","iv","v"])
ds
56/13: ds.index
56/14: ds= pd.Series(["UCLA","UCD", "UC Irving", "Florida University","UTA"])
56/15:
ds= pd.Series(["UCLA","UCD", "UC Irving", "Florida University","UTA"])
ds
56/16:
ds={"a":"abc", "b":"bcd","c":"cde"}
sd = pd.Series(ds)
sd
56/17:
ds={"a":3, "b":4,"c":5}
sd = pd.Series(ds)
sd
56/18: s[0]
56/19:
s=pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])
s
56/20: s[0]
56/21:
s = {"a":123,"b":234,"c":345}
sd = pd.DataFrame(s)
sd
56/22:
s = {"a":pd.Series([1,2,3], index=["a","b","c"]),"b":pd.series([3,4,5], index=["b","c","d"]),"c":pd.series([4,5,6], index=["c","d","e"])}
sd = pd.DataFrame(s)
sd
56/23:
s = {"a":pd.Series([1,2,3], index=["a","b","c"]),"b":pd.Series([3,4,5], index=["b","c","d"]),"c":pd.Series([4,5,6], index=["c","d","e"])}
sd = pd.DataFrame(s)
sd
56/24:
sd=pd.DataFrame(s, index=[1,2,3])
sd
56/25:
sd=pd.DataFrame(s, index=["a","b","c"])
sd
56/26: sd.index
56/27: sd.columns
56/28:
data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')])
d=pd.DataFrame(data)
58/1: !jupyter nbconvert --to python LocAndIloc.ipynb
58/2: !pwd
58/3: import pandas as pd
58/4: file = "Resources?sampleData.csv"
58/5: file = "Resources/sampleData.csv"
58/6: help(pd.read_csv)
58/7: help(pd.DataFrame)
58/8: help(pd.Series)
58/9:
df_original = pd.read_csv(file)
df_original.head()
59/1: import pandas as pd
59/2: file = pd.read_csv("Resources/sample_data.csv")
59/3: file = pd.read_csv("Resources/movie_scores.csv")
59/4: file = pd.read_csv("Resources/movie_scores.csv")
59/5: file = ("Resources/movie_scores.csv")
59/6: df_movie = pd.read_csv(file)
59/7:
df_movie = pd.read_csv(file)
df_movie.head()
59/8:
df_movie = pd.read_csv(file)
df_movie.head(0)
59/9: file = "Resources/movie_scores.csv"
59/10: import pandas as pd
59/11: file = "Resources/movie_scores.csv"
59/12:
df_movie = pd.read_csv(file)
df_movie.head(0)
59/13:
df_movie = pd.read_csv(file)
df_movie.head(0)
59/14:
df_movie = pd.read_csv(file)
df_movie.head()
59/15:
df_movie = pd.read_csv(file)
df_movie.head()
59/16:
df_movie = pd.read_csv(file)
df_movie.head()
59/17: file = "/Resources/movie_scores.csv"
59/18:
df_movie = pd.read_csv(file)
df_movie.head()
60/1:
# Dependencies
import pandas as pd
60/2:
# Load in File from resources
# 'movie_scores.csv'
file = "Resources/movie_scores.csv"
60/3:
# Read and display with pandas
df_movie = pd.read_csv(file)
60/4:
# Read and display with pandas
df_movie = pd.read_csv(file)
df_movie.head()
60/5:
# List all the columns the table provided
file_pd.columns
60/6:
# List all the columns the table provided
df_movie_pd.columns
61/1:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file)
61/2:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file, , encoding="ISO-8859-1")
61/3:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
61/4:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file,encoding="ISO-8859-1")
61/5:
# Identify incomplete rows
df.count()
61/6:
# Dependencies
import pandas as pd
import numpy as np
61/7:
# Name of the CSV file
file = 'Resources/donors2008.csv'
61/8:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
61/9:
# Identify incomplete rows
df.count()
61/10:
# replace renames columns- works on series
#Clean up Employer category. Replace 'Self Employed' and 'Self' with 'Self-Employed'
df['Employer'] = df['Employer'].replace(
    {'Self Employed': 'Self-Employed', 'Self': 'Self-Employed'})
61/11:
# Verify clean-up.
df['Employer'].value_counts()
61/12:
df['Employer'] = df['Employer'].replace({'Not Employed': 'Unemployed'})
df['Employer'].value_counts()
62/1:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
62/2:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = df.read_csv(file, encoding="ISO-8859-1")
62/3:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = df.read_csv(file, encoding="ISO-8859-1")
62/4:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/5:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/6:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/7:
# Import Dependencies
import pandas as pd
import numpy as np
62/8:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/9:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident_data20.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/10:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident_data20.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/11:
# Import Dependencies
import pandas as pd
import numpy as np
62/12:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident_data20.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/13:
# Reference the file where the CSV is located

# Import the data into a Pandas DataFrame
file = "Resources/crime_incident_data2017.csv"
df = pd.read_csv(file, encoding="ISO-8859-1")
62/14:
# look for missing values
df(NaN)
62/15:
# look for missing values
df.head()
62/16:
# look for missing values
df.head()
df = df.dropna(how='NaN')
62/17:
# look for missing values
df.head()
62/18:
# look for missing values
df.head()
df.count()
62/19:
# look for missing values
df.head()
#df.count()
62/20:
# look for missing values
#df.head()
df.count()
61/13:
# Delete extraneous column
#del - removes columns from dataframe
del df['FIELD8']
#df.head()
62/21:
# drop null rows
df = df.dropna(how='NaN')
62/22:
# drop null rows
df = df.dropna(how='NaN')
df
62/23:
# drop null rows
df = df.dropna(how='any'S)
62/24:
# drop null rows
df = df.dropna(how='any')
62/25:
# drop null rows
df = df.dropna(how='any')
62/26:
# drop null rows
df = df.dropna(how='any')
62/27:
# drop null rows
df = df.dropna(how='any')
df.head()
62/28:
# drop null rows
df = df.dropna(how='any')
df.count()
62/29:
# verify counts
df.count()
62/30:
# drop null rows
df = df.dropna(how='any')
62/31:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
df.dtypes
62/32:
# Combine similar offenses
df["Offense Type"] = df.["Offense Type"].replace({"Offense Category": "Offense Type"})
62/33:
# Combine similar offenses
df["Offense Type"] = df.["Offense Type"].replace({"Offense Category": "Offense Type"})
62/34:
# Combine similar offenses
df['Offense Type'] = df.['Offense Type'].replace({'Offense Category': 'Offense Type', 'Offense Count': 'Offense Type'})
df['Offense Type'].valuecounts()
62/35:
# Combine similar offenses
df['Offense Type'] = df.['Offense Type'].replace({'Offense Type': 'Offense Category', 'Offense Type': 'Offense Count'})
df['Offense Type'].valuecounts()
62/36:
# Combine similar offenses
df['Offense Type'] = df.['Offense Type'].replace({'Offense Type': 'Offense Category', 'Offense Type': 'Offense Count'})
df['Offense Type'].valuecounts()
62/37:
# Combine similar offenses
df['Offense Type'] = df.['Offense Type'].replace({'Offense Category': 'Offense Type'})
df['Offense Type'].valuecounts()
62/38:
# Combine similar offenses
df['Offense Type'] = df['Offense Type'].replace({'Offense Category': 'Offense Type'})
df['Offense Type'].valuecounts()
62/39:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
df.dtypes
df['Offense Type'].valuecounts()
62/40:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
#df.dtypes
df['Offense Type'].valuecounts()
62/41:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
#df.dtypes
df['Offense Type'].valuecounts()
62/42:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
#df.dtypes
df['Offense Type'].valuecounts()
62/43:
# Check to see if there are any values with mispelled or similar values in "Offense Type"
#df.dtypes
df['Offense Type'].value_counts()
66/1:
# Import Dependencies
import pandas as pd
66/2:
# Create a reference the CSV file desired
csv_path = "Resources/ufoSightings.csv"

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv(csv_path)

# Print the first five rows of data to the screen
ufo_df.head()
66/3:
# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")
clean_ufo_df.count()
66/4:
# Converting the "duration (seconds)" column's values to numeric
clean_ufo_df["duration (seconds)"] = pd.to_numeric(
    clean_ufo_df["duration (seconds)"])
66/5:
# Filter the data so that only those sightings in the US are in a DataFrame
usa_ufo_df = clean_ufo_df.loc[clean_ufo_df["country"] == "us", :]

usa_ufo_df.head()
66/6:
# Count how many sightings have occured within each state
state_counts = usa_ufo_df["state"].value_counts()
state_counts.head()
66/7:
# Using GroupBy in order to separate the data into fields according to "state" values
grouped_usa_df = usa_ufo_df.groupby(['state'])

# The object returned is a "GroupBy" object and cannot be viewed normally...
print(grouped_usa_df)

# In order to be visualized, a data function must be used...
grouped_usa_df.count().head(10)
66/8:
# Since "duration (seconds)" was converted to a numeric time, it can now be summed up per state
state_duration = grouped_usa_df["duration (seconds)"].sum()
state_duration.head()
66/9:
# Creating a new DataFrame using both duration and count
state_summary_table = pd.DataFrame({"Number of Sightings": state_counts,
                                    "Total Visit Time": state_duration})
state_summary_table.head()
66/10:
# It is also possible to group a DataFrame by multiple columns
# This returns an object with multiple indexes, however, which can be harder to deal with
grouped_international_data = clean_ufo_df.groupby(['country', 'state'])

grouped_international_data.count().head(20)
66/11:
# Converting a GroupBy object into a DataFrame
international_duration = pd.DataFrame(
    grouped_international_data["duration (seconds)"].sum())
international_duration.head(10)
67/1:
# Dependencies
import pandas as pd
67/2:
# File to load
file = "Reources/pokeman.csv"
ufo = pd.read_csv(file)
67/3:
# File to load
file = "Resources/pokeman.csv"
ufo = pd.read_csv(file)
67/4:
# Dependencies
import pandas as pd
67/5:
# File to load
file = "Resources/pokeman.csv"
ufo = pd.read_csv(file)
67/6:
# File to load
file = "Resources/Pokemon.csv"
ufo = pd.read_csv(file)
67/7:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head()
67/8:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head(:)
67/9:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head(all)
67/10:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head()
help.head()
67/11:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head()
67/12:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head(0)
67/13:
# Read with Pandas
ufo = pd.read_csv(file)
ufo.head()
67/14:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"

ufo = ufo.df["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]
67/15:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"

ufo = ufo.df[["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]]
ufo()
67/16:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"

ufo = ufo.DatFrame[["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]]
ufo()
67/17:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"

ufo = ufo[["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]]
ufo()
67/18:
# Extract the following columns: "Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", and "Speed"

ufo = ufo[["Type 1", "HP", "Attack", "Sp. Atk", "Sp. Def", "Speed"]]
ufo
68/1: import pandas as pd
68/2: my_list = [1,2,3,4,5,6,7]
68/3:
my_list = [1,2,3,4,5,6,7]
my_series = pd.Series(my_list)
68/4:
my_list = [1,2,3,4,5,6,7]
my_series = pd.Series(my_list)
my_series
68/5: print(my_series[6])
68/6: print(my_series[0,1])
68/7: print(my_series[0:5])
68/8:
 #Creating a Pandas DataFrame by passing in a LIST OF DICTIONARIES
# Each value in the list is a dictionary
# Imagine that each dictionary represents a row of data in our eventual dataframe
# Each dictionary should have the same keys, since these keys dictate the column headers of our dataframe

my_list = [{"id": 1, "name": "Bob", "account_balance": 500.14},
           {"id": 2, "name": "Amanda", "account_balance": 300.42},
           {"id": 3, "name": "Jill", "account_balance": 943.54},
           {"id": 4, "name": "Dylan", "account_balance": 112.53},
           {"id": 5, "name": "Alex", "account_balance": 895.51}]
my_dataframe = pd.DataFrame(my_list)
68/9:
 #Creating a Pandas DataFrame by passing in a LIST OF DICTIONARIES
# Each value in the list is a dictionary
# Imagine that each dictionary represents a row of data in our eventual dataframe
# Each dictionary should have the same keys, since these keys dictate the column headers of our dataframe

my_list = [{"id": 1, "name": "Bob", "account_balance": 500.14},
           {"id": 2, "name": "Amanda", "account_balance": 300.42},
           {"id": 3, "name": "Jill", "account_balance": 943.54},
           {"id": 4, "name": "Dylan", "account_balance": 112.53},
           {"id": 5, "name": "Alex", "account_balance": 895.51}]
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/10:
 #Creating a Pandas DataFrame by passing in a LIST OF DICTIONARIES
# Each value in the list is a dictionary
# Imagine that each dictionary represents a row of data in our eventual dataframe
# Each dictionary should have the same keys, since these keys dictate the column headers of our dataframe

my_list = [{"id": 1, "name": "Bob", "account_balance": 500.14},
           {"id": 2, "name": "Amanda", "account_balance": 300.42},
           {"id": 3, "name": "Jill", "account_balance": 943.54},
           {"id": 4, "name": "Dylan", "account_balance": 112.53},
           {"id": 5, "name": "Alex", "account_balance": 895.51}, index = none]
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/11:
 #Creating a Pandas DataFrame by passing in a LIST OF DICTIONARIES
# Each value in the list is a dictionary
# Imagine that each dictionary represents a row of data in our eventual dataframe
# Each dictionary should have the same keys, since these keys dictate the column headers of our dataframe

my_list = [{"id": 1, "name": "Bob", "account_balance": 500.14},
           {"id": 2, "name": "Amanda", "account_balance": 300.42},
           {"id": 3, "name": "Jill", "account_balance": 943.54},
           {"id": 4, "name": "Dylan", "account_balance": 112.53},
           {"id": 5, "name": "Alex", "account_balance": 895.51}, index =""]
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/12:
 #Creating a Pandas DataFrame by passing in a LIST OF DICTIONARIES
# Each value in the list is a dictionary
# Imagine that each dictionary represents a row of data in our eventual dataframe
# Each dictionary should have the same keys, since these keys dictate the column headers of our dataframe

my_list = [{"id": 1, "name": "Bob", "account_balance": 500.14},
           {"id": 2, "name": "Amanda", "account_balance": 300.42},
           {"id": 3, "name": "Jill", "account_balance": 943.54},
           {"id": 4, "name": "Dylan", "account_balance": 112.53},
           {"id": 5, "name": "Alex", "account_balance": 895.51}]
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/13:
# Re-create the previous Pandas DataFrame, passing in a DICTIONARY WITH LISTS
# The keys of the dictionary represent the column headers of our eventual dataframe
# The lists contain the data for each column
my_dict = {["id"=[1,2,3,4,5], "name"=[Bob,Amanda,Jill,Dylan,Alex], "account_balance"=[550.14,300.42,943.54,112.53,895.51]]}
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/14:
# Re-create the previous Pandas DataFrame, passing in a DICTIONARY WITH LISTS
# The keys of the dictionary represent the column headers of our eventual dataframe
# The lists contain the data for each column
my_dict = {"id":[1,2,3,4,5],"name":[Bob,Amanda,Jill,Dylan,Alex],"account_balance":[550.14,300.42,943.54,112.53,895.51]]}
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/15:
# Re-create the previous Pandas DataFrame, passing in a DICTIONARY WITH LISTS
# The keys of the dictionary represent the column headers of our eventual dataframe
# The lists contain the data for each column
my_dict = {"id":[1,2,3,4,5],"name":[Bob,Amanda,Jill,Dylan,Alex],"account_balance":[550.14,300.42,943.54,112.53,895.51]}
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/16:
# Re-create the previous Pandas DataFrame, passing in a DICTIONARY WITH LISTS
# The keys of the dictionary represent the column headers of our eventual dataframe
# The lists contain the data for each column
my_dict = {"id":[1,2,3,4,5],"name":["Bob","Amanda","Jill","Dylan","Alex"],"account_balance":[550.14,300.42,943.54,112.53,895.51]}
my_dataframe = pd.DataFrame(my_list)
my_dataframe
68/17: my_dataframe["name"]
68/18: my_dataframe["id"]
68/19: my_dataframe["account_balance"]
68/20: my_df = my_dataframe[]
68/21: my_df = my_dataframe
68/22: my_df["name"]
68/23:
name_only = my_df["name"]
print(name_only[])
68/24:
name_only = my_df["name"]
print(name_only)
68/25:
name_only = my_df["name"]
print(name_only[2])
68/26:
name_only = my_df["name"]
print(name_only[:])
68/27:
name_only = my_df["name"]
print(name_only[1:])
68/28: my_df["id"]
68/29: my_df["id", "name"]
68/30: my_df[["id", "name"]]
68/31: my_df[["id", "name","account_balance"]]
68/32:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
68/33:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.head()
68/34: my.df1.decribe()
68/35: my_df1.decribe()
68/36: my_df1.describe()
68/37: my_df1.describe(all)
68/38: my_df1.describe()
68/39:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.head(0)
68/40:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.head(100)
68/41:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.tail(5)
68/42: help.pd.series()
68/43: help(pd.series())
68/44: help(pd.series)
68/45: help(pd.Series)
68/46: help(pd.DataFrame)
68/47: my_df1.dtypes
68/48: my_df1.dtype
68/49: my_df1.dtypes
70/1:
# Note that .describe() will only return summary statistics for your numeric columns
# In this case, statistics for order_id and price columns are returned

purchase_df.describe(include='all')
68/50: my_df1.describe(include='all')
70/2:
# Note that .describe() will only return summary statistics for your numeric columns
# In this case, statistics for order_id and price columns are returned

purchase_df.describe()
70/3:
# Note that .describe() will only return summary statistics for your numeric columns
# In this case, statistics for order_id and price columns are returned

purchase_df.describe()
68/51: my_df1["price"].mean()
68/52: my_df["order_id"].sum()
68/53: my_df1["order_id"].sum()
68/54:
m = my_df1["price"].mean()
s = my_df1["order_id"].sum()
m+s
68/55: my_df1["purchase_category"].unique()
68/56:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.tail(include='all')
68/57:
my_dict2 = {"order_id": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
             "price": [13.50, 9.99, 12.00, 29.99, 
                       14.99, 7.99, 3.49, 10.00, 
                       9.99, 17.99, 20.00, 21.00, 14.99],
             "purchase_category": ["Apparel", "Sports", "Toys", 
                                   "Apparel", "Apparel", "Household", 
                                   "Household", "Toys", "Sports", 
                                   "Sports", "Apparel", "Household", "Apparel"],
             "clicked_ad": [True, True, False, True, False, 
                            True, True, False, False, True, 
                            True, True, False]}
my_df1 = pd.DataFrame(my_dict2)
my_df1.tail()
68/58: my_df1.count()
68/59: my_df1.value_counts
68/60: my_df1.value_counts()
68/61: my_df1.value_count()
68/62: my_df1.value_counts()
68/63: my_df1["order_id"].value_counts()
68/64: my_df1["price"].value_counts()
68/65: my_df1["purchase_category"].value_counts()
68/66: my_df1["purchase_category"].value_counts()
68/67: my_df1["purchase_category"].count()
68/68: my_df1
68/69: my_df1["order_id"]
68/70: my_df1.iloc[:, 1]
68/71: my_df1.iloc[:, 0]
68/72: my_df1.iloc[0:4,:]
68/73: my_df1.iloc[[2,3,5],[2:4]]
68/74: my_df1.iloc[[2,3,5],[2:]]
68/75: my_df1.iloc[[1,2,4],2:4]
68/76: my_df1.iloc[[1,2,4],1:4]
68/77:
new = {"first_name": ["Bill", "James", "Tyler", "Matt", "Jon"],
                "last_name": ["Smith", "Alvarez", "Dant", "May", "Livingston"],
                "age": [25, 34, 52, 26, 43],
                "credit_score": [721, 683, 761, 641, 602]}
new1= pd.DataFrame(new)
new1
68/78: new1=new1.set_index("first_name")
68/79:
new1=new1.set_index("first_name")
new1
68/80:
new1 = new1.set_index("first_name")
new1
68/81: new1 = new1.set_index("first_name")
68/82:
new1 = new1.set_index("first_name")
new1
68/83:
new1 = new1.set_index('first_name')
new1
68/84:
new1 = new1.set_index("last_name")
new1
68/85:
new1 = new1.set_index("first_name")
new1
68/86:
new1 = new1.set_index("last_name")
new1
68/87:
new1 = new1.set_index("last_name")
new1
68/88: import pandas as pd
68/89:
new = {"first_name": ["Bill", "James", "Tyler", "Matt", "Jon"],
                "last_name": ["Smith", "Alvarez", "Dant", "May", "Livingston"],
                "age": [25, 34, 52, 26, 43],
                "credit_score": [721, 683, 761, 641, 602]}
new1= pd.DataFrame(new)
new1
68/90:
new1 = new1.set_index("last_name")
new1
68/91:
new1 = new1.set_index("first_name")
new1
68/92: new1.loc[["James","Tyler"],:]
68/93: new1.loc[["Bill":"Matt"],["age","credit_score"]]
68/94: new1.loc["Bill":"Matt",["age","credit_score"]]
74/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
months = 0
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        months += 1 
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])

        

    print("Financial Analysis")
    print("----------------------------------")
    print("Total Months:" + str(months))
    print("NetAmount:" + str(sum(revenue)))
    print("Average Change:" + str(sum(avgchange)/(months)-1))
    print("Greatest Increase in Profits:" + str("$"))
74/2:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
# Declare variables
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    print(min_index)
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])

    print("Financial Analysis")
    print("----------------------------------")
    print("Total Months:" + str(len(month_list)))
    print("Total:" + str(sum(revenue)))
    print("Average Change:" + str("$") + str(sum(avgchange)/(len(month_list)-1)))
    print("Greatest Increase in Profits:" + str(greatest_month) + str("$") + str(max_index))
77/1: bitcoin_df.head()
77/2:
# Import Dependencies
import pandas as pd
77/3: bitcoin_df.head()
77/4:
bitcoin_df = pd.read_csv(bitcoin_csv)
dash_df = pd.read_csv(dash_csv)
77/5:
# Import Dependencies
import pandas as pd
77/6:
bitcoin_csv = "Resources/bitcoin_cash_price.csv"
dash_csv = "Resources/dash_price.csv"
77/7:
bitcoin_df = pd.read_csv(bitcoin_csv)
dash_df = pd.read_csv(dash_csv)
77/8: bitcoin_df.head()
77/9: dash_df.head()
77/10:
# Merge the two DataFrames together based on the Dates they share_
merged_df = pd.merge(bitcoin_df, dash_df,on="Date" )
77/11:
# Merge the two DataFrames together based on the Dates they share_
merged_df = pd.merge(bitcoin_df, dash_df,on="Date" )
merged_df
77/12:
# Rename columns so that they are differentiated
merged_df.count()
77/13:
# Rename columns so that they are differentiated
type(merged_df)
77/14:
# Rename columns so that they are differentiated
merged_df = merged_df.rename({"Open_x":"Open_bit_coin","Close_x":"Close_bit_coin",
                             "High_x":"High_bit_coin","Low_x":"Low_bit_coin",
                             "Volume_x":"Volume_bit_coin","Market Cap_x":"Market_bit_coin",
                             "Open_x":"Open_dash",
                             "Close_x":"Close_dash","High_x":"High_dash",
                             "Low_x":"Low_dash","Volume_x":"Volume_dash",
                             "Market Cap_x":"Market_dash"})
77/15:
# Rename columns so that they are differentiated
merged_df = merged_df.rename({"Open_x":"Open_bit_coin","Close_x":"Close_bit_coin",
                             "High_x":"High_bit_coin","Low_x":"Low_bit_coin",
                             "Volume_x":"Volume_bit_coin","Market Cap_x":"Market_bit_coin",
                             "Open_x":"Open_dash",
                             "Close_x":"Close_dash","High_x":"High_dash",
                             "Low_x":"Low_dash","Volume_x":"Volume_dash",
                             "Market Cap_x":"Market_dash"})
77/16:
# Rename columns so that they are differentiated
merged_df = merged_df.rename({"Open_x":"Open_bit_coin","Close_x":"Close_bit_coin",
                             "High_x":"High_bit_coin","Low_x":"Low_bit_coin",
                             "Volume_x":"Volume_bit_coin","Market Cap_x":"Market_bit_coin",
                             "Open_x":"Open_dash",
                             "Close_x":"Close_dash","High_x":"High_dash",
                             "Low_x":"Low_dash","Volume_x":"Volume_dash",
                             "Market Cap_x":"Market_dash"})
merged_df
77/17:
# Rename columns so that they are differentiated
merged_df = merged_df.rename(columns={"Open_x":"Open_bit_coin","Close_x":"Close_bit_coin",
                             "High_x":"High_bit_coin","Low_x":"Low_bit_coin",
                             "Volume_x":"Volume_bit_coin","Market Cap_x":"Market_bit_coin",
                             "Open_x":"Open_dash",
                             "Close_x":"Close_dash","High_x":"High_dash",
                             "Low_x":"Low_dash","Volume_x":"Volume_dash",
                             "Market Cap_x":"Market_dash"})
merged_df
77/18:
# Rename columns so that they are differentiated
merged_df = merged_df.rename(columns={"Open_x":"Open_bit_coin","Close_x":"Close_bit_coin",
                             "High_x":"High_bit_coin","Low_x":"Low_bit_coin",
                             "Volume_x":"Volume_bit_coin","Market Cap_x":"Market_bit_coin",
                             "Open_y":"Open_dash",
                             "Close_y":"Close_dash","High_y":"High_dash",
                             "Low_y":"Low_dash","Volume_y":"Volume_dash",
                             "Market Cap_y":"Market_dash"})
merged_df
77/19:
# Rename columns so that they are differentiated
merged_df = merged_df.rename(columns={"Open_x":"Open_bitcoin","Close_x":"Close_bitcoin",
                             "High_x":"High_bitcoin","Low_x":"Low_bitcoin",
                             "Volume_x":"Volume_bitcoin","Market Cap_x":"Market_bitcoin",
                             "Open_y":"Open_dash",
                             "Close_y":"Close_dash","High_y":"High_dash",
                             "Low_y":"Low_dash","Volume_y":"Volume_dash",
                             "Market Cap_y":"Market_dash"})
merged_df
77/20:
# Import Dependencies
import pandas as pd
77/21:
bitcoin_csv = "Resources/bitcoin_cash_price.csv"
dash_csv = "Resources/dash_price.csv"
77/22:
bitcoin_df = pd.read_csv(bitcoin_csv)
dash_df = pd.read_csv(dash_csv)
77/23: bitcoin_df.head()
77/24:
# Rename columns so that they are differentiated
merged_df = merged_df.rename(columns={"Open_x":"Open_bitcoin","Close_x":"Close_bitcoin",
                             "High_x":"High_bitcoin","Low_x":"Low_bitcoin",
                             "Volume_x":"Volume_bitcoin","Market Cap_x":"Market_bitcoin",
                             "Open_y":"Open_dash",
                             "Close_y":"Close_dash","High_y":"High_dash",
                             "Low_y":"Low_dash","Volume_y":"Volume_dash",
                             "Market Cap_y":"Market_dash"})
merged_df
77/25:
# Merge the two DataFrames together based on the Dates they share_
merged_df = pd.merge(bitcoin_df, dash_df,on="Date" )
merged_df.head()
77/26:
# Rename columns so that they are differentiated
merged_df = merged_df.rename(columns={"Open_x":"Open_bitcoin","Close_x":"Close_bitcoin",
                             "High_x":"High_bitcoin","Low_x":"Low_bitcoin",
                             "Volume_x":"Volume_bitcoin","Market Cap_x":"Market_bitcoin",
                             "Open_y":"Open_dash",
                             "Close_y":"Close_dash","High_y":"High_dash",
                             "Low_y":"Low_dash","Volume_y":"Volume_dash",
                             "Market Cap_y":"Market_dash"})
merged_df.head()
82/1:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
82/2:
# Import Dependencies
import pandas as pd
82/3:
# Create a path to the csv and read it into a Pandas DataFrame
csv_path = "Resources/ted_talks.csv"
ted_df = pd.read_csv(csv_path)

ted_df.head()
82/4: # Figure out the minimum and maximum views for a TED Talk
82/5:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/6:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/7:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/8:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/9:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
df["views"].min()
82/10:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
df["views"].min()
82/11:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/12: df["views"].min()
82/13:
# Slice the data and place it into bins
df[views] = pd.cut(df["languages"], bins, labels = label_name)
df
82/14:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/15:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e","f",]
82/16:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/17:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/18:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/19:
# Import Dependencies
import pandas as pd
82/20:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/21:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/22: # Place the data series into a new column inside of the DataFrame
82/23:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/24:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e"]
82/25:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df
82/26:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df.head()
82/27:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")

# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
82/28:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")

# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
82/29:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df

# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
82/30:
# Import Dependencies
import pandas as pd
82/31:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/32:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e"]
82/33:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df.head()
82/34:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df

# Find how many rows fall into each bin


# Get the average of each column within the GroupBy object
82/35:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].mean()

df = 
# Get the average of each column within the GroupBy object
82/36:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].mean()
df

df = 
# Get the average of each column within the GroupBy object
82/37:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df = df["views"].mean()
df

df = 
# Get the average of each column within the GroupBy object
82/38:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
# Get the average of each column within the GroupBy object
82/39:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
# Get the average of each column within the GroupBy object
82/40:
# Import Dependencies
import pandas as pd
82/41:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/42:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/43: df["views"].min()
82/44:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e"]
82/45:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df.head()
82/46:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
# Get the average of each column within the GroupBy object
82/47:
# Get the average of each column within the GroupBy object
df["views"].mean()
82/48:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].couny
82/49:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].count
82/50:
# Import Dependencies
import pandas as pd
82/51:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/52:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/53: df["views"].min()
82/54:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e"]
82/55:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df.head()
82/56: # Place the data series into a new column inside of the DataFrame
82/57:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].count
82/58:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].count
df
82/59:
# Import Dependencies
import pandas as pd
82/60:
# Create a path to the csv and read it into a Pandas DataFrame
file = "Resources/ted_talks.csv"
df = pd.read_csv(file)

df.head()
82/61:
# Figure out the minimum and maximum views for a TED Talk
df["views"].max()
82/62: df["views"].min()
82/63:
# Create bins in which to place values based upon TED Talk views
bins = [1000, 10000, 100000, 1000000, 10000000, 100000000]
# Create labels for these bins
label_name = ["a","b","c","d","e"]
82/64:
# Slice the data and place it into bins
df["test_data"] = pd.cut(df["views"], bins, labels=label_name)
df.head()
82/65: # Place the data series into a new column inside of the DataFrame
82/66:
# Create a GroupBy object based upon "View Group"
df = df.groupby("views")
df
# Find how many rows fall into each bin
df["views"].count
df
82/67:
# Get the average of each column within the GroupBy object
df["views"].mean()
78/1:
# Get a list of all of our columns for easy reference
df.head()
78/2: import pandas as pd
78/3:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
78/4:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
78/5: import pandas as pd
78/6:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
78/7: import pandas as pd
78/8:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
78/9:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
78/10:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
78/11:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head()
78/12:
# The path to our CSV file
file = "Resources/KickstarterData"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head()
78/13:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head()
78/14:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/15:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]]
78/16:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]]
df.head()
78/17:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]]
78/18:
# Get a list of all of our columns for easy reference
df.column()
78/19:
# Get a list of all of our columns for easy reference
df.columns
78/20:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/21:
# Remove projects that made no money at all df.count()- df = df.dropna(how='any', subset =["LastName", "FirstName"])-df.count()
df.count()
78/22:
# Remove projects that made no money at all
df.count()
del df["is_starred","is_backing","permissions"]
78/23:
# Remove projects that made no money at all
df.count()
del df["is_starred","is_backing","permissions"]
df.head()
78/24: import pandas as pd
78/25:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/26:
# Get a list of all of our columns for easy reference
df.columns
78/27:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/28:
# Remove projects that made no money at all
df.count()
del df["is_starred","is_backing","permissions"]
df.head()
78/29:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
df.head()
78/30:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
df.head()
df.count()
78/31: import pandas as pd
78/32:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/33:
# Get a list of all of our columns for easy reference
df.columns
78/34:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/35:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
df.count()
78/36:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
def df["friends"]
df.count()
78/37: import pandas as pd
78/38:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/39:
# Get a list of all of our columns for easy reference
df.columns
78/40:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/41:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
78/42:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(how='any',"pledged" )
78/43: import pandas as pd
78/44:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/45:
# Get a list of all of our columns for easy reference
df.columns
78/46:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/47:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(how='any',"pledged")
78/48:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(how='any')
80/1:

# Drop all rows with missing information
df = df.dropna(hsubset="pledged")
#df = df.dropna(how='any', subset =["LastName", "FirstName"])
80/2:
# Dependencies
import pandas as pd
import numpy as np
80/3:
# Name of the CSV file
file = 'Resources/donors2008.csv'
80/4:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
80/5:
# Preview of the DataFrame
# Note that FIELD8 is likely a meaningless column
df.head()
80/6:

# Drop all rows with missing information
df = df.dropna(hsubset="pledged")
#df = df.dropna(how='any', subset =["LastName", "FirstName"])
80/7:
# Verify dropped rows
df.count()
#fills in any rows or columns- need to mention the column names for specific columns
df = df.fillna("")
80/8:

# Drop all rows with missing information
df = df.dropna(hsubset="pledged")
#df = df.dropna(how='any', subset =["LastName", "FirstName"])
80/9:

# Drop all rows with missing information
df = df.dropna(hsubset="pledged")
df
#df = df.dropna(how='any', subset =["LastName", "FirstName"])
80/10:
# Dependencies
import pandas as pd
import numpy as np
80/11:
# Name of the CSV file
file = 'Resources/donors2008.csv'
80/12:
# The correct encoding must be used to read the CSV in pandas
df = pd.read_csv(file, encoding="ISO-8859-1")
80/13:
# Preview of the DataFrame
# Note that FIELD8 is likely a meaningless column
df.head()
80/14:
# Use pd.to_numeric() method to convert the datatype of the Amount column
#overwrites values in a column with values
df['Amount'] = pd.to_numeric(df['Amount'])
80/15:
# Verify that the Amount column datatype has been made numeric
#
df['Amount'].dtype
80/16:
# Display an overview of the Employers column
df['Employer'].value_counts()
80/17:
# replace renames columns- works on series
#Clean up Employer category. Replace 'Self Employed' and 'Self' with 'Self-Employed'
df['Employer'] = df['Employer'].replace(
    {'Self Employed': 'Self-Employed', 'Self': 'Self-Employed'})
80/18:
# Verify clean-up.
df['Employer'].value_counts()
78/49:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(subset="pledged" )
78/50:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(subset="pledged")
df
78/51: import pandas as pd
78/52:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/53:
# Get a list of all of our columns for easy reference
df.columns
78/54:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/55:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(subset="pledged")
df
78/56:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
df = df.dropna(subset="pledged")
df
78/57:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
#df = df.dropna(subset="pledged")
#df
78/58:
# Remove projects that made no money at all
df.count()
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
#df = df.dropna(subset="pledged")
#df
78/59: import pandas as pd
78/60:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/61:
# Get a list of all of our columns for easy reference
df.columns
78/62:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/63:
# Remove projects that made no money at all
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]
df.count()
#df = df.dropna(subset="pledged")
#df
80/19:

# Drop all rows with missing information
df = df.dropna(how='any')
#df = df.dropna(how='any', subset =["LastName", "FirstName"])
78/64:
# Remove projects that made no money at all
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]

df = df.dropna(subset="pledged")
df.count()
78/65:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(20)
78/66:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(35)
78/67:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(50)
78/68:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(100)
78/69:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(200)
78/70:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head(500)
78/71:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/72:
# Remove projects that made no money at all
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]

#df = df.dropna(axis=0, how='any')
df[df.pledged != 0]
df.count()
78/73: import pandas as pd
78/74:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/75:
# Get a list of all of our columns for easy reference
df.columns
78/76:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/77:
# Remove projects that made no money at all
del df["is_starred"]
del df["is_backing"]
del df["permissions"]
del df["friends"]

#df = df.dropna(axis=0, how='any')
df[df.pledged != 0]
df.count()
78/78:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
#df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
#alternative 
new_df = df.loc[:,"name","goal","pledged","state","country","staff_pick","backers_count","spotlight" ].head()
78/79: import pandas as pd
78/80:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/81:
# Get a list of all of our columns for easy reference
df.columns
78/82:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
#df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
#alternative 
new_df = df.loc[:,"name","goal","pledged","state","country","staff_pick","backers_count","spotlight" ].head()
78/83: import pandas as pd
78/84:
# The path to our CSV file
file = "Resources/KickstarterData.csv"
# Read our Kickstarter data into pandas
df = pd.read_csv(file)
df.head(3)
78/85:
# Get a list of all of our columns for easy reference
df.columns
78/86:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
#df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
#alternative 
new_df = df.loc[:,["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
78/87:
# Extract "name", "goal", "pledged", "state", "country", "staff_pick",
# "backers_count", and "spotlight"
#df[["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
#alternative 
new_df = df.loc[:,["name","goal","pledged","state","country","staff_pick","backers_count","spotlight"]].head()
new_df
84/1:
# import libraries
import os
import csv

# set file path
csvpath = os.path.join('Resources', 'election_data_1.csv')

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(csvpath, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/2:
# import libraries
import os
import csv

# set file path
csvpath = os.path.join("Resources/election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(csvpath, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/3:
# import libraries
import os
import csv

# set file path
csvpath = os.path.join("Resources/election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(csvpath, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/4:
# import libraries
import os
import csv

# set file path
csvpath = os.path.join("Resources\election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(csvpath, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/5:
# import libraries
import os
import csv

# set file path
csvpath = os.path.join("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(csvpath, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/6:
# import libraries
import os
import csv

# set file path
file = ("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(file, newline = "") as csvfile:
  csvreader = csv.reader(csvfile, delimiter = ",")
  next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/7:
# import libraries
import os
import csv

# set file path
file = ("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(file, newline = "") as csvfile:
    csvreader = csv.reader(csvfile, delimiter = ",")
    next(csvreader, None)

  # finding total vote count; finding individual vote counts
  for row in csvreader:
      vote_count += 1
      if row[2] in candidates.keys():
          candidates[row[2]] += 1
      else:
          candidates[row[2]] = 1

# percentages for each candidate
for key, value in candidates.items():
  candidates_percent[key] = round((value/vote_count) * 100, 2)

# finding the winner
for key in candidates.keys():
  if candidates[key] > winner_count:
      winner = key
      winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
print("Election Results")
print("-------------------------------------")
print("Total Votes: " + str(vote_count))
print("-------------------------------------")
for key, value in candidates.items():
  print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
print("-------------------------------------")
print("Winner: " + winner)
print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/8:
# import libraries
import os
import csv

# set file path
file = ("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(file, newline = "") as csvfile:
    csvreader = csv.reader(csvfile, delimiter = ",")
    next(csvreader, None)
    for row in csvreader:
        vote_count += 1
        if row[2] in candidates.keys():
            candidates[row[2]] += 1
        else:
            candidates[row[2]] = 1
    for key, value in candidates.items():
        candidates_percent[key] = round((value/vote_count) * 100, 2)
    for key in candidates.keys():
        if candidates[key] > winner_count:
            winner = key
            winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
    print("Election Results")
    print("-------------------------------------")
    print("Total Votes: " + str(vote_count))
    print("-------------------------------------")
    for key, value in candidates.items():
        print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
        print("-------------------------------------")
        print("Winner: " + winner)
        print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/9:
# import libraries
import os
import csv

# set file path
file = ("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(file) as csvfile:
    csvreader = csv.reader(csvfile, delimiter = ",")
    next(csvreader, None)
    for row in csvreader:
        vote_count += 1
        if row[2] in candidates.keys():
            candidates[row[2]] += 1
        else:
            candidates[row[2]] = 1
    for key, value in candidates.items():
        candidates_percent[key] = round((value/vote_count) * 100, 2)
    for key in candidates.keys():
        if candidates[key] > winner_count:
            winner = key
            winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
    print("Election Results")
    print("-------------------------------------")
    print("Total Votes: " + str(vote_count))
    print("-------------------------------------")
    for key, value in candidates.items():
        print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
        print("-------------------------------------")
        print("Winner: " + winner)
        print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/10:
# import libraries
import os
import csv

# set file path
file = ("Resources","election_data.csv")

# declaring variables
vote_count = 0
candidates = {}
candidates_percent = {}
winner = ""
winner_count = 0

# reading file
with open(file, newline = " ") as csvfile:
    csvreader = csv.reader(csvfile, delimiter = ",")
    next(csvreader, None)
    for row in csvreader:
        vote_count += 1
        if row[2] in candidates.keys():
            candidates[row[2]] += 1
        else:
            candidates[row[2]] = 1
    for key, value in candidates.items():
        candidates_percent[key] = round((value/vote_count) * 100, 2)
    for key in candidates.keys():
        if candidates[key] > winner_count:
            winner = key
            winner_count = candidates[key]

# tests
# print(total_vote_count)
# print(candidates)
# print(candidates_percent)
# print(winner)

# printing to the terminal
    print("Election Results")
    print("-------------------------------------")
    print("Total Votes: " + str(vote_count))
    print("-------------------------------------")
    for key, value in candidates.items():
        print(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ")")
        print("-------------------------------------")
        print("Winner: " + winner)
        print("-------------------------------------")

# creating new text file
new_file = open("Output/results_1.txt", "w")

# writing the new file
new_file.write("Election Results \n")
new_file.write("------------------------------------- \n")
new_file.write("Total Votes: " + str(vote_count) + "\n")
new_file.write("------------------------------------- \n")
for key, value in candidates.items():
  new_file.write(key + ": " + str(candidates_percent[key]) + "% (" + str(value) + ") \n")
new_file.write("------------------------------------- \n")
new_file.write("Winner: " + winner + "\n")
new_file.write("------------------------------------- \n")
84/11: !pwd
85/1: !pwd
85/2:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index+1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$"(min_index)))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
85/3:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index+1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$"(min_index)))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
85/4:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index+1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$"(min_index)))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
85/5:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index+1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index)))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
85/6:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index+1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
85/7:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/2:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank","budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/3:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
88/1:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   #Initialize variables to empty lists and dictionary
   total_votes = 0
   candidates = []
   candidates_dict = {}
   #winning_votes = []
   next(csvreader)
   #go line by line and process each vote
   for row in csvreader:
       # add to total number of votes
       total_votes += 1
       if row[2] not in candidates:
           #add candidates to the empty list
           candidates.append(row[2])
           #to make candidate name as the key
           candidates_dict[row[2]] = 0
           candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    

   print('Election Results\n')
   print('---------------------------\n')
   print("Total Votes: " + str(total_votes) + '\n')
   print('----------------------------\n')
   print(str(candidates_dict) + '\n')
   print('----------------------------\n')
   #print("Winner: ")
   print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
87/4:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/5:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/6:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + float(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$")(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/7:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_index) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_index))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/8:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))

#opens the output file in r mode and prints to terminal
with open(output, 'r') as readfile:
    print(readfile.read())
87/9:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))
87/10:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))
87/11:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_index) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_index))
87/12:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(min_change))
87/13:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$  ') + str(max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$  ') + str(min_change))
87/14:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('$') + str(  max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('$') + str(  min_change))
87/15:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(  max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(  min_change))
87/16:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(  max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(  min_change))
87/17:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(" "max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(" "min_change))
87/18:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#"
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(min_change))
88/2:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   #Initialize variables to empty lists and dictionary
   total_votes = 0
   candidates = []
   candidates_dict = {}
   #winning_votes = []
   next(csvreader)
   #go line by line and process each vote
   for row in csvreader:
       # add to total number of votes
       total_votes += 1
       if row[2] not in candidates:
           #add candidates to the empty list
           candidates.append(row[2])
           #to make candidate name as the key
           candidates_dict[row[2]] = 0
           candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    

   print('Election Results\n')
   print('---------------------------\n')
   print("Total Votes: " + str(total_votes) + '\n')
   print('----------------------------\n')
   print(str(candidates_dict) + '\n')
   print('----------------------------\n')
   #print("Winner: ")
   print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/3:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
   csvreader = csv.reader(csvfile, delimiter=",")
   #Initialize variables to empty lists and dictionary
   total_votes = 0
   candidates = []
   candidates_dict = {}
   #winning_votes = []
   next(csvreader)
   #go line by line and process each vote
   for row in csvreader:
       # add to total number of votes
       total_votes += 1
       if row[2] not in candidates:
           #add candidates to the empty list
           candidates.append(row[2])
           #to make candidate name as the key
           candidates_dict[row[2]] = 0
           candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    

   print('Election Results\n')
   print('---------------------------\n')
   print("Total Votes: " + str(total_votes) + '\n')
   print('----------------------------\n')
   print(str(candidates_dict) + '\n')
   print('----------------------------\n')
   #print("Winner: ")
   print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/4:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    #winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

   print('Election Results\n')
   print('---------------------------\n')
   print("Total Votes: " + str(total_votes) + '\n')
   print('----------------------------\n')
   print(str(candidates_dict) + '\n')
   print('----------------------------\n')
   #print("Winner: ")
   print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/5:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    #winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates_dict) + '\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/6:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    #winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates_dict) + '\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/7:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    #winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates_dict) + '\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/8:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    #winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/9:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/10:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/11:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/12:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/13:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/14:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/15:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = []
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/16:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/17:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
88/18:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
90/1:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    
#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
90/2:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')
#open new file
#new_file = open("output_PyPoll.txt", "w")

# writing the new file
#new_file.write("Election Results \n")
#new_file.write("------------------------------------- \n")
#new_file.write("Total Votes: " + str(total_votes) + "\n")
#new_file.write("------------------------------------- \n")
#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')
   
   #closing of file
#new_file.close()

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    
#sets path for output file
#filepath = os.path.join('raw_data',file_name)
#output = os.path.join("Output_PyPoll", "output.txt")
# opens the output destination in write mode and prints the summary
#with open(output, 'w') as writefile:
    #writefile.writelines('Election Results\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines('Total Votes: ' + str(total_votes) + '\n')
    #writefile.writelines('------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('----------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('----------------------------\n')

#opens the output file in r mode and prints to terminal
#with open(output, 'r') as readfile:
    #print(readfile.read())
90/3:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Election Results \n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines('Total Votes: " + str(len(total_votes)) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines(str(candidates_dict) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines("Winner: ")
    writefile.writelines('-------------------------------\n')
90/4:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Election Results \n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines('Total Votes: " + str(len(total_votes)) + '\n'
    writefile.writelines('-------------------------------\n')
    writefile.writelines(str(candidates_dict) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines("Winner: ")
    writefile.writelines('-------------------------------\n')
90/5:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Election Results \n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines('Total Votes: " + str(len(total_votes)) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines(str(candidates_dict) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines("Winner: ")
    writefile.writelines('-------------------------------\n')
90/6:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Election Results \n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines(str(candidates_dict) + '\n')
    writefile.writelines('-------------------------------\n')
    writefile.writelines("Winner: ")
    writefile.writelines('-------------------------------\n')
91/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(min_change))
90/7:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/8:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates) + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/9:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = {}
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
            
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = votes / total_votes
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates) + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/10:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/11:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/12:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/13:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = []
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        #if votes > winning_votes:
            #winning_votes = votes
            #winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')

print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
90/14:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')

print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
104/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = ("/schools_complete.csv")
students = ("/students_complete.csv")

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
Complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','student_name'])
complete_school_data
104/2:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
Complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','student_name'])
complete_school_data
104/3:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
Complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','student_name'])
complete_school_data
104/4:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
Complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data
104/5:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
Complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data()
104/6:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data
104/7:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/8:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head(25)
104/9:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/10:
schools_total = df[school_name].sum()
schools_total
104/11:
schools_total = complete_school_data[school_name].sum()
schools_total
104/12:
schools_total = complete_school_data["school_name"].sum()
schools_total
104/13:
schools_total = complete_school_data["school_name"].sum()
print(schools_total)
104/14:
schools_total = complete_school_data["school_name"].sum()
print(schools_total)
104/15:
schools_total = complete_school_data["school_name"].sum()
schools_total
104/16:
schools_total = complete_school_data["school_name"].sum()
schools_total,head()
104/17:
schools_total = complete_school_data["school_name"].sum()
schools_total.head()
104/18:
schools_total = complete_school_data["school_name"].sum()
schools_total
104/19:
schools_total = complete_school_data[axis=1].sum()
schools_total
#students_total = complete_school_data["student_name"].sum()
104/20:
schools_total = complete_school_data.sum(axis=1)
schools_total
#students_total = complete_school_data["student_name"].sum()
104/21:
schools_total = complete_school_data.sum(axis=0)
schools_total
#students_total = complete_school_data["student_name"].sum()
104/22:
schools_total = complete_school_data["school_name"].sum()
schools_total
#students_total = complete_school_data["student_name"].sum()
104/23:
schools_total = complete_school_data["school_name"].sum()
schools_total
students_total = complete_school_data["student_name"].sum()
students_total
104/24:
# schools_total = complete_school_data["school_name"].sum()
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
school_name = complete_school_data.groupby["school_name"].sum()
school_name
104/25:
# schools_total = complete_school_data["school_name"].sum()
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
school_name = complete_school_data.groupby.sum()
school_name
104/26:
# schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    schools_total
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/27:
# schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    print(schools_total)
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/28:
# schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    print(str(schools_total))
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/29:
# schools_total = 0 
for row in complete_school_data:
    str(schools_total)=str(schools_total)+1
    print(str(schools_total))
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/30:
schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    print(schools_total)
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/31:
schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    schools_total
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/32:
schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    schools_total
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/33:
schools_total = 0 
for row in complete_school_data:
    schools_total=schools_total+1
    schools_total
    
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/34:
schools_total = 0 
students_total = 0
total_budget = 0
for row in complete_school_data:
    schools_total=schools_total+1
    schools_total
    students_total=students_total+1
    students_total
    total_budget = total_budget+1
    total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/35:
schools_total = 0 
students_total = 0
total_budget = 0
for row in complete_school_data:
    schools_total=schools_total+1
    print schools_total
    students_total=students_total+1
    students_total
    total_budget = total_budget+1
    total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/36:
schools_total = 0 
students_total = 0
total_budget = 0
for row in complete_school_data:
    schools_total=schools_total+1
    print(schools_total)
    students_total=students_total+1
    students_total
    total_budget = total_budget+1
    total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
104/37:
# schools_total = 0 
# students_total = 0
# total_budget = 0
# for row in complete_school_data:
#     schools_total=schools_total+1
#     print(schools_total)
#     students_total=students_total+1
#     students_total
#     total_budget = total_budget+1
#     total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
sum_all = complete_school_data["school_name","student_name","budget"].sum()
sum_all
104/38:
# schools_total = 0 
# students_total = 0
# total_budget = 0
# for row in complete_school_data:
#     schools_total=schools_total+1
#     print(schools_total)
#     students_total=students_total+1
#     students_total
#     total_budget = total_budget+1
#     total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
sum_all = complete_school_data["school_name"].sum()
sum_all
104/39:
# schools_total = 0 
# students_total = 0
# total_budget = 0
# for row in complete_school_data:
#     schools_total=schools_total+1
#     print(schools_total)
#     students_total=students_total+1
#     students_total
#     total_budget = total_budget+1
#     total_budget
# schools_total
# students_total = complete_school_data["student_name"].sum()
# students_total
sum_all = complete_school_data["school_name"].sum()
print(sum_all)
104/40:
# schools_total = 0 
# students_total = 0
# total_budget = 0
# for row in complete_school_data:
#     schools_total=schools_total+1
#     print(schools_total)
#     students_total=students_total+1
#     students_total
#     total_budget = total_budget+1
#     total_budget
# schools_total
students_total = complete_school_data["student_name"].sum()
students_total
104/41:
# schools_total = 0 
# students_total = 0
# total_budget = 0
# for row in complete_school_data:
#     schools_total=schools_total+1
#     print(schools_total)
#     students_total=students_total+1
#     students_total
#     total_budget = total_budget+1
#     total_budget
# schools_total
students_total = complete_school_data["student_name"].count()
students_total
104/42:
# The count method adds every entry in that particular series
school_total = complete_school_data["school_name].count()
school_total
student_total = complete_school_data["student_name"].count()
student_total
104/43:
# The count method adds every entry in that particular series
school_total = complete_school_data["school_name"].count()
school_total
student_total = complete_school_data["student_name"].count()
student_total
104/44:
# The count method adds every entry in that particular series
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/45:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/46:
# The count method adds every entry in that particular series
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/47:
# The count method adds every entry in that particular series
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/48:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/49:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/50:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/51:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = complete_school_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/52:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = complete_school_data["school_name"].count()
print(school_total)
# student_total = complete_school_data["student_name"].count()
# print(student_total)
104/53:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
# student_total = complete_school_data["student_name"].count()
# print(student_total)
104/54:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
104/55:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data.["school_name"].count()
budget_total
104/56:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["school_name"].count()
budget_total
104/57:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["school_name"].sum()
budget_total
104/58:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
104/59:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
avg_math = students_data["math_score"].mean()
avg_math
104/60:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
avg_math = students_data["math_score"].mean()
avg_math
104/61:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
# avg_math = students_data["math_score"].mean()
# avg_math
104/62:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
avg_math = students_data["math_score"].mean()
avg_math
104/63:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/64:
# The count method adds every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
budget_total = schools_data["budget"].sum()
budget_total
avg_math = students_data["math_score"].mean()
avg_math
104/65:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
avg_math
104/66:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_mathreading)
avg_ = students_data["reading_score"].mean()
print(avg_reading)
104/67:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_mathreading)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
104/68:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
104/69:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
104/70:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = student_data.loc[student_data["math_score"]>=70]
stu_math_70["math_score"].count()
stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/71:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = student_data.loc[student_data["math_score"]>=70]
stu_math_70["math_score"].count()
#stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/72:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = student_data.loc[students_data["math_score"]>=70]
stu_math_70["math_score"].count()
#stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/73:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
104/74:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = student_data.loc[students_data["math_score"]>=70]
stu_math_70["math_score"].count()
#stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/75:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
stu_math_70["math_score"].count()
#stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/76:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
stu_math_70["math_score"].count()
percent_math = (stu_math_70/students_total)*100
print(percent_math)
#stu_percent_math = (no.of students having score greater than 70/student_total)*100
104/77:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
#percentage os students with >=70 = (no.of students having score greater than 70/students_total)*100
stu_math_70 = students_data.loc[students_data["math_score"]>=70].count()
#stu_math_70["math_score"].count()
#percent_math = (stu_math_70/students_total)*100
#print(percent_math)
104/78:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
#percentage os students with >=70 = (no.of students having score greater than 70/students_total)*100
stu_math_70 = students_data.loc[students_data["math_score"]>=70].count()
print(stu_math_70)
#stu_math_70["math_score"].count()
#percent_math = (stu_math_70/students_total)*100
#print(percent_math)
104/79:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
#percentage os students with >=70 = (no.of students having score greater than 70/students_total)*100
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
stu_math_70["math_score"].count()
#stu_math_70["math_score"].count()
#percent_math = (stu_math_70/students_total)*100
#print(percent_math)
104/80:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
#percentage os students with >=70 = (no.of students having score greater than 70/students_total)*100
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
percent_math = (stu_math_70["math_score"].count()/student_total)*100
#stu_math_70["math_score"].count()
#percent_math = (stu_math_70/students_total)*100
#print(percent_math)
104/81:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#Calculate the percentage of students with a passing math score (70 or greater)
#percentage os students with >=70 = (no.of students having score greater than 70/students_total)*100
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
percent_math = (stu_math_70["math_score"].count()/student_total)*100
print(percent_math)
#stu_math_70["math_score"].count()
#percent_math = (stu_math_70/students_total)*100
#print(percent_math)
104/82:
# The count method counts every entry in that particular series: var=df["column_name"].count()
school_total = schools_data["school_name"].count()
print(school_total)
student_total = complete_school_data["student_name"].count()
print(student_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/student_total)*100
print(percent_math)
stu_reading_70 = student_data.loc[student_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/student_total)*100
print(percent_reading)
104/83:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
print(schools_total)
students_total = students_data["student_name"].count()
print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
print(percent_reading)
104/84:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
print(schools_total)
students_total = students_data["student_name"].count()
print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
print(percent_reading)
first_school_df = pd.DataFrame{"Total Schools":"schools_total", "Total Students": "students_total",
                               "Total Budget": "budget_total", "Average Math Score": "avg_math", "Average Reading Score": "avg_reading",
                              "%Passing Math": "percent_math", "%Passing Reading": "percent_reading", "%Overall Passing Rate": "overall_score"}
first_school_df
104/85:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
print(schools_total)
students_total = students_data["student_name"].count()
print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":"schools_total", "Total Students": "students_total",
                               "Total Budget": "budget_total", "Average Math Score": "avg_math", "Average Reading Score": "avg_reading",
                              "%Passing Math": "percent_math", "%Passing Reading": "percent_reading", "%Overall Passing Rate": "overall_score"})
first_school_df
104/86:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
print(schools_total)
students_total = students_data["student_name"].count()
print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
print(avg_math)
avg_reading= students_data["reading_score"].mean()
print(avg_reading)
overall_score = (avg_math + avg_reading)/2
print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
first_school_df
104/87:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)
students_total = students_data["student_name"].count()
#print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
#print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
#print(avg_math)
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)
overall_score = (avg_math + avg_reading)/2
#print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
first_school_df
118/1:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')

print('----------------------------\n')
#print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/2:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/3:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/4:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates_dict[row[2]])
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/5:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            #candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/6:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/7:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2],:] = candidates_dict[row[2],:] + 1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/8:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates.keys():
            #add candidates to the empty list
            candidates.append(row[2])
            #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/9:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = 0
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        print(candidates)
        #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#     print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/10:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        if row[2] in candidates:
            votes.append(row[2])
print(candidates)
print(votes)
        #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#     print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/11:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #if row[2] in candidates:
           # votes.append(row[2])
#print(candidates)
#print(votes)
        #to make candidate name as the key
             candidates_dict[row[2]] = 0
             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
     print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/12:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #if row[2] in candidates:
           # votes.append(row[2])
#print(candidates)
#print(votes)
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
     print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
119/1:
import os
import csv

#choose 1 or 2
file_num = 1

# Identifies file with poll data
file = os.path.join('raw_data', 'election_data_' + str(file_num) + '.csv')

#Creates dictionary to be used for candidate name and vote count.
poll = {}

#Sets variable, total votes, to zero for count.
total_votes = 0

#gets data file
with open(file, 'r') as csvfile:
    csvread = csv.reader(csvfile)

    #skips header line
    next(csvread, None)

    #creates dictionary from file using column 3 as keys, using each name only once.
    #counts votes for each candidate as entries
    #keeps a total vote count by counting up 1 for each loop (# of rows w/o header)
    for row in csvread:
        total_votes += 1
        if row[2] in poll.keys():
            poll[row[2]] = poll[row[2]] + 1
        else:
            poll[row[2]] = 1
 
#create empty list for candidates and his/her vote count
candidates = []
num_votes = []

#takes dictionary keys and values and, respectively, dumps them into the lists, 
# candidates and num_votes
for key, value in poll.items():
    candidates.append(key)
    num_votes.append(value)

# creates vote percent list
vote_percent = []
for n in num_votes:
    vote_percent.append(round(n/total_votes*100, 1))

# zips candidates, num_votes, vote_percent into tuples
clean_data = list(zip(candidates, num_votes, vote_percent))

#creates winner_list to put winners (even if there is a tie)
winner_list = []

for name in clean_data:
    if max(num_votes) == name[1]:
        winner_list.append(name[0])

# makes winner_list a str with the first entry
winner = winner_list[0]

#only runs if there is a tie and puts additional winners into a string separated by commas
if len(winner_list) > 1:
    for w in range(1, len(winner_list)):
        winner = winner + ", " + winner_list[w]

#prints to file
output_file = os.path.join('Output', 'election_results_' + str(file_num) +'.txt')

with open(output_file, 'w') as txtfile:
    txtfile.writelines('Election Results \n------------------------- \nTotal Votes: ' + str(total_votes) + 
      '\n-------------------------\n')
    for entry in clean_data:
        txtfile.writelines(entry[0] + ": " + str(entry[2]) +'%  (' + str(entry[1]) + ')\n')
    txtfile.writelines('------------------------- \nWinner: ' + winner + '\n-------------------------')
119/2:
import os
import csv

#choose 1 or 2
file_num = 1

# Identifies file with poll data
file = os.path.join("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")

#Creates dictionary to be used for candidate name and vote count.
poll = {}

#Sets variable, total votes, to zero for count.
total_votes = 0

#gets data file
with open(file, 'r') as csvfile:
    csvread = csv.reader(csvfile)

    #skips header line
    next(csvread, None)

    #creates dictionary from file using column 3 as keys, using each name only once.
    #counts votes for each candidate as entries
    #keeps a total vote count by counting up 1 for each loop (# of rows w/o header)
    for row in csvread:
        total_votes += 1
        if row[2] in poll.keys():
            poll[row[2]] = poll[row[2]] + 1
        else:
            poll[row[2]] = 1
 
#create empty list for candidates and his/her vote count
candidates = []
num_votes = []

#takes dictionary keys and values and, respectively, dumps them into the lists, 
# candidates and num_votes
for key, value in poll.items():
    candidates.append(key)
    num_votes.append(value)

# creates vote percent list
vote_percent = []
for n in num_votes:
    vote_percent.append(round(n/total_votes*100, 1))

# zips candidates, num_votes, vote_percent into tuples
clean_data = list(zip(candidates, num_votes, vote_percent))

#creates winner_list to put winners (even if there is a tie)
winner_list = []

for name in clean_data:
    if max(num_votes) == name[1]:
        winner_list.append(name[0])

# makes winner_list a str with the first entry
winner = winner_list[0]

#only runs if there is a tie and puts additional winners into a string separated by commas
if len(winner_list) > 1:
    for w in range(1, len(winner_list)):
        winner = winner + ", " + winner_list[w]

#prints to file
output_file = os.path.join('Output', 'election_results_' + str(file_num) +'.txt')

with open(output_file, 'w') as txtfile:
    txtfile.writelines('Election Results \n------------------------- \nTotal Votes: ' + str(total_votes) + 
      '\n-------------------------\n')
    for entry in clean_data:
        txtfile.writelines(entry[0] + ": " + str(entry[2]) +'%  (' + str(entry[1]) + ')\n')
    txtfile.writelines('------------------------- \nWinner: ' + winner + '\n-------------------------')
119/3:
import os
import csv

#choose 1 or 2
file_num = 1

# Identifies file with poll data
file = os.path.join("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")

#Creates dictionary to be used for candidate name and vote count.
poll = {}

#Sets variable, total votes, to zero for count.
total_votes = 0

#gets data file
with open(file, 'r') as csvfile:
    csvread = csv.reader(csvfile)

    #skips header line
    next(csvread, None)

    #creates dictionary from file using column 3 as keys, using each name only once.
    #counts votes for each candidate as entries
    #keeps a total vote count by counting up 1 for each loop (# of rows w/o header)
    for row in csvread:
        total_votes += 1
        if row[2] in poll.keys():
            poll[row[2]] = poll[row[2]] + 1
        else:
            poll[row[2]] = 1
 
#create empty list for candidates and his/her vote count
candidates = []
num_votes = []

#takes dictionary keys and values and, respectively, dumps them into the lists, 
# candidates and num_votes
for key, value in poll.items():
    candidates.append(key)
    num_votes.append(value)

# creates vote percent list
vote_percent = []
for n in num_votes:
    vote_percent.append(round(n/total_votes*100, 1))

# zips candidates, num_votes, vote_percent into tuples
clean_data = list(zip(candidates, num_votes, vote_percent))

#creates winner_list to put winners (even if there is a tie)
winner_list = []

for name in clean_data:
    if max(num_votes) == name[1]:
        winner_list.append(name[0])

# makes winner_list a str with the first entry
winner = winner_list[0]

#only runs if there is a tie and puts additional winners into a string separated by commas
if len(winner_list) > 1:
    for w in range(1, len(winner_list)):
        winner = winner + ", " + winner_list[w]

# #prints to file
# output_file = os.path.join('Output', 'election_results_' + str(file_num) +'.txt')

# with open(output_file, 'w') as txtfile:
#     txtfile.writelines('Election Results \n------------------------- \nTotal Votes: ' + str(total_votes) + 
#       '\n-------------------------\n')
#     for entry in clean_data:
#         txtfile.writelines(entry[0] + ": " + str(entry[2]) +'%  (' + str(entry[1]) + ')\n')
#     txtfile.writelines('------------------------- \nWinner: ' + winner + '\n-------------------------')
119/4:
import os
import csv

#choose 1 or 2
file_num = 1

# Identifies file with poll data
file = os.path.join("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")

#Creates dictionary to be used for candidate name and vote count.
poll = {}

#Sets variable, total votes, to zero for count.
total_votes = 0

#gets data file
with open(file, 'r') as csvfile:
    csvread = csv.reader(csvfile)

    #skips header line
    next(csvread, None)

    #creates dictionary from file using column 3 as keys, using each name only once.
    #counts votes for each candidate as entries
    #keeps a total vote count by counting up 1 for each loop (# of rows w/o header)
    for row in csvread:
        total_votes += 1
        if row[2] in poll.keys():
            poll[row[2]] = poll[row[2]] + 1
        else:
            poll[row[2]] = 1
 
#create empty list for candidates and his/her vote count
candidates = []
num_votes = []

#takes dictionary keys and values and, respectively, dumps them into the lists, 
# candidates and num_votes
for key, value in poll.items():
    candidates.append(key)
    num_votes.append(value)

# creates vote percent list
vote_percent = []
for n in num_votes:
    vote_percent.append(round(n/total_votes*100, 1))

# zips candidates, num_votes, vote_percent into tuples
clean_data = list(zip(candidates, num_votes, vote_percent))

#creates winner_list to put winners (even if there is a tie)
winner_list = []

for name in clean_data:
    if max(num_votes) == name[1]:
        winner_list.append(name[0])

# makes winner_list a str with the first entry
winner = winner_list[0]

#only runs if there is a tie and puts additional winners into a string separated by commas
if len(winner_list) > 1:
    for w in range(1, len(winner_list)):
        winner = winner + ", " + winner_list[w]
    

#prints to file
output_file = os.path.join("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")

with open(output_file, 'w') as txtfile:
    txtfile.writelines('Election Results \n------------------------- \nTotal Votes: ' + str(total_votes) + 
      '\n-------------------------\n')
    for entry in clean_data:
        txtfile.writelines(entry[0] + ": " + str(entry[2]) +'%  (' + str(entry[1]) + ')\n')
    txtfile.writelines('------------------------- \nWinner: ' + winner + '\n-------------------------')
118/13:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    votes_percent = {}
    winner_list = []

    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
#         if row[2] not in candidates:
#             #add candidates to the empty list
#             candidates.append(row[2])
#             #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#      print(candidates_dict)
    for key, value in candidates_dict.items():
            candidates.append(key)
            votes.append(value)
    for n in votes:
        vote_percent.append(round(n/total_votes*100, 1))
    # zips candidates, num_votes, vote_percent into tuples
    clean_data = list(zip(candidates, votes, vote_percent))
    for name in clean_data:
    if max(votes) == name[1]:
        winner_list.append(name[0])
    # makes winner_list a str with the first entry
    winner = winner_list[0]

    #only runs if there is a tie and puts additional winners into a string separated by commas
    if len(winner_list) > 1:
        for w in range(1, len(winner_list)):
            winner = winner + ", " + winner_list[w]
            print('Winner is: ' + winner + ' (' + str(winner_list) + ')''\n')
            
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/14:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    votes_percent = {}
    winner_list = []

    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
#         if row[2] not in candidates:
#             #add candidates to the empty list
#             candidates.append(row[2])
#             #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#      print(candidates_dict)
    for key, value in candidates_dict.items():
            candidates.append(key)
            votes.append(value)
    for n in votes:
        vote_percent.append(round(n/total_votes*100, 1))
    # zips candidates, num_votes, vote_percent into tuples
    clean_data = list(zip(candidates, votes, vote_percent))
    for name in clean_data:
        if max(votes) == name[1]:
            winner_list.append(name[0])
            # makes winner_list a str with the first entry
            winner = winner_list[0]

    #only runs if there is a tie and puts additional winners into a string separated by commas
    if len(winner_list) > 1:
        for w in range(1, len(winner_list)):
            winner = winner + ", " + winner_list[w]
            print('Winner is: ' + winner + ' (' + str(winner_list) + ')''\n')
            
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/15:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    votes_percent = {}
    winner_list = []

    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
#         if row[2] not in candidates:
#             #add candidates to the empty list
#             candidates.append(row[2])
#             #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#      print(candidates_dict)
    for key, value in candidates_dict.items():
            candidates.append(key)
            votes.append(value)
    for n in votes:
        votes_percent.append(round(n/total_votes*100, 1))
    # zips candidates, num_votes, vote_percent into tuples
    clean_data = list(zip(candidates, votes, votes_percent))
    for name in clean_data:
        if max(votes) == name[1]:
            winner_list.append(name[0])
            # makes winner_list a str with the first entry
            winner = winner_list[0]

    #only runs if there is a tie and puts additional winners into a string separated by commas
    if len(winner_list) > 1:
        for w in range(1, len(winner_list)):
            winner = winner + ", " + winner_list[w]
            print('Winner is: ' + winner + ' (' + str(winner_list) + ')''\n')
            
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/16:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    votes_percent = {}
    winner_list = []

    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
#         if row[2] not in candidates:
#             #add candidates to the empty list
#             candidates.append(row[2])
#             #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#      print(candidates_dict)
    for key, value in candidates_dict.items():
            candidates.append(key)
            votes.append(value)
    for n in votes:
        votes_percent.append(round(n/total_votes*100, 1))
    # zips candidates, num_votes, vote_percent into tuples
    clean_data = list(zip(candidates, votes, votes_percent))
    for name in clean_data:
        if max(votes) == name[1]:
            winner_list.append(name[0])
            # makes winner_list a str with the first entry
            winner = winner_list[0]

    #only runs if there is a tie and puts additional winners into a string separated by commas
    if len(winner_list) > 1:
        for w in range(1, len(winner_list)):
            winner = winner + "," + winner_list[w]
        print('Winner is: ' + winner + ' (' + str(winner_list) + ')''\n')
            
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/17:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    votes_percent = {}
    winner_list = []

    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
#         if row[2] not in candidates:
#             #add candidates to the empty list
#             candidates.append(row[2])
#             #to make candidate name as the key
#             candidates_dict[row[2]] = 0
#             candidates_dict[row[2]] = candidates_dict[row[2]] + 1
#      print(candidates_dict)
    for key, value in candidates_dict.items():
            candidates.append(key)
            votes.append(value)
    for n in votes:
        votes_percent.append(round(n/total_votes*100, 1))
    # zips candidates, num_votes, vote_percent into tuples
    clean_data = list(zip(candidates, votes, votes_percent))
    for name in clean_data:
        if max(votes) == name[1]:
            winner_list.append(name[0])
            # makes winner_list a str with the first entry
            winner = winner_list[0]

    #only runs if there is a tie and puts additional winners into a string separated by commas
    if len(winner_list) > 1:
        for w in range(1, len(winner_list)):
            winner = winner + "," + winner_list[w]
        print('Winner is: ' + winner + ' (' + str(winner_list))
            
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/18:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else :
            
     print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/19:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else :
            
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/20:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        #else :     
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/21:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            print(candidates_dict.get(row[2]))
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/22:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    print(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/23:
import csv
import pprint
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    pprint.pprint(candidates_dict)
#     for candidates in candidates_dict:
#         votes = candidates_dict.get(candidates)
#         percentage = (votes / total_votes)*100
#         print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
#         if votes > winning_votes:
#             winning_votes = votes
#             winner = candidates
    

# print('Election Results\n')
# print('---------------------------\n')
# print("Total Votes: " + str(total_votes) + '\n')
# print('----------------------------\n')

# print('----------------------------\n')
# #print("Winner: ")
# print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/24:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = {}
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')

print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/25:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')

print('----------------------------\n')
print("Winner: ")
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/26:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
    

print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: ")
print("Winnering Votes: " + str(winning_votes) + " Winner: " + str(winner) +'\n')
print('----------------------------\n')

#for candidates in candidates_dict:
    #votes = candidates_dict.get(candidates)
    #percentage = votes / total_votes
    #new_file.write(candidates + ':' + str(round(percentage,2)*100) + ' : (' + str(votes) + ')''\n')
    #if votes > winning_votes:
        #winning_votes = votes
        #winner = candidates
#new_file.write("------------------------------------- \n")        
#new_file.write('Winner is: ' + winner + ' (' + str(winning_votes) + ')''\n')

#set path for output file
#filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\output_pypoll.txt")
# opens the output destination in write mode and prints the summary
#with open(filepath_1, 'w') as writefile:
    #csvwriter = csv.writer(writefile, delimiter=",")
    #writefile.writelines('Election Results \n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines('Total Votes: " + str(total_votes) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines(str(candidates_dict) + '\n')
    #writefile.writelines('-------------------------------\n')
    #writefile.writelines("Winner: ")
    #writefile.writelines('-------------------------------\n')
118/27:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates_dict[candidates]) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: ")
print("Winnering Votes: " + str(winning_votes) + " Winner: " + str(winner) +'\n')
print('----------------------------\n')
118/28:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: ")
print("Winnering Votes: " + str(winning_votes) + " Winner: " + str(winner) +'\n')
print('----------------------------\n')
118/29:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winnering Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/30:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ("$") + ':' + str(round(percentage,2)) + ' : (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/31:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,2)) + ("%") (' + str(votes) + ')''\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/32:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,2)) + ("%") '(' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/33:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,2)) + ("%") + '(' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/34:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage)) + ("%") + '(' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/35:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage)) + ("%") + '   (' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/36:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,3)) + ("%") + '   (' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/37:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")

#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,3)) + ("%") + '   (' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
print('Election Results\n')
print('---------------------------\n')
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
118/38:
import csv
import os
from pathlib import Path

#set path
filepath = Path("C:\\Users\\Subha\\repos\\python-challenge\\PyPoll\\election_data.csv")
print('Election Results\n')
print('---------------------------\n')
#open the file
with open(filepath, newline="",encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")
    #Initialize variables to empty lists and dictionary
    votes = []
    total_votes = 0
    candidates = []
    candidates_dict = {}
    winning_votes = 0
    next(csvreader)
    #go line by line and process each vote
    for row in csvreader:
        #add to total number of votes
        total_votes += 1
        if row[2] not in candidates:
            #add candidates to the empty list
            candidates.append(row[2])
        #to make candidate name as the key
            candidates_dict[row[2]] = 0
            candidates_dict[row[2]] = candidates_dict[row[2]] + 1
        else:     
            candidates_dict[row[2]] +=1
    #print(candidates_dict)
    for candidates in candidates_dict:
        votes = candidates_dict.get(candidates)
        percentage = (votes / total_votes)*100
        print(str(candidates) + ':' + str(round(percentage,3)) + ("%") + '   (' + str(votes) + ')' '\n')
        if votes > winning_votes:
            winning_votes = votes
            winner = candidates
            
print("Total Votes: " + str(total_votes) + '\n')
print('----------------------------\n')
print("Winner: " + str(winner) +'\n')
print("Winning Votes: " + str(winning_votes) +'\n')
print('----------------------------\n')
124/1:
# Importing dependency modules for os,csv & path
import os
import csv
from pathlib import Path

# Link file path to Python
filepath = ("PyBank//budget_data.csv")
#Initialize variables to empty lists
month_list = []
revenue = []
avgchange = []
# open file which is linked from the path 
with open(filepath, encoding='utf-8') as csvfile:
    # Read files
    csvreader = csv.reader(csvfile, delimiter=",")
    # Skips the headers of the csv file
    next(csvreader,None)
    # Loop through all the datas we collect 
    for row in csvreader:
        #print(row)
        # add total no. of months for the entire period
        # Calculate net amount over entire period
        revenue.append(int(row[1]))   
        month_list.append(row[0])
    
    for i in range(1,len(revenue)):
        avgchange.append(revenue[i] - revenue[i-1])
    
    max_change = max(avgchange)
    max_index = avgchange.index(max_change)
    #print(max_index)
    greatest_month = month_list[max_index + 1]
    #print(greatest_month)
    min_change = min(avgchange)
    min_index = avgchange.index(min_change)
    #print(min_index)
    least_month = month_list[min_index + 1]
    #print(least_month)

    print("Financial Analysis\n")
    print("----------------------------------\n")
    print("Total Months:" + str(len(month_list)) + '\n')
    print("Total: $" + str(sum(revenue)) + '\n')
    print("Average Change: " + str("$") + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    print("Greatest Increase in Profits: " + str(greatest_month) + str("$") + str(max_change) + '\n')
    print("Greatest Decrease in Profits: " + str(least_month) + str("$") + str(min_change))


#set path for output file
filepath_1 = ("C:\\Users\\Subha\\repos\\python-challenge\\PyBank\\output_pybank.txt")
# opens the output destination in write mode and prints the summary
with open(filepath_1, 'w+') as writefile:
    csvwriter = csv.writer(writefile, delimiter=",")
    writefile.writelines('Financial Analysis\n')
    writefile.writelines('----------------------------' + '\n')
    writefile.writelines('Total Months: ' + str(len(month_list)) + '\n')
    writefile.writelines('Total: $' + str(sum(revenue)) + '\n')
    writefile.writelines('Average Change: ' + str('$') + str(sum(avgchange)/(len(month_list)-1)) + '\n')
    writefile.writelines('Greatest Increase in Profits: ' + str(greatest_month) +  str('  $') + str(max_change) + '\n')
    writefile.writelines('Greatest Decrease in Profits: ' + str(least_month) + str('  $') + str(min_change))
127/1: school_summary = complete_school_data.sst_index(["school_name"])
127/2: school_summary = complete_school_data.set_index(["school_name"])
127/3: school_summary = complete_school_data.set_index("school_name")
127/4:
school_summary = complete_school_data.set_index("school_name")
schhol_summary.head()
127/5:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
127/6:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)
students_total = students_data["student_name"].count()
#print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
#print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
#print(avg_math)
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)
overall_score = (avg_math + avg_reading)/2
#print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
first_school_df
127/7:
school_summary = complete_school_data.set_index("school_name")
schhol_summary.head()
127/8:
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
127/9:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
school_summary.groupby("school_name")
127/10:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
#school_summary.groupby("school_name")
127/11:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
school_summary.groupby("school_name")
127/12:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
school_summary.groupby("school_name")
print(school_summary)
127/13:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
127/14:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name")
school_type
127/15:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
school_type
127/16:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
print(school_type)
127/17:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
print(school_type)
127/18:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
print(school_type)
127/19:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
#The object returned is a "GroupBy" object and cannot be viewed normally
print(school_type)
#In order to be visualized, a data function must be used
school_type()
127/20:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
#The object returned is a "GroupBy" object and cannot be viewed normally
print(school_type)
#In order to be visualized, a data function must be used
school_type.head()
127/21:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
school_type = schools_data.set_index("school_name").groupby("type")
#The object returned is a "GroupBy" object and cannot be viewed normally
print(school_type)
127/22:
In order to be visualized, a data function must be used
school_type.head()
127/23:
#In order to be visualized, a data function must be used
school_type.head()
127/24:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#school_type = schools_data.set_index("school_name").groupby("type")
#The object returned is a "GroupBy" object and cannot be viewed normally
#print(school_type)
school_type = schools_data.set_index("school_name")["type"]
127/25:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#school_type = schools_data.set_index("school_name").groupby("type")
#The object returned is a "GroupBy" object and cannot be viewed normally
#print(school_type)
school_type = schools_data.set_index("school_name")["type"]
127/26:
#In order to be visualized, a data function must be used
school_type.head()
127/27:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#Firstly tackling the school-type
school_type = schools_data.set_index("school_name")["type"]
school_type
127/28:
#next the total students in each school
total_students = students_complete["Student ID"].count()
total_students
127/29:
#next the total students in each school
total_students = students_data["Student ID"].count()
total_students
127/30:
#next the total students in each school
total_students = students_data.set_index("Student ID").count()
total_students
127/31:
#next the total students in each school
total_students = students_data.set_index("school_name")["Student ID"].count()
total_students
127/32:
#next the total students in each school
total_students = complete_school_data.set_index("school_name")["Student ID"].count()
total_students
127/33:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#
school_summary_data = complete_school_data.set_index("school_name") 
#Firstly tackling the school-type
school_type = schools_data.set_index("school_name")["type"]
school_type
127/34:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#
school_summary_data = complete_school_data.set_index("school_name") 
#Firstly tackling the school-type
# school_type = schools_data.set_index("school_name")["type"]
# school_type
127/35:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#
school_summary_data = complete_school_data.set_index("school_name") 
#Firstly tackling the school-type
# school_type = schools_data.set_index("school_name")["type"]
# school_type
127/36:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#
school_summary_data = complete_school_data.set_index("school_name") 
school_summary_data
#Firstly tackling the school-type
# school_type = schools_data.set_index("school_name")["type"]
# school_type
127/37:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
school_summary.groupby(["school_name"])
127/38:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
school_summary.head()
school_summary.groupby(["school_name"]).head()
127/39:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
school_summary.groupby(["school_name"]).head()
127/40:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
school_summary.groupby(["school_name"]).head(5)
127/41:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
school_summary.groupby(["school_name"]).head()
127/42:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
school_summary.groupby(["school_name"]).head()
127/43:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
school_summary.groupby(["school_name"]).head()
127/44:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"]).head()
127/45:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"]).head()
127/46:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
127/47:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)
students_total = students_data["student_name"].count()
#print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
#print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
#print(avg_math)
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)
overall_score = (avg_math + avg_reading)/2
#print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation - counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
first_school_df
127/48:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"]).head()
127/49:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#Firstly tackling the school-type
# school_type = schools_data.set_index("school_name")["type"]
# school_type
127/50:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"]).head()
grouped_school_summary
127/51:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"]).head()
grouped_school_summary.head()
127/52:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"])
grouped_school_summary.head()
127/53:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#school_summary.head()
grouped_school_summary = school_summary.groupby(["school_name"])
grouped_school_summary.head()
127/54:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#Firstly tackling the school-type
school_type = grouped_school_summary.set_index("school_name")["type"]
school_type
127/55:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#Firstly tackling the school-type
school_type = school_data.set_index("school_name")["type"]
school_type
127/56:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!   
#Firstly tackling the school-type
school_type = schools_data.set_index("school_name")["type"]
school_type
127/57:
#next the total students in each school
total_students = grouped_school_summary["Student ID"].count()
total_students
127/58:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
total_school_budget
127/59:
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
per_student_budget
127/60:
average_math = students_data.set_index("school_name")["math_score"].mean()
average_math
127/61:
average_math = students_data["math_score"].mean()
average_math
127/62:
average_math = grouped_school_summary["math_score"].mean()
average_math
127/63:
average_reading = grouped_school_summary["reading_score"].mean()
average_reading
127/64:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary[grouped_school_summary["math_score"]>=70].groupby("school_name)".[Student ID].count()
percentage_math
127/65:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary[grouped_school_summary["math_score"]>=70].groupby("school_name)".["Student ID"].count()
percentage_math
127/66:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary[grouped_school_summary["math_score"]>=70].groupby("school_name")["Student ID"].count()
percentage_math
127/67:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary[grouped_school_summary["math_score"]>="70"].groupby("school_name")["Student ID"].count()
percentage_math
127/68:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary[grouped_school_summary["math_score"]>"70"].groupby("school_name")["Student ID"].count()
percentage_math
127/69:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary.loc[grouped_school_summary["math_score"]>"70"].groupby("school_name")["Student ID"].count()
percentage_math
127/70:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
percentage_math = grouped_school_summary.loc[grouped_school_summary["math_score"]>"70"]
percentage_math
127/71:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
127/72:
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)
students_total = students_data["student_name"].count()
#print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum()
budget_total = schools_data["budget"].sum()
#print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean()
avg_math = students_data["math_score"].mean()
#print(avg_math)
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)
overall_score = (avg_math + avg_reading)/2
#print(overall_score)
#percentage of students with >=70 in math = (no.of students having score greater than 70/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
first_school_df
127/73:
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
127/74:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
school_type
127/75:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
total_students
127/76:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
total_school_budget
127/77:
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
per_student_budget
127/78:
average_math = grouped_school_summary["math_score"].mean()
average_math
127/79:
average_reading = grouped_school_summary["reading_score"].mean()
average_reading
127/80:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = grouped_school_summary.loc[grouped_school_summary["math_score"]>"70"]
numerator_value
127/81:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = grouped_school_summary.loc[grouped_school_summary["math_score"]>"70"]
numerator_value
127/82:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = grouped_school_summary[grouped_school_summary["math_score"]>"70"]
numerator_value
127/83:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>="70"]
numerator_value
127/84:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70]
numerator_value
127/85:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator_value
127/86:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
print(numerator_value)
127/87:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator_value.count()
127/88:
#percentage of students passing math = (no.of students with mathscores>=70/total no. os students in that school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator_value["Student ID"].count()
127/89:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator = numerator_value["Student ID"].count()
final_percent_math = (numerator/total_students)*100
127/90:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator = numerator_value["Student ID"].count()
final_percent_math = (numerator/total_students)*100
final_percent_math
127/91:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70].groupby("school_name")
numerator_math = numerator_value_math["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
final_percent_math
127/92:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70].groupby("school_name")
numerator_reading = numerator_value_reading["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
final_percent_reading
127/93:
#overall passing rate = (percent of students passing in math in each school + percent of students passing in reading in each school)/ total_students in each school
overall_passing = final_percent_reading + final_percent_math)/ total_students
overall_passing
127/94:
#overall passing rate = (percent of students passing in math in each school + percent of students passing in reading in each school)/ total_students in each school
overall_passing = (final_percent_reading + final_percent_math)/ total_students
overall_passing
127/95:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
overall_passing = (numerator_value_math + numerator_value_reading)/ total_students
overall_passing
127/96:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
overall_passing = (numerator_value_math and numerator_value_reading)/ total_students
overall_passing
127/97:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
overall_passing = ((numerator_value_math and numerator_value_reading)/ total_students)
overall_passing
127/98:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
overall_passing = ((numerator_value_math & numerator_value_reading)/ total_students)
overall_passing
127/99:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)
# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)
# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)
# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)
#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)
#overall score for all the schools
overall_score = (avg_math + avg_reading)/students_total
#print(overall_score)
#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)
first_school_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
first_school_df
127/100:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
final_percent_reading
127/101:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
# overall_passing = ((numerator_value_math & numerator_value_reading)/ total_students)
# overall_passing
127/102:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": "school_type","Total Students": "total_students", "Total_School_Budget":"total_school_budget",
                           "Per Student Budget": "per_student_budget","Average Math Score":"average_math","Average Reading Score":"average_reading","%Passing Math":"final_percent_math"
                           "%Passing Reading":"final_percent_reading"})
each_school.head()
127/103:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": "school_type","Total Students": "total_students", "Total_School_Budget":"total_school_budget",
                           "Per Student Budget": "per_student_budget","Average Math Score":"average_math","Average Reading Score":"average_reading","%Passing Math":"final_percent_math",
                           "%Passing Reading":"final_percent_reading"})
each_school.head()
127/104:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": [total_students], "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school.head()
128/1:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
all_schools_df
128/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
all_schools_df
128/3:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
128/4:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
all_schools_df
128/5:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
#all_schools_df
128/6:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
128/7:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_score]})
#use Map to format the the entire df
#all_schools_df
128/8:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#use Map to format the the entire df
#all_schools_df
128/9:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#use Map to format the the entire df
all_schools_df
128/10:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
128/11:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
school_type
128/12:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": [total_students], "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school.head()
128/13:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": total_students, "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school.head()
128/14:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
128/15:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning keys and values in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#use Map to format the the entire df
all_schools_df
128/16:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
128/17:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
128/18:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
128/19:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
128/20:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
128/21:
average_math = grouped_school_summary["math_score"].mean()
#average_math
128/22:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
128/23:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
128/24:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
128/25:
#overall passing rate = (no. of students passing in math in each school + no. of students passing in reading in each school)/ total_students in each school
# overall_passing = ((numerator_value_math & numerator_value_reading)/ total_students)
# overall_passing
128/26:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": [total_students], "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school.head()
128/27:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": [total_students], "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school.columns
128/28:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": [school_type],"Total Students": [total_students], "Total_School_Budget":[total_school_budget],
                           "Per Student Budget": [per_student_budget],"Average Math Score":[average_math],"Average Reading Score":[average_reading],"%Passing Math":[final_percent_math],
                           "%Passing Reading":[final_percent_reading]})
each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading" ]]
each_school.head()
128/29:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading})
each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading" ]]
each_school.head()
128/30:
#creating another DF for each school in the district
each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading})
each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading" ]]
each_school
128/31:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":schools_total, "Total Students": students_total,
                               "Total Budget": budget_total, "Average Math Score": avg_math, "Average Reading Score": avg_reading,
                              "%Passing Math": percent_math, "%Passing Reading": percent_reading, "%Overall Passing Rate": overall_percent_score})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
128/32:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":schools_total, "Total Students": students_total,
                               "Total Budget": budget_total, "Average Math Score": avg_math, "Average Reading Score": avg_reading,
                              "%Passing Math": percent_math, "%Passing Reading": percent_reading, "%Overall Passing Rate": overall_percent_score})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df.head()
128/33:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
132/1:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df
132/2:
# Import Dependencies
import pandas as pd
import numpy as np
132/3:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df
132/4:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df.head()
132/5:
# Collect a list of all the unique values in "Preferred Position"
soccer_2018_df["Preferred Position"].unique()
137/1:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df.head()
137/2:
# Import Dependencies
import pandas as pd
import numpy as np
137/3:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df.head()
132/6:
# Looking only at strikers (ST) to start
st = soccer_2018_df.loc[soccer_2018_df["Preferred Position"=="ST",:]]
st
132/7:
# Looking only at strikers (ST) to start
st = soccer_2018_df.loc[soccer_2018_df["Preferred Position"]=="ST",:]]
st
132/8:
# Looking only at strikers (ST) to start
st = soccer_2018_df.loc[soccer_2018_df["Preferred Position"]=="ST",:]
st
132/9:
# Looking only at strikers (ST) to start
st = soccer_2018_df.loc[soccer_2018_df["Preferred Position"]=="ST",:]
st.head()
132/10:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]

# Reset the index so that the index is now based on the sorting locations
132/11:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]
st

# Reset the index so that the index is now based on the sorting locations
132/12:
# Import Dependencies
import pandas as pd
import numpy as np
132/13:
# Create reference to CSV file
csv_path = "Resources/Soccer2018Data.csv"

# Import the CSV into a pandas DataFrame
soccer_2018_df = pd.read_csv(csv_path, low_memory=False)
soccer_2018_df.head()
132/14:
# Collect a list of all the unique values in "Preferred Position"
soccer_2018_df["Preferred Position"].unique()
132/15:
# Looking only at strikers (ST) to start
st = soccer_2018_df.loc[soccer_2018_df["Preferred Position"]=="ST",:]
st.head()
132/16:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]
st

# Reset the index so that the index is now based on the sorting locations
132/17:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]
st.head()

# Reset the index so that the index is now based on the sorting locations
132/18:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]
st = st.reset_index(drop = True)
st

# Reset the index so that the index is now based on the sorting locations
132/19:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values("ST")
#st = st.reset_index(drop = True)
st

# Reset the index so that the index is now based on the sorting locations
132/20:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values("ST")
#st = st.reset_index(drop = True)
st.head()

# Reset the index so that the index is now based on the sorting locations
132/21:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values["ST"]
#st = st.reset_index(drop = True)
st.head()

# Reset the index so that the index is now based on the sorting locations
132/22:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values("ST")
#st = st.reset_index(drop = True)
st.head()

# Reset the index so that the index is now based on the sorting locations
132/23:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values("ST")
st = st.reset_index(drop = True)
st.head()

# Reset the index so that the index is now based on the sorting locations
132/24:
# Sort the DataFrame by the values in the "ST" column to find the worst
st = st.sort_values("ST")
#st = st.reset_index(drop = True)
st.head()

# Reset the index so that the index is now based on the sorting locations
132/25:
# Save all of the information collected on the worst striker
worst_striker = st.loc(0,:)
132/26:
# Save all of the information collected on the worst striker
worst_striker = st.loc(0,:)
worst_striker
132/27:
# Save all of the information collected on the worst striker
worst_striker = st.loc[0,:]
worst_striker
138/1:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
overall_passing_rate
138/2:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
138/3:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
138/4:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
138/5:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
138/6:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
138/7:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
138/8:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
138/9:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
138/10:
average_math = grouped_school_summary["math_score"].mean()
#average_math
138/11:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
138/12:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
138/13:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
138/14:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
overall_passing_rate
138/15:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading})
#To display the columns in the new DF, use [["","",""]]
each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading" ]]
each_school
138/16:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
each_school
138/17:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
138/18:
for_each_school = for_each_school["Overall Passing Rate"].sort_values
for_each_school
138/19:
for_each_school = for_each_school["Overall Passing Rate"].sort_values
for_each_school.head()
138/20:
for_each_school = for_each_school("Overall Passing Rate").sort_values
for_each_school.head()
138/21:
for_each_school = for_each_school.sort_values("Overall Passing Rate")
for_each_school.head()
138/22:
for_each_school = for_each_school.sort_values("Overall Passing Rate")
for_each_school.head()
138/23:
sorted_for_each_school = for_each_school.sort_values("Overall Passing Rate")
sorted_for_each_school.head()
138/24:
sorted_for_each_school = for_each_school.sort_values("overall_passing_rate")
sorted_for_each_school.head()
138/25: sorted_for_each_school = sorted_for_each_school.nlargest(5, "Overall Passing Rate")
138/26: for_each_school = for_each_school.nlargest(5, "Overall Passing Rate")
138/27: for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
138/28:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
138/29:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
138/30:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
138/31:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
138/32:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
138/33:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
138/34:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
138/35:
average_math = grouped_school_summary["math_score"].mean()
#average_math
138/36:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
138/37:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
138/38:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
138/39:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
overall_passing_rate
138/40:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
overall_passing_rate
138/41:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
138/42: for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
138/43:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
138/44:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
144/1: help.arange()
144/2:
# Import Numpy for calculations and matplotlib for charting
import numpy as np
import matplotlib.pyplot as plt
144/3:
# Creates a list from 0 to 5 with each step being 0.1 higher than the last
x_axis = np.arange(0, 5, 0.1)
x_axis
144/4:
# Creates a list from 0 to 5 with each step being 0.1 higher than the last
x_axis = np.arange(0, 5, 0.1)
len(x_axis)
144/5:
# Creates a list from 0 to 5 with each step being 0.1 higher than the last
x_axis = np.arange(0, 5, 0.1)
len(x_axis)
x_axis
144/6:
# Give our graph axis labels
plt.xlabel("Time With MatPlotLib")
plt.ylabel("How Cool MatPlotLib Seems")
pli.title("$\alpha")

# Have to plot our chart once again as it doesn't stick after being shown
plt.plot(x_axis, e_x)
plt.show()
144/7:
# Give our graph axis labels
plt.xlabel("Time With MatPlotLib")
plt.ylabel("How Cool MatPlotLib Seems")
plt.title("$\alpha")

# Have to plot our chart once again as it doesn't stick after being shown
plt.plot(x_axis, e_x)
plt.show()
146/1:
import numpy as np
import matplotlib.pyplot as plt
146/2:
x_axis = np.arange([39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44])
x_axis
146/3:
x_axis = np.arange(39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44)
x_axis
146/4:
x_axis =[39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
x_axis
146/5:
x_axis =[39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
x_axis_1 = np.aarange(x_axis)
146/6:
x_axis =[39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
x_axis_1 = np.arange(x_axis)
146/7:
import numpy as np
import matplotlib.pyplot as plt
146/8:
x_axis =[39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
x_axis_1 = np.arange(x_axis)
146/9:
x_axis_1 = np.arange(1, 13, 1)
x_axis
147/1: %matplotlib notebook
147/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
147/3:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
147/4:
# Draw a horizontal line with 0.25 transparency
plt.hlines(0, 0, 10, alpha=0.25)
147/5:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
147/6:
# Adds a legend and sets its location to the lower right
plt.legend(loc="lower right")
147/7:
# Adds a legend and sets its location to the lower right
plt.legend(loc="lower right")
147/8:
# Saves an image of our chart so that we can view it in a folder
plt.savefig("../Images/lineConfig.png")
plt.show()
149/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
149/2:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
149/3: %matplotlib notebook
149/4:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
149/5:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
149/6:
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
149/7:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
149/8: help(plt.plot)
150/1:
import matplotlib.pyplot as plt
import numpy as np
150/2:
x_axis = np.arange(1,13,1)
X-axis
150/3:
x_axis = np.arange(1,13,1)
x-axis
150/4:
# Include this line to make plots interactive
%matplotlib notebook
150/5:
import matplotlib.pyplot as plt
import numpy as np
150/6:
x_axis = np.arange(1,13,1)
x-axis
150/7:
x_axis = np.arange(1,13,1)
x-axis
150/8:
import matplotlib.pyplot as plt
import numpy as np
150/9:
x_axis = np.arange(1,13,1)
x-axis
150/10:
x_axis = np.arange(1,13,1)
x_axis
150/11:
points = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
celsius_axis  = [(x-32)*0.56 for x in points]
celsius_axis
# Plot both of these lines so that they will appear on our final chart
plt.plot(x_axis, points)
plt.plot(x_axis, celsius_axis)

plt.show()
150/12:
sin = np.sin(x_axis)
cos = np.cos(x_axis)
150/13:
sin = np.sin(x_axis)
cos = np.cos(x_axis)
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
150/14:
Farhenheit = np.sin(x_axis)
Celsius = np.cos(x_axis)
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
150/15:
# Each point on the sine chart is marked by a blue circle
Farhenheit_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
Celsius_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
150/16:
points = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
celsius_axis  = [(x-32)*0.56 for x in points]
celsius_axis
150/17:
# Plot both of these lines so that they will appear on our final chart
plt.plot(x_axis, points)
plt.plot(x_axis, celsius_axis)

plt.show()
150/18:
# Each point on the sine chart is marked by a blue circle
Farhenheit_handle, = plt.plot(x_axis, sin, marker ='+', color='blue', label="Fahrenheit")
# Each point on the cosine chart is marked by a red triangle
Celsius_handle, = plt.plot(x_axis, cos, marker='S', color='red', label="Celsius")
150/19:
# Each point on the sine chart is marked by a blue circle
Farhenheit_handle, = plt.plot(x_axis, sin, marker ='+', color='blue', label="Fahrenheit")
# Each point on the cosine chart is marked by a red triangle
Celsius_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Celsius")
153/1: %matplotlib notebook
153/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
153/3:
# Generate the x values from 0 to 10 using a step of 0.1
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
153/4:
# Add a semi-transparent horizontal line at y = 0
plt.hlines(0, 0, 10, alpha=0.25)
153/5:
# Use dots or other markers for your plots, and change their colors
plt.plot(x_axis, sin, linewidth=0, marker="o", color="blue")
plt.plot(x_axis, cos, linewidth=0, marker="^", color="red")
153/6:
# Add labels to the x and y axes
plt.title("Juxtaposed Sine and Cosine Curves")
plt.xlabel("Input (Sampled Real Numbers from 0 to 10)")
plt.ylabel("Value of Sine (blue) and Cosine (red)")
153/7:
# Set your x and y limits
plt.xlim(0, 10)
plt.ylim(-1, 1)
153/8:
# Set a grid on the plot
plt.grid()
153/9:
# Save the plot and display it
plt.savefig("../Images/sin_cos_with_markers.png")
plt.show()
153/10:
# Add a semi-transparent horizontal line at y = 0
plt.hlines(0, 0, 10, alpha=0.25)
153/11:
# Use dots or other markers for your plots, and change their colors - use sin and cos to plot the valus
plt.plot(x_axis, sin, linewidth=0, marker="o", color="blue")
plt.plot(x_axis, cos, linewidth=0, marker="^", color="red")
153/12:
# Set a grid on the plot
plt.grid()
154/1:
import numpy as np
import matplotlib as plt
154/2:
#Generate x values and y values
x_axis_danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
y_axis_rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
x_axis
154/3:
#Generate x values and y values
x_axis_danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
y_axis_rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
x_axis_danger
154/4:
#Generate x values and y values
x_axis_danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
y_axis_rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
x_axis_danger
y_axis_rail
154/5:
sin = np.sin(x_axis_danger)
cos = no.cos(y_axis_rail)
154/6:
sin = np.sin(x_axis_danger)
cos = np.cos(y_axis_rail)
154/7:
#generate x and y values with a step of 5
x_axis_danger = np.arange(0, 120, 5)
y_axis_rail = np.arange(0, 100, 5)
154/8: plt.hlines(0, 0, 10, alpha=0.25)
155/1: %matplotlib notebook
155/2:
import numpy as np
import matplotlib as plt
155/3:
#Generate x values and y values
x_axis_danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
y_axis_rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
x_axis_danger
y_axis_rail
155/4:
sin = np.sin(x_axis_danger)
cos = np.cos(y_axis_rail)
155/5:
#generate x and y values with a step of 10 seconds
x_axis_danger = np.arange(0, 120, 10)
y_axis_rail = np.arange(0, 100, 10)
155/6: plt.hlines(0, 0, 10, alpha=0.25)
156/1: %matplotlib notebook
156/2:
import numpy as np
import matplotlib as plt
156/3:
#Generate x values and y values
x_axis_danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
y_axis_rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
x_axis_danger
y_axis_rail
156/4:
sin = np.sin(x_axis_danger)
cos = np.cos(y_axis_rail)
156/5:
#generate x and y values with a step of 10 seconds
x_axis_danger = np.arange(0, 120, 10)
y_axis_rail = np.arange(0, 100, 10)
156/6:
plt.plot(x_axis_danger,color = "red", danger )
plt.plot(y_axis_rail,color="blue", rail)
156/7:
plt.plot(x_axis_danger,color = "red", danger)
plt.plot(y_axis_rail,color="blue", rail)
157/1: %matplotlib notebook
157/2:
import numpy as np
import matplotlib as plt
157/3:
#Generate x values and y values
danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
danger
rail
157/4:
#generate x and y values with a step of 10 seconds
x_axis_danger = np.arange(0, 130, 10)
y_axis_rail = np.arange(0, 130, 10)
157/5:
plt.plot(x_axis_danger,color = "red", danger)
plt.plot(y_axis_rail,color="blue", rail)
157/6:
plt.plot(time, x_axis_danger,color = "red", label ="danger_drop")
plt.plot(time, y_axis_rail,color="blue", rail, label = "rail_gun")
157/7:
#danger_drop and rain_gun are handles
danger_drop, = plt.plot(time, x_axis_danger,color = "red", label ="danger_drop")
rail_gun, = plt.plot(time, y_axis_rail,color="blue", rail, label = "rail_gun")
158/1: %matplotlib notebook
158/2:
import numpy as np
import matplotlib as plt
158/3:
#Generate x values and y values
danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
danger
rail
158/4:
#generate x and y values with a step of 10 seconds
time = np.arange(0, 130, 10)
158/5:
#danger_drop and rain_gun are handles
danger_drop, = plt.plot(time, x_axis_danger,color = "red", label ="danger_drop")
rail_gun, = plt.plot(time, y_axis_rail,color="blue", rail, label = "rail_gun")
158/6:
#danger_drop and rain_gun are handles
danger_drop, = plt.plot(time, x_axis_danger,color = "red", label ="danger_drop")
rail_gun, = plt.plot(time, y_axis_rail,color="blue", label = "rail_gun")
158/7:
#danger_drop and rain_gun are handles
danger_drop, = plt.plot(time, x_axis_danger,color = "red", label ="danger")
rail_gun, = plt.plot(time, y_axis_rail,color="blue", label = "rail")
159/1: %matplotlib notebook
159/2:
import numpy as np
import matplotlib as plt
159/3:
#Generate x values and y values
danger = [9, 8, 90, 85, 80, 70, 70, 65, 55, 60, 70, 65, 50]
rail = [75, 70, 60, 65, 60, 45, 55, 50, 40, 40, 35, 35, 30]
danger
rail
159/4:
#generate x and y values with a step of 10 seconds
time = np.arange(0, 130, 10)
159/5:
#danger_drop and rain_gun are handles
danger_drop, = plt.plot(time, x_axis_danger,color = "red", label ="danger")
rail_gun, = plt.plot(time, y_axis_rail,color="blue", label = "rail")
162/1: %matplotlib notebook
162/2:
import matplotlib.pyplot as plt
import numpy as np
162/3:
# Create an array that contains the number of users each language has
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
162/4:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
162/5:
# Tell matplotlib where we would like to place each of our x axis headers
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ["Java", "C++", "Python", "Ruby", "Clojure"])
162/6:
# Sets the x limits of the current chart
plt.xlim(-0.75, len(x_axis)-0.25)
162/7:
# Sets the y limits of the current chart
plt.ylim(0, max(users)+5000)
162/8:
# Give our chart some labels and a tile
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
163/1:
import matplotlib.pyplot as plt
import numpy as np
163/2:
# Create an array that contains the number of users each language has
#for the height of the bars
users = [13000, 26000, 52000, 30000, 9000]
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
x_axis = np.arange(len(users))
163/3:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
#align = center means the bar will be aligned at the center
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
163/4:
# Tell matplotlib where we would like to place each of our x axis headers
tick_locations = [value for value in x_axis]
#associate the values with the tick positions/locations
plt.xticks(tick_locations, ["Java", "C++", "Python", "Ruby", "Clojure"])
163/5:
# Tell matplotlib where we would like to place each of our x axis headers
tick_locations = [value for value in x_axis]
#associate the values with the tick positions/locations
plt.xticks(tick_locations, ["Java", "C++", "Python", "Ruby", "Clojure"])
163/6: %matplotlib notebook
163/7:
import matplotlib.pyplot as plt
import numpy as np
163/8:
# Create an array that contains the number of users each language has
#for the height of the bars
users = [13000, 26000, 52000, 30000, 9000]
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
x_axis = np.arange(len(users))
163/9:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
#align = center means the bar will be aligned at the center
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
163/10:
# Sets the x limits of the current chart
plt.xlim(-0.75, len(x_axis)-0.25)
163/11:
# Sets the y limits of the current chart
plt.ylim(0, max(users)+5000)
163/12:
# Give our chart some labels and a tile
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
164/1: %matplotlib notebook
164/2:
import matplotlib.pyplot as plt
import numpy as np
164/3:
# Create an array that contains the number of users each language has
#for the height of the bars
users = [13000, 26000, 52000, 30000, 9000]
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
x_axis = np.arange(len(users))
164/4:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
#align = center means the bar will be aligned at the center
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
164/5:
# Tell matplotlib where we would like to place each of our x axis headers
tick_locations = [value for value in x_axis]
#associate the values with the tick positions/locations
plt.xticks(tick_locations, ["Java", "C++", "Python", "Ruby", "Clojure"])
164/6:
# Sets the x limits of the current chart
plt.xlim(-0.75, len(x_axis)-0.25)
164/7:
# Sets the y limits of the current chart
plt.ylim(0, max(users)+5000)
164/8:
# Give our chart some labels and a tile
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
164/9:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
#align = center means the bar will be aligned at the center
plt.bar(x_axis, users, color='r', alpha=0.5, align="edge")
164/10:
# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="edge" to ensure our bars line up with our tick marks
#align = center means the bar will be aligned at the center
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
161/1:
# Create a bar chart based upon the above data
plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
161/2:
# Create a bar chart based upon the above data
bar_chart = plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
165/1: %matplotlib notebook
165/2:
import matplotlib.pyplot as plt
import numpy as np
165/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
165/4:
# Create a bar chart based upon the above data
bar_chart = plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
165/5: # Create the ticks for our bar chart's x axis
165/6: # Set the limits of the x axis
165/7: # Set the limits of the y axis
165/8: # Give the chart a title, x label, and y label
165/9: # Save an image of the chart and print it to the screen
166/1: %matplotlib notebook
166/2:
import matplotlib.pyplot as plt
import numpy as np
166/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
166/4:
# Create a bar chart based upon the above data
bar_chart = plt.bar(bars_in_cities, cities, color='b', alpha=0.5, align="center")
166/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
166/6: # Set the limits of the x axis
166/7: # Set the limits of the y axis
166/8: # Give the chart a title, x label, and y label
166/9: # Save an image of the chart and print it to the screen
166/10:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"])
166/11:
# Give the chart a title, x label, and y label
plt.title("Density of bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars per 10,000 Households")
166/12:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+9)
166/13:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
167/1: %matplotlib notebook
167/2:
import matplotlib.pyplot as plt
import numpy as np
167/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
cities = np.arange(len(bars_in_cities))
167/4:
# Create a bar chart based upon the above data
bar_chart = plt.bar(bars_in_cities, cities, color='b', alpha=0.5, align="center")
167/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"])
167/6:
# Create a bar chart based upon the above data
plt.bar(bars_in_cities, cities, color='b', alpha=0.5, align="center")
167/7:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"])
167/8:
# Set the limits of the x axis
plt.xlim(-0.75, len(cities)-0.25)
168/1: %matplotlib notebook
168/2:
import matplotlib.pyplot as plt
import numpy as np
168/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
cities = np.arange(len(bars_in_cities))
168/4:
# Create a bar chart based upon the above data
plt.bar(bars_in_cities, cities, color='b', alpha=0.5, align="center")
168/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"])
168/6:
# Create a bar chart based upon the above data
plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
169/1: %matplotlib notebook
169/2:
import matplotlib.pyplot as plt
import numpy as np
169/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
169/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
169/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"])
169/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(cities)-0.25)
169/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+9)
169/8:
# Give the chart a title, x label, and y label
plt.title("Density of bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars per 10,000 Households")
169/9: # Save an image of the chart and print it to the screen
170/1: %matplotlib notebook
170/2:
import matplotlib.pyplot as plt
import numpy as np
170/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
170/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, cities, color='b', alpha=0.5, align="center")
170/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,cities)
170/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
170/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+9)
170/8:
# Give the chart a title, x label, and y label
plt.title("Density of bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars per 10,000 Households")
170/9: # Save an image of the chart and print it to the screen
171/1: %matplotlib notebook
171/2:
import matplotlib.pyplot as plt
import numpy as np
171/3:
cities = ["New Orleans", "Milwaukee", "Omaha", "Pittsburgh", "Toledo"]
bars_in_cities = [8.6, 8.5, 8.3, 7.9, 7.2]
x_axis = np.arange(len(bars_in_cities))
171/4:
# Create a bar chart based upon the above data
plt.bar(x_axis, bars_in_cities, color='b', alpha=0.5, align="center")
171/5:
# Create the ticks for our bar chart's x axis
tick_location = [value for value in x_axis]
plt.xticks(tick_location,cities)
171/6:
# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
171/7:
# Set the limits of the y axis
plt.ylim(0, max(bars_in_cities)+9)
171/8:
# Give the chart a title, x label, and y label
plt.title("Density of bars in Cities")
plt.xlabel("Cities")
plt.ylabel("Bars per 10,000 Households")
171/9: # Save an image of the chart and print it to the screen
172/1:
# Import our dependencies
import matplotlib.pyplot as plt
import numpy as np
#magnitude check - best bet - bar graphs
#percentages - pie charts feasible
172/2:
# Labels for the sections of our pie chart
labels = ["Humans", "Smurfs", "Hobbits", "Ninjas"]

# The values of each section of the pie chart
sizes = [220, 95, 80, 100]

# The colors of each section of the pie chart
colors = ["red", "orange", "lightcoral", "lightskyblue"]

# Tells matplotlib to seperate the "Python" section from the others
explode = (0.1, 0, 0, 0)
172/3:
# Labels for the sections of our pie chart
labels = ["Humans", "Smurfs", "Hobbits", "Ninjas"]

# The values of each section of the pie chart
sizes = [220, 95, 80, 100]

# The colors of each section of the pie chart
colors = ["red", "orange", "lightcoral", "lightskyblue"]

# Tells matplotlib to seperate the "Python" section from the others
#distance from center of circle to 
explode = (0.1, 0, 0, 0)
help(explode)
172/4:
# Labels for the sections of our pie chart
labels = ["Humans", "Smurfs", "Hobbits", "Ninjas"]

# The values of each section of the pie chart
#[a,b,c] = calculated as (a/sum of the list values) and returns percentage values
sizes = [220, 95, 80, 100]

# The colors of each section of the pie chart
colors = ["red", "orange", "lightcoral", "lightskyblue"]

# Tells matplotlib to seperate the "Python" section from the others
# first value 0.1 = distance from the center to the vertex
#if data needs to be expnaded, then use tuple instead of list/arrays
#the length of the array in a tuple needs to match with the values
explode = (0.1, 0, 0, 0)
help(plt.pie)
172/5:
# Creates the pie chart based upon the values above
# Automatically finds the percentages of each part of the pie chart
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct="%1.1f%%", shadow=True,)
172/6:
# Creates the pie chart based upon the values above
# Automatically finds the percentages of each part of the pie chart
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct="%1.1f%%", shadow=True,startangle=140)
174/1: %matplotlib notebook
174/2:
# Import our dependencies
import matplotlib.pyplot as plt
import numpy as np
#magnitude check - best bet - bar graphs
#percentages - pie charts feasible
174/3:
# Labels for the sections of our pie chart
labels = ["Humans", "Smurfs", "Hobbits", "Ninjas"]

# The values of each section of the pie chart
#[a,b,c] = calculated as (a/sum of the list values) and returns percentage values
sizes = [220, 95, 80, 100]

# The colors of each section of the pie chart
colors = ["red", "orange", "lightcoral", "lightskyblue"]

# Tells matplotlib to seperate the "Python" section from the others
# first value 0.1 = distance from the center to the vertex
#if data needs to be expnaded, then use tuple instead of list/arrays
#the length of the array in a tuple needs to match with the values
explode = (0.1, 0, 0, 0)
help(plt.pie)
174/4:
# Creates the pie chart based upon the values above
# Automatically finds the percentages of each part of the pie chart
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct="%1.1f%%", shadow=True,startangle=140)
174/5:
# Tells matplotlib that we want a pie chart with equal axes
plt.axis("equal")
173/1:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangel = 140, labeldistance = 2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
175/1: %matplotlib notebook
175/2:
import matplotlib.pyplot as plt
import numpy as np
175/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
175/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangel = 140, labeldistance = 2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
176/1: %matplotlib notebook
176/2:
import matplotlib.pyplot as plt
import numpy as np
176/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
176/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 140, labeldistance = 2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
177/1: %matplotlib notebook
177/2:
import matplotlib.pyplot as plt
import numpy as np
177/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
177/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 140, labeldistance =0.8)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
178/1: %matplotlib notebook
178/2:
import matplotlib.pyplot as plt
import numpy as np
178/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
178/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =0.8)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
179/1: %matplotlib notebook
179/2:
import matplotlib.pyplot as plt
import numpy as np
179/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
179/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =0.8, rotatelabels = true)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
180/1: %matplotlib notebook
180/2:
import matplotlib.pyplot as plt
import numpy as np
180/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
180/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =0.8)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
181/1: %matplotlib notebook
181/2:
import matplotlib.pyplot as plt
import numpy as np
181/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
181/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =1.2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
182/1: %matplotlib notebook
182/2:
import matplotlib.pyplot as plt
import numpy as np
182/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["yellogreen", "red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
182/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =1.2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
183/1: %matplotlib notebook
183/2:
import matplotlib.pyplot as plt
import numpy as np
183/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["yellowgreen", "red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
183/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =1.2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
184/1: %matplotlib notebook
184/2:
import matplotlib.pyplot as plt
import numpy as np
184/3:
cities = ["Apple", "Pumpkin", "Chocolate Creme", "Cherry", "Apple Crumb", "Pecan", "Lemon Meringue", "Blueberry", "Key Lime", "Peach"]
bars_in_cities = [47,37,32,27,25,24,24,21,18,16]
colors = ["yellow","green", "red", "orange", "lightcoral", "lightskyblue", "purple","pink","blue","black","green"]
explode = (0.1,0,0,0,0,0,0,0,0,0)
184/4:
# Tell matplotlib to create a pie chart based upon the above data
#pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, 
    #labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)
plt.pie(bars_in_cities, explode=explode, labels = cities, colors = colors, autopct = "%1.1f%%", pctdistance = 1, shadow = True,
       startangle = 120, labeldistance =1.2)
# Create axes which are equal so we have a perfect circle

# Save an image of our chart and print the final product to the screen
184/5: plt.axis("equal")
186/1:
# Import Dependencies
import random
import matplotlib.pyplot as plt
import numpy as np
186/2:
# The maximum x value for our chart will be 100
x_limit = 100

# List of values from 0 to 100 each value being 1 greater than the last
x_axis = np.arange(0, x_limit, 1)

# Create a random array of data that we will use for our y values
data = [random.random() for value in x_axis]
185/1:
temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]
186/3:
# Tells matplotlib that we want to make a scatter plot
# The size of each point on our plot is determined by their x value
plt.scatter(x_axis, data, marker="o", facecolors="red", edgecolors="black",
            s=x_axis, alpha=0.75)
help(plt.scatter)
185/2:
# Tell matplotlib to create a scatter plot based upon the above data
#scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, 
#vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs)
plt.scatter(temp, sales, marker="o", facecolors="red", edgecolors="black",
            s=temp, alpha=0.75)
189/1: %matplotlib notebook
189/2:
import matplotlib.pyplot as plt
import numpy as np
189/3:
temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]
#x_limit = 26
#temp = np.arange(0,x_limit,1)
temp = np.arange(10,26,2)
189/4:
# Tell matplotlib to create a scatter plot based upon the above data
#scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, 
#vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs)
plt.scatter(temp, sales, marker="o", facecolors="red", edgecolors="black",
            s=temp, alpha=0.75)
197/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
197/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
197/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
197/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
197/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
197/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
197/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
197/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
197/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
197/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
197/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
197/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
197/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
198/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
198/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
198/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
198/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
198/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
198/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
198/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
198/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
198/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
198/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
198/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
198/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
198/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
198/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
198/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
198/16:
for_each_school = for_each_school.nsmallest(15, ["Overall Passing Rate"])
for_each_school
198/17:
for_each_school = for_each_school.nsmallest(15, ["Overall Passing Rate"])
for_each_school
199/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
199/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
199/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
199/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
199/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
199/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
199/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
199/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
199/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
199/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
199/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
199/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
199/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
199/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
199/15:
for_each_school = for_each_school.nsmallest(15, ["Overall Passing Rate"])
for_each_school
199/16:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/17:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/18:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
199/19:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
199/20:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
199/21:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
199/22:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
199/23:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
199/24:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
199/25:
average_math = grouped_school_summary["math_score"].mean()
#average_math
199/26:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
199/27:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
199/28:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
199/29:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
199/30:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/31:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/32:
for_each_school = for_each_school.nsmallest(5, ["%Passing Math")
for_each_school
199/33:
for_each_school = for_each_school.nsmallest(5, ["%Passing Math"])
for_each_school
199/34:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/35:
#for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
#for_each_school
199/36:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/37:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"])
for_each_school
199/38:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
199/39:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
200/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
200/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
200/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
200/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
200/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
200/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
200/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
200/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
200/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
200/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
200/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
200/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
200/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
200/14:
#for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
#for_each_school
200/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
200/16:
# Create a table that lists the average Reading Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

# Create a pandas series for each grade. Hint: use a conditional statement.

# Group each series by school

# Combine the series into a dataframe
math_score_grade = pd.series(["9th", "10th", "11th", "12th"])
math_score_grade
200/17:
# Create a table that lists the average Reading Score for students of each grade level (9th, 10th, 11th, 12th) at each school.

# Create a pandas series for each grade. Hint: use a conditional statement.

# Group each series by school

# Combine the series into a dataframe
math_score_grade = pd.Series(["9th", "10th", "11th", "12th"])
math_score_grade
200/18:
#Assign individual grade levels
math_grade_9th = students_data.loc(students_data["grade"]=="9th")
#groupby the school name and calculate the mean for the math score
math_grade_9th = groupby("school_name")["math_score"].mean()
math_grade_9th
200/19:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = groupby("school_name")["math_score"].mean()
math_grade_9th
200/20:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
math_grade_9th
200/21:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
math_grade_10th
200/22:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
math_grade_11th
200/23:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th", 
                                "12th":"math_grade_12th"})
final_math_grade
200/24:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th", 
                                "12th":"math_grade_12th"})
final_math_grade = final_math_grade.set_index("school_name")
200/25:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th", 
                                "12th":"math_grade_12th"})
final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
200/26:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th","12th":"math_grade_12th"})
final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
200/27:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
200/28:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
200/29:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
200/30:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
200/31:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
200/32:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
200/33:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
200/34:
average_math = grouped_school_summary["math_score"].mean()
#average_math
200/35:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
200/36:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
200/37:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
200/38:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
200/39:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
200/40:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
200/41:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
200/42:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th","12th":"math_grade_12th"})
final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
200/43:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th","12th":"math_grade_12th"})
# final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
202/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
202/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
202/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
202/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
202/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
202/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
202/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
202/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
202/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
202/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
202/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
202/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
202/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
202/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
202/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
202/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":"math_grade_9th", "10th":"math_grade_10th", "11th":"math_grade_11th","12th":"math_grade_12th"})
# final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
202/17:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
# final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
202/18:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
202/19:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = students_data.set_index("school_name")
final_math_grade
202/20:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"].set_index("school_name")
final_math_grade
202/21:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]].set_index("school_name")
final_math_grade
202/22:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
# .set_index("school_name")
final_math_grade
202/23:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade = final_math_grade.set_index("school_name")
final_math_grade
202/24:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade = final_math_grade.set_index["school_name"]
final_math_grade
202/25:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade = students_data.set_index("school_name")
final_math_grade
202/26:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
# final_math_grade = students_data.set_index("school_name")
final_math_grade
202/27:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
reading_grade_9th = math_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
final_reading_grade = pd.DataFrame({"9th":[reading_grade_9th], "10th":[reading_grade_10th],
                                    "11th":[reading_grade_11th],"12th":[reading_grade_12th]})
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
# final_reading_grade = students_data.set_index("school_name")
final_reading_grade
203/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
203/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
203/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
203/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
203/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
203/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
203/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
203/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
203/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
203/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
203/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
203/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
203/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
203/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
203/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
203/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":[math_grade_9th], "10th":[math_grade_10th], "11th":[math_grade_11th],"12th":[math_grade_12th]})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
# final_math_grade = students_data.set_index("school_name")
final_math_grade
203/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
reading_grade_9th = math_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
final_reading_grade = pd.DataFrame({"9th":[reading_grade_9th], "10th":[reading_grade_10th],
                                    "11th":[reading_grade_11th],"12th":[reading_grade_12th]})
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
# final_reading_grade = students_data.set_index("school_name")
final_reading_grade
203/18:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
final_reading_grade = pd.DataFrame({"9th":[reading_grade_9th], "10th":[reading_grade_10th],
                                    "11th":[reading_grade_11th],"12th":[reading_grade_12th]})
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
# final_reading_grade = students_data.set_index("school_name")
final_reading_grade
203/19:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
# final_math_grade = students_data.set_index("school_name")
final_math_grade
203/20:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade = students_data.set_index("school_name")
final_math_grade
203/21:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
#final_math_grade = students_data.set_index("school_name")
final_math_grade
203/22:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
final_reading_grade = pd.DataFrame({"9th":reading_grade_9th, "10th":reading_grade_10th,
                                    "11th":reading_grade_11th,"12th":reading_grade_12th})
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
204/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
204/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
204/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
204/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
204/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
204/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
204/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
204/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
204/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
204/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
204/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
204/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
204/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
204/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
204/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
204/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
final_math_grade = pd.DataFrame({"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th})
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
204/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
final_reading_grade = pd.DataFrame({"9th":reading_grade_9th, "10th":reading_grade_10th,
                                    "11th":reading_grade_11th,"12th":reading_grade_12th})
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
204/18:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
204/19:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
204/20:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
204/21:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
#create DF for avg_spending_range
for_each_school["spending_range"] = pd.cut(for_each_school["average_spending_range"], bins_to_group, labels = group_name_for_bins)
for_each_school
204/22:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = budget/size
#create DF for avg_spending_range
for_each_school["spending_range"] = pd.cut(for_each_school["average_spending_range"], bins_to_group, labels = group_name_for_bins)
for_each_school
204/23:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = schools_data["budget"]/schools_data["size"]
#create DF for avg_spending_range
for_each_school["spending_range"] = pd.cut(for_each_school["average_spending_range"], bins_to_group, labels = group_name_for_bins)
for_each_school
204/24:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = schools_data["budget"]/schools_data["size"]
#create DF for avg_spending_range
schools_data["spending_range"] = pd.cut(schools_data["average_spending_range"], bins_to_group, labels = group_name_for_bins)
schools_data
204/25:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/schools_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(complete_school_data["average_spending_range"], bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/26:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/aompleteschool_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(complete_school_data["average_spending_range"], bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/27:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(complete_school_data["average_spending_range"], bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/28:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut([]"average_spending_range"], bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/29:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(["average_spending_range"], bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/30:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut("average_spending_range", bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/31:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
complete_schools_data
204/32:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
# complete_schools_data
204/33:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#create DF for avg_spending_range
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
print(complete_schools_data)
204/34:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data.max()
204/35:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data
204/36:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data.head()
207/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
207/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
207/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
207/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
207/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
207/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
207/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
207/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
207/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
207/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
207/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
207/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
207/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
207/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
207/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
207/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
207/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
207/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data.head()
207/19:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data.max()
208/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
208/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
208/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
208/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
208/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
208/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
208/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
208/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
208/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
208/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
208/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
208/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
208/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
208/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
208/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
208/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
208/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
208/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the bins
complete_school_data = complete_school_data.groupby("spending_range")
complete_school_data.max()
208/19:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
209/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
209/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
209/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
209/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
209/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
209/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
209/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
209/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
209/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
209/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
209/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
209/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
209/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
209/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
209/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
209/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
209/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
209/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
209/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = complete_
209/20:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = = (numerator_math/students_data[total_students].count
spending_percent_math
# spending_percent_reading = 
# spending_overall_passing =
209/21:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/students_data[total_students].count
spending_percent_math
# spending_percent_reading = 
# spending_overall_passing =
209/22:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/students_data[total_students].count
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/students_data[total_students].count
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/23:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/students_data[total_students].count()
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/students_data[total_students].count
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/24:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/students_data["total_students"].count()
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/students_data[total_students].count
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/25:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/students_data[total_students].count
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/26:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_rate["math_score"].mean()
spending_avg_reading = student_spending_rate["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/27:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/28:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range.loc[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range.loc[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/29:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
209/30:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
210/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
210/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
210/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
210/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
210/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
210/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
210/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
210/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
210/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
210/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
210/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
210/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
210/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
210/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
210/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
210/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
210/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
210/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
210/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
210/20:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
211/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
211/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
211/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
211/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
211/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
211/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
211/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
211/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
211/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
211/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
211/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
211/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
211/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
211/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
211/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
211/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
211/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
211/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
211/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
211/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
212/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
212/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
212/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
212/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
212/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
212/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
212/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
212/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
212/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
212/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
212/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
212/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
212/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
212/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
212/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
212/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
212/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
212/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
212/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
213/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
213/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
213/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
213/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
213/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
213/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
213/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
213/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
213/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
213/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
213/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
213/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
213/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
213/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
213/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
213/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
213/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
213/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
213/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range.loc[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(students_data["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range.loc[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(students_data["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
213/20:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range.loc[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(student_spending_range["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range.loc[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(student_spending_range["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
214/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
214/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
214/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
214/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
214/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
214/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
214/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
214/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
214/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
214/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
214/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
214/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
214/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
214/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
214/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
214/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
214/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
214/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
214/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = student_spending_range.loc[student_spending_range["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(student_spending_range["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = student_spending_range.loc[student_spending_range["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(student_spending_range["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
214/20:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(student_spending_range["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(student_spending_range["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
215/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
215/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
215/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
215/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
215/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
215/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
215/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
215/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
215/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
215/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
215/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
215/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
215/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
215/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
215/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
215/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
215/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
215/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
215/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(student_spending_range["total_students"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(student_spending_range["total_students"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
216/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
216/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
216/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
216/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
216/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
216/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
216/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
216/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
216/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
216/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
216/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
216/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
216/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
216/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
216/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
216/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
216/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
216/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
216/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = numerator_math/(student_spending_range["Student ID"].count())
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = numerator_reading/(student_spending_range["Student ID"].count())
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
216/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
217/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
217/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
217/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
217/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
217/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
217/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
217/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
217/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
217/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
217/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
217/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
217/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
217/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
217/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
217/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
217/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
217/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
217/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
217/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
spending_overall_passing
217/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
217/21:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
# spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
# #percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
# numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
# numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
# spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
# spending_percent_math
# #percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
# numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
# numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
# spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
# spending_percent_reading
# #overall passing rate = Average of the spending_percent_math and spending_percent_reading
# spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
# spending_overall_passing
217/22:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
# spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
# #percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
# numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
# numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
# spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
# spending_percent_math
# #percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
# numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
# numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
# spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
# spending_percent_reading
# #overall passing rate = Average of the spending_percent_math and spending_percent_reading
# spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
# spending_overall_passing
217/23:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
217/24:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
# spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
spending_avg_reading
# #percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
# numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
# numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
# spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
# spending_percent_math
# #percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
# numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
# numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
# spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
# spending_percent_reading
# #overall passing rate = Average of the spending_percent_math and spending_percent_reading
# spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
# spending_overall_passing
217/25:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
# spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
# spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
# #percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
spending_percent_math
# #percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
# numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
# numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
# spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
# spending_percent_reading
# #overall passing rate = Average of the spending_percent_math and spending_percent_reading
# spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
# spending_overall_passing
217/26:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
218/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
218/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
218/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
218/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
218/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
218/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
218/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
218/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
218/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
218/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
218/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
218/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
218/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
218/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
218/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
218/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
218/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
218/18:
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
218/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
218/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
218/21:
#################################SCORES BY SCHOOL SIZE###################################
#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
218/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = student_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = student_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(student_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(student_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
218/23:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = student_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = student_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(scholl_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(student_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
220/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
220/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
220/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
220/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
220/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
220/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
220/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
220/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
220/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
220/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
220/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
220/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
220/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
220/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
220/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
220/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
220/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
220/18:
#################################SCORES BY SCHOOL SPENDING###################################
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
220/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
220/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
220/21:
#################################SCORES BY SCHOOL SIZE###################################
#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
220/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = student_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = student_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(scholl_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(student_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
221/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
221/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
221/3:
#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
221/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
221/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
221/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
221/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
221/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
221/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
221/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
221/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
221/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
221/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
221/14:
for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
221/15:
for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
221/16:
#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
221/17:
#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
221/18:
#################################SCORES BY SCHOOL SPENDING###################################
# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
221/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
221/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
221/21:
#################################SCORES BY SCHOOL SIZE###################################
#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
221/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = student_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = student_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
221/23:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
221/24:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
221/25:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("Total Students")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("Total Students")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
221/26:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
229/1:
import numpy as np
import matplotlib.pyplot as plt
229/2: np.arange(0, 5,0.1)
229/3: x_axis = np.arange(0.5,0.1)
229/4: x_axis
229/5: x_axis = np.arange(0,5,0.1)
229/6: x_axis
230/1: import matplotlib.pyplot as plt
230/2:
import matplotlib.pyplot as plt
plt.plot([1,2,3,4])
plt.show()
230/3:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7])
plt.show()
230/4:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],[2,3,4,5,6,7,8])
plt.show()
230/5:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],[a,b,c,d,e,f,g])
plt.show()
230/6:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"])
plt.show()
230/7:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"ro")
plt.show()
230/8:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"go")
plt.show()
230/9:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"b-")
plt.show()
230/10:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"r-")
plt.show()
230/11:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"r^")
plt.show()
230/12: plt.plot(["a","b","c","d","e"])
230/13:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"w^")
plt.show()
230/14:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7],["a","b","c","d","e","f","g"],"y^")
plt.show()
230/15:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7])
plt.axis([0,6,2,4])
230/16:
plt.plot([0,1,2,3,4,5,6],[1,2,3,4,5,6,7])
plt.axis([0,6,2,4])
plt.show()
230/17:
import numpy as np
import matplotlib.pyplot as plt
plt.plot([1,2,3,4])
plt.show()
230/18:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time**2,"bs", time,time**4,"g^")
plt.show()
230/19:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time**4,"bs", time,time**7,"g^")
plt.show()
230/20:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time**2,"bs", time,time**3,"g^")
plt.show()
230/21:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time,"bs", time,time,"g^")
plt.show()
230/22:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time*2,"bs", time,time,"g^")
plt.show()
230/23:
time = np.arange(0.,5.,0.2)
plt.plot(time,time,"r--",time,time**2,"bs", time,time,"g^")
plt.show()
230/24:
hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
happiness = [9.8, 9.9, 9.2, 8.6, 8.3, 9.0, 8.7, 9.1, 7.0, 6.4, 6.9, 7.5]
plt.plot(hours,happiness)
plt.show()
230/25:
plt.xlabel("Time of day")
plt.ylabel("Happiness Rating(out of 10)")
plt.title("My self-reported Happiness while awake")
230/26:
hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
happiness = [9.8, 9.9, 9.2, 8.6, 8.3, 9.0, 8.7, 9.1, 7.0, 6.4, 6.9, 7.5]
plt.plot(hours,happiness)
plt.xlabel("Time of day")
plt.ylabel("Happiness Rating(out of 10)")
plt.title("My self-reported Happiness while awake")
plt.show()
230/27:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.show()
230/28: help(plt.subplot)
230/29:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.subplot_adjust(left=0.5)
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.show()
230/30:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.subplots_adjust(left=0.5)
plt.show()
230/31:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.subplots_adjust(left=0.1)
plt.show()
230/32:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.subplots_adjust(left=0.1,wspace=o.35)
plt.show()
230/33:
x = [1,2,3,4]
y = [1,2,3,4]

plt.subplot(1,2,1)
plt.plot(x,y, color ="green")
plt.title("first Subplot")

plt.subplot(1,2,2)
plt.plot(x,y,color="red")
plt.title("Second Subplot")
plt.subplots_adjust(left=0.1,wspace=0.35)
plt.show()
230/34:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
#plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.show()
230/35:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.show()
230/36:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.show()
230/37:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.show()
230/38:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.legend(["Parabola", "Cubic"])
plt.show()
230/39:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],label = "Parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64], label = "Cubic")
plt.legend()
plt.show()
230/40:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],label = "Parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64], label = "Cubic")
#plt.legend()
plt.show()
230/41:
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],label = "Parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64], label = "Cubic")
plt.legend()
plt.show()
233/1: #help(plt.plot)
238/1:
plt.plot(gyms, members)
plt.show()
239/1:
plt.plot(members, gyms)
plt.show()
239/2:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
239/3:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
239/4:
plt.plot(members, gyms)
plt.show()
239/5:
plt.plot(gyms,members)
plt.show()
239/6:
plt.plot(x_axis, sin)
plt.show()
239/7:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
239/8:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
239/9:
plt.plot(gyms,members)
plt.show()
239/10:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
239/11:
plt.plot(x_axis, sin)
plt.show()
243/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
243/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
243/3:
plt.plot(gyms,members)
plt.show()
243/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
243/5:
plt.plot(x_axis, sin)
plt.show()
243/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
243/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.show()
243/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
244/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
244/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
244/3:
plt.plot(gyms,members)
plt.show()
244/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
244/5:
plt.plot(x_axis, sin)
plt.show()
244/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
244/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
244/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
244/9:
plt.plot(x_axis,times)
plt.show()
245/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
245/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
245/3:
plt.plot(gyms,members)
plt.show()
245/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
245/5:
plt.plot(x_axis, sin)
plt.show()
245/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
245/7:
plt.pie(x_axis, explode=explode, colors=colors,labels=gyms, )
plt.axis("equal")
plt.show()
245/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
245/9:
plt.plot(x_axis,times)
plt.show()
246/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
246/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
246/3:
plt.plot(gyms,members)
plt.show()
246/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
246/5:
plt.plot(x_axis, sin)
plt.show()
246/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
246/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms, )
plt.axis("equal")
plt.show()
246/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
246/9:
plt.plot(x_axis,times)
plt.show()
247/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
247/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
247/3:
plt.plot(gyms,members)
plt.show()
247/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
247/5:
plt.plot(x_axis, sin)
plt.show()
247/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
247/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
247/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
247/9:
plt.plot(x_axis,times)
plt.show()
247/10:
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
248/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
248/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
248/3:
plt.plot(gyms,members)
plt.show()
248/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
248/5:
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
248/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
248/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
248/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
248/9:
plt.plot(x_axis,times)
plt.show()
249/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
249/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
249/3:
plt.plot(gyms,members)
plt.show()
249/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
249/5:
plt.title("sin from 0 to 2$\pi$")
plt.xlabel("Real numbers from 0 to 2$\pi$")
plt.ylabel("sin(x)")
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
249/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
249/7:
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
249/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
249/9:
plt.plot(x_axis,times)
plt.show()
250/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
250/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
250/3:
plt.plot(gyms,members)
plt.show()
250/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
250/5:
plt.title("sin from 0 to 2$\pi$")
plt.xlabel("Real numbers from 0 to 2$\pi$")
plt.ylabel("sin(x)")
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
250/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.05, 0, 0)
250/7:
plt.title("NYC gym popularity")
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
250/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
250/9:
plt.plot(x_axis,times)
plt.show()
251/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
251/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
251/3:
plt.plot(gyms,members)
plt.show()
251/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
251/5:
plt.title("sin from 0 to 2$\pi$")
plt.xlabel("Real numbers from 0 to 2$\pi$")
plt.ylabel("sin(x)")
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
251/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.5, 0, 0)
251/7:
plt.title("NYC gym popularity")
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
251/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
251/9:
plt.plot(x_axis,times)
plt.show()
252/1:
# Import Dependencies
import numpy as np
import matplotlib.pyplot as plt
252/2:
# DATASET 1
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
252/3:
plt.plot(gyms,members)
plt.show()
252/4:
# DATASET 2
x_lim = 2 * np.pi
x_axis = np.arange(0, x_lim, 0.1)
sin = np.sin(x_axis)
252/5:
plt.title("sin from 0 to 2$\pi$")
plt.xlabel("Real numbers from 0 to 2$\pi$")
plt.ylabel("sin(x)")
plt.plot(x_axis, sin, marker= "o", linewidth = 1, color = "Y")

plt.show()
252/6:
# DATASET 3
gyms = ["Crunch", "Planet Fitness", "NY Sports Club", "Rickie's Gym"]
members = [49, 92, 84, 53]
x_axis = np.arange(0, len(gyms))
colors = ["yellowgreen", "red", "lightcoral", "lightskyblue"]
explode = (0, 0.5, 0, 0)
252/7:
plt.title("NYC gym popularity")
plt.pie(members, explode=explode, colors=colors,labels=gyms)
plt.axis("equal")
plt.show()
252/8:
# DATASET 4
x_axis = np.arange(0, 10, 0.1)
times = []
for x in x_axis:
    times.append(x * x + np.random.randint(0, np.ceil(max(x_axis))))
252/9:
plt.scatter(x_axis,times, marker ="o", color="Y")
plt.show()
254/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
254/2:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
254/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
254/4:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
254/5:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df_dtypes()
254/6: %matplotlib notebook
254/7:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
254/8:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.Sdtypes()
254/9:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
254/10:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes()
254/11:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
254/12:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes
254/13:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]
254/14:
# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
254/15:
# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
254/16:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
254/17:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
254/18:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
255/1: %matplotlib notebook
255/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
255/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes
255/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]
255/5:
# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
255/6:
# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
255/7:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
255/8:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
255/9:
# Filter the DataFrame down only to those columns to chart
state_and_inches = rain_df[["State","Inches"]]

# Set the index to be "State" so they will be used as labels
state_and_inches = state_and_inches.set_index("State")

state_and_inches.head()
255/10:
# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3))

# Set a title for the chart
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()
255/11:
# Pandas can also plot multiple columns if the DataFrame includes them
multi_plot = rain_df.plot(kind="bar", figsize=(20,5))

# PandasPlot.set_xticklabels() can be used to set the tick labels as well
multi_plot.set_xticklabels(rain_df["State"], rotation=45)

plt.show()
plt.tight_layout()
255/12:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()
255/13:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()
256/1: %matplotlib notebook
256/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
256/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes
256/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]
256/5:
# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
256/6:
# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
256/7:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()
256/8:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
256/9:
# Filter the DataFrame down only to those columns to chart
state_and_inches = rain_df[["State","Inches"]]

# Set the index to be "State" so they will be used as labels
state_and_inches = state_and_inches.set_index("State")

state_and_inches.head()
256/10:
# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3))

# Set a title for the chart
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()
256/11:
# Pandas can also plot multiple columns if the DataFrame includes them
multi_plot = rain_df.plot(kind="bar", figsize=(20,5))

# PandasPlot.set_xticklabels() can be used to set the tick labels as well
multi_plot.set_xticklabels(rain_df["State"], rotation=45)

plt.show()
plt.tight_layout()
256/12:
# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3))

# Set a title for the chart
#all the 3 below function are similar to the plot method
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()
help(pd.DataFrame.plot)
257/1: %matplotlib notebook
257/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
257/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes
257/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]
257/5:
# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
257/6:
# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
257/7:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()
257/8:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
257/9:
# Filter the DataFrame down only to those columns to chart
state_and_inches = rain_df[["State","Inches"]]

# Set the index to be "State" so they will be used as labels
state_and_inches = state_and_inches.set_index("State")

state_and_inches.head()
257/10:
# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3))

# Set a title for the chart
#all the 3 below function are similar to the plot method
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()
help(pd.DataFrame.plot)
257/11:
# Pandas can also plot multiple columns if the DataFrame includes them
multi_plot = rain_df.plot(kind="bar", figsize=(20,5))

# PandasPlot.set_xticklabels() can be used to set the tick labels as well
multi_plot.set_xticklabels(rain_df["State"], rotation=45)

plt.show()
plt.tight_layout()
258/1: %matplotlib notebook
258/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
258/3:
# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()
len(rain_df)
rain_df.count()
rain_df.dtypes
258/4:
# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]
258/5:
# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,3))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
258/6:
# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
258/7:
# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()
258/8:
# Save our graph and show the grap
#easy way to fix wierd looking files
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()
258/9:
# Filter the DataFrame down only to those columns to chart
state_and_inches = rain_df[["State","Inches"]]

# Set the index to be "State" so they will be used as labels
state_and_inches = state_and_inches.set_index("State")

state_and_inches.head()
258/10:
# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3))

# Set a title for the chart
#all the 3 below function are similar to the plot method
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()
help(pd.DataFrame.plot)
258/11:
# Pandas can also plot multiple columns if the DataFrame includes them
#multi_plot = rain_df.plot(kind="bar", figsize=(20,5))
multi_plot = rain_df[["Inches", "Rank"]].plot(kind="bar", figsize=(20,5))

# PandasPlot.set_xticklabels() can be used to set the tick labels as well
multi_plot.set_xticklabels(rain_df["State"], rotation=45)

plt.show()
plt.tight_layout()
259/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/2:
# Read CSV
battling_kings = pd_read.csv("Resources","got.csv")
battling_kings.head()
259/3:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/4:
# Read CSV
battling_kings = pd.read_csv("Resources","got.csv")
battling_kings.head()
259/5:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/6:
# Read CSV
battling_kings = pd.read_csv("http://localhost:8888/edit/Class_Work/05-Matplotlib/2/Activities/03-Stu_BattlingKings/Unsolved/Resources/got.csv")
battling_kings.head()
259/7:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/8:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/9:
# Read CSV
battling_kings = pd.read_csv(C:\\Users\\Subha\\Master Repo\\Working Class Repo\\Class_Work\\05-Matplotlib\\2\\Activities\\03-Stu_BattlingKings\\Unsolved\\Resources\\got.csv)
battling_kings.head()
259/10:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
259/11:
# Read CSV
battling_kings = pd.read_csv("./Resources/got.csv")
battling_kings.head()
259/12:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
259/13:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
attacker.head()
defender.head()
259/14:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
attacker.head()
259/15:
defender.head()

# Get total battle data
259/16:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
attacker.head()
attacker.count()
259/17:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
attacker.head()
attacker.count()
attacker= attacker.dropna(how="any")
259/18:
# Get attacker and defender data
attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
attacker.head()
attacker.count()
attacker= attacker.dropna(how="any")
attacker.count()
259/19:
defender.head()
defender.count()

# Get total battle data
259/20:
defender.head()
defender.count()
defender= defender.dropna(how="any")
defender.count()

# Get total battle data
259/21:
# Get attacker and defender data
# attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
# defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
# attacker.head()
# attacker.count()
# attacker= attacker.dropna(how="any")
# attacker.count()
#create series
attacker = battling_kings["attacker_king"].value_counts
259/22:
# Get attacker and defender data
# attacker = battling_kings[["attacker_king", "attacker_1","attacker_2","attacker_3","attacker_4","attacker_size","attacker_commander"]]
# defender = battling_kings[["defender_king", "defender_1","defender_size", "defender_commander"]]
# attacker.head()
# attacker.count()
# attacker= attacker.dropna(how="any")
# attacker.count()
#create series
attacker = battling_kings["attacker_king"].value_counts
defender = battling_kings["defender_king"].value_counts
261/1:
import matplotlib.pypolt as plt
import pandas as pd
import numpy as np
261/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
261/3: bike = pd.read_csv("../Resources/trip.csv")
261/4: bike = pd.read_csv("../Resources/trip.csv")
261/5: bike = pd.read_csv("./Resources/trip.csv")
261/6:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
261/7: bike = pd.read_csv("./Resources/trip.csv")
261/8:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
261/9: bike = pd.read_csv("../Resources/trip.csv")
261/10:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
261/11:
bike_gender = bike.groupby["gender"]
bike_gender
261/12:
bike_gender = bike.groupby("gender")
bike_gender
261/13:
bike_gender = bike.groupby("gender").value_count()
bike_gender
261/14:
bike_gender = bike.groupby("gender")
bike_gender.value_count
261/15:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
261/16:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
261/17:
bike_gender = bike.groupby("gender")
bike_gender.value_count
261/18:
bike_gender = bike.groupby("gender")
bike_gender.count()
261/19:
bike_gender = bike.groupby("gender")
bike_gender.count()
bike_gender = plt.bar(x_axis,bike["gender"],color="r",align="center")
bike_gender.show()
263/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
263/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
263/3:
bike_gender = bike.groupby("gender")
bike_gender.count()
x_axis = np.arange(len(bike))
tick_locations = [value for value in x_axis]
bike_gender = plt.bar(,bike["gender"],color="r",align="center")
bike_gender.show()
264/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
264/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
264/3:
bike_gender = bike.groupby("gender")
bike_gender.count()
x_axis = np.arange(len(bike))
tick_locations = [value for value in x_axis]
bike_gender = plt.bar(x_axis,bike["gender"],color="r",align="center")
bike_gender.show()
264/4:
bike_gender = bike.groupby("gender")
bike_gender.count()
bike_gender.plot(kind="bar", figsize=(20,3))
bike_gender.show()
265/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
265/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
265/3:
bike_gender = bike.groupby("gender")
bike_gender.count()
bike_gender.plot(kind="bar", figsize=(20,3))
bike_gender.show()
267/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
267/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
267/3:
bike_gender = bike.groupby("gender")
bike_gender.count(["Females","Males","Other"])
bike_gender.plot(kind="bar", figsize=(20,3))
bike_gender.show()
267/4:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
267/5:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
267/6:
bike_gender = bike.groupby("gender")
bike_gender.count(["Females","Males","Other"])
bike_gender.plot(kind="bar", figsize=(20,3))
bike_gender.show()
267/7:
bike_gender = bike.groupby("gender")
bike_gender.count
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/8:
#bike_gender = bike.groupby("gender")
bike_gender.count
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/9:
#bike_gender = bike.groupby("gender")
bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/10:
#bike.count()
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/11:
bike.count()
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/12:
bike.dtypes
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
267/13:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
trips = trips.drop(trips.index[3])
267/14:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
268/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
268/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
268/3: bike.count()
268/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
268/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
trips = trips.drop(trips.index[3])
268/6:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
trips = trips.drop(trips.index[3])
trips
268/7:
bike_gender = trip.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
269/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
269/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
269/3: bike.count()
269/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
269/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
269/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
269/7:
# 
bike_grp = bike_gender.groupby(["bikeid","gender"])
total_bike = bike_grp.sum()
270/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
270/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
270/3: bike.count()
270/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
270/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
270/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
270/7:
# 
bike_grp = bike_gender.groupby(["bikeid","gender"])
total_bike = bike_grp.sum()
270/8:
# 
bike_grp = bike_gender.groupby(["bikeid","gender"])
total_bike = bike_grp.sum()
total_bike.head()
271/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
271/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
271/3: bike.count()
271/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
271/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
271/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
271/7:
# 
bike_grp = bike_gender.groupby(["bikeid","gender"])
total_bike = bike_grp.sum()
total_bike.head()
272/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
272/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
272/3: bike.count()
272/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
272/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
272/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
272/7:
# 
bike_grp = bike_gender.groupby(["gender","bikeid"])
total_bike = bike_grp.sum()
total_bike.head()
273/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
273/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
273/3: bike.count()
273/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
273/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
273/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
273/7:

bike_grp = bikes.groupby(["gender","bikeid"])
total_bike = bike_grp.sum()
total_bike.head()
275/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
275/2:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
275/3: bike.count()
275/4:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
275/5:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
275/6:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
275/7:

bike_grp = bike.groupby(["gender","bikeid"])
total_bike = bike_grp.sum()
total_bike.head()
275/8:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
275/9:
bike = pd.read_csv("../Resources/trip.csv")
bike.head()
275/10: bike.count()
275/11:
bike.dtypes
#return an object for birthyear since there are some null values
#bike_gender = bike.groupby("gender")
#bike_gender.count()
# bike_gender.plot(kind="bar", figsize=(20,3))
# bike_gender.show()
275/12:
bike_gender = bike.groupby("gender")
trips = bike_gender["tripduration"].count()
#dropping stoptime since there's just 1 value and it is indexed at 3
trips = trips.drop(trips.index[3])
trips
275/13:
bike_gender = trips.plot(kind="bar", figsize=(15,3))
plt.show()
plt.tight_layout()
275/14:

bike_grp = bike.groupby(["gender","bikeid"])
total_bike = bike_grp.sum()
total_bike.head()
276/1:
import matplotlib.pyplot as plt
import pandas pd
import numpy as np
276/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
276/3: miles = pd.read_csv("../Resources/mpg.csv)
276/4:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
276/5: miles = pd.read_csv("../Resources/mpg.csv")
276/6: miles = pd.read_csv("../Resources/mpg.csv")
276/7:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
276/8: miles.count()
276/9: miles.dtypes
276/10: miles.value_count()
276/11: miles["horsepower"].value_count()
277/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
277/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
277/3: miles.count()
277/4: miles.dtypes
277/5: miles["horsepower"].value_count()
277/6: miles = miles["horsepower"].value_count()
278/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
278/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
278/3: miles.count()
278/4: miles.dtypes
278/5: miles = miles["horsepower"].value_count()
279/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
279/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
279/3: miles.count()
279/4: miles.dtypes
279/5: miles = miles["horsepower"].value_counts()
279/6: miles_err = miles["horsepower"].value_counts()
279/7: miles_err = miles["horsepower"].value_counts()
280/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
280/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
280/3: miles.count()
280/4: miles.dtypes
280/5:
miles_err = miles["horsepower"].value_counts()
miles.err
281/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
281/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
281/3: miles.count()
281/4: miles.dtypes
281/5:
miles_err = miles["horsepower"].value_counts()
miles.err.head()
281/6:
miles_err = miles["horsepower"].value_counts()
miles.err.head()
282/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
282/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
282/3: miles.count()
282/4: miles.dtypes
282/5:
miles_err = miles["horsepower"].value_counts()
miles_err.head()
282/6:
miles_err = miles["horsepower"].value_counts()
miles_err
282/7:
miles_err = miles["horsepower"].value_counts()
miles_err.head()
miles = miles_err[miles_err[horsepower == "?",:]]
miles
282/8:
miles_err = miles["horsepower"].value_counts()
miles_err.head()
miles = miles_err[miles_err["horsepower" == "?",:]]
miles
283/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
283/2:
miles = pd.read_csv("../Resources/mpg.csv")
miles.head()
283/3: miles.count()
283/4: miles.dtypes
283/5:
miles_err = miles["horsepower"].value_counts()
miles_err.head()
miles = miles_err[miles_err["horsepower" == "?",:]]
miles
284/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
284/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
284/3:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
284/4:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
284/5:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010"; "Year 2010"})
combined_unemployed_data.head()
284/6:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
284/7:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
284/8:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
284/9:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
284/10:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

# Collect the years where data was collected
years = average_unemployment.keys()
284/11:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
285/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
285/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
285/3:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
285/4:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
285/5:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

# Collect the years where data was collected
years = average_unemployment.keys()
285/6:
# Plot the world average as a line chart
world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
                        color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
285/7:
average_unemployment.plot(label="World Average")
combined_unemployed_data.loc['USA', "2010":"2014"].plot(label="United States")
plt.legend()
plt.show()
285/8:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
combined_unemployed_data
286/1:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2,wwe_3, wwe_4, on="Wrestler")
combined_dat
286/2:
# Import the necessary modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
286/3:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2,wwe_3, wwe_4, on="Wrestler")
combined_data
286/4:
# Import the necessary modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
286/5:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2,wwe_3, wwe_4, on="Wrestler")
combined_data.head()
286/6:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2,wwe_3, wwe_4, on ="Wrestler")
combined_data.head()
286/7:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2,wwe_3, wwe_4, on ="Wrestler")
combined_data.head()
help(pd.merge)
286/8:
# Import the necessary modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
286/9:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2, on ="Wrestler")
combined_data.head()
help(pd.merge)
286/10:
# Bring each CSV into a separate data frame
wwe_1 = pd.read_csv("../Resources/WWE-Data-2013.csv")
wwe_2 = pd.read_csv("../Resources/WWE-Data-2014.csv")
wwe_3 = pd.read_csv("../Resources/WWE-Data-2015.csv")
wwe_4 = pd.read_csv("../Resources/WWE-Data-2016.csv")

combined_data = pd.merge(wwe_1, wwe_2, how ="outer", on ="Wrestler")
combined_data.head()
#help(pd.merge)
291/1:
new = pd.DataFrame({"name":["shu","iksh","ishy","pra"],"no":[3,4,5,3]})
new
291/2:
import pandas as pd
new = pd.DataFrame({"name":["shu","iksh","ishy","pra"],"no":[3,4,5,3]})
new
291/3:
import pandas as pd
new = pd.DataFrame({"name":["shu","iksh","ishy","pra"],"no":[3,4,5,3],"age":[38,7,2,41]})
new
291/4:
import pandas as pd
new = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west'])
north = new["clinic_north"]
north
291/5: type(north)
291/6: type(new)
291/7:
new_1 = new[["clinic_north","clinic_south"]]
new_1
291/8: type(new_1)
291/9: march = new.iloc[2]
291/10:
march = new.iloc[2]
march
291/11:
call_cols = new.iloc["clinic_north","clinic_south"]
call_cols
291/12:
call_cols = new.loc["clinic_north","clinic_south"]
call_cols
292/1:
import pandas as pd
new = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west'])
north = new["clinic_north"]
north
292/2: type(north)
292/3: type(new)
292/4:
new_1 = new[["clinic_north","clinic_south"]]
new_1
292/5: type(new_1)
292/6:
march = new.iloc[2]
march
292/7:
call_cols = new.loc["clinic_north","clinic_south"]
call_cols
292/8:
call_cols = new[["clinic_north","clinic_south"]]
call_cols
292/9:
jan = new.iloc[0]
jan
292/10: type(jan)
292/11:
january = new[new.month==January]
january
292/12:
january = new[new.month=="January"]
january
292/13: type(january)
292/14:
january = new[new.clinc_east=>50]
january
292/15:
january = new[new.clinc_east=>50]
january
292/16:
january = new[new.clinc_east>50]
january
292/17:
january = new[new.clinic_east>50]
january
292/18:
january = new[new.clinic_east<50]
january
292/19:
january = new[new.clinic_east==100]
january
292/20:
january = new[new["clinic_east"==100]
january
292/21:
january = new[new("clinic_east"==100)]
january
292/22:
january = new[("clinic_east"==100)]
january
292/23:
march = new.iloc[2,3]
march
292/24:
march = new.iloc[2][3]
march
292/25:
march = new.iloc[2][3]
march
292/26:
import pandas as pd
new = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west'])
north = new["clinic_north"]
north
292/27:
march = new.iloc[2][3]
march
292/28:
new_1 = new[["clinic_north","clinic_south"]]
new_1
292/29:
import pandas as pd
new = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west'])
north = new["clinic_north"]
north
292/30: type(north)
292/31:
march = new.iloc[2][3]
march
292/32:
march = new.iloc[2]
march
292/33:
march = new[new.month == "March"]
march
292/34:
row_call = new[(new.month == "March")&(new.month=="April")]
row_call
292/35:
row_call = new[(new.month == "March")|(new.month=="April")]
row_call
#march_april = df[(df.month == 'March') | (df.month == 'April')]
292/36:
row_call = new[(new.month == "March")&(new.month=="April")]
row_call
#march_april = df[(df.month == 'March') | (df.month == 'April')]
292/37:
row_call = new[(new.month == "March")(new.month=="April")]
row_call
#march_april = df[(df.month == 'March') | (df.month == 'April')]
292/38:
row_call = new[(new.month == "March")&(new.month=="April")]
row_call
#march_april = df[(df.month == 'March') | (df.month == 'April')]
294/1:
# Import Dependencies
import pandas as pd
294/2:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
data = {"Frame":["Ornate", "Classical","Modern","Wood","Cardboard"], "Price":[15,12.5,10,5,1],"Sales":[100,200,150,300,"N/A"]}
new_data = pd.DataFrame(data)
new_data
295/1:
# Import Dependencies
import pandas as pd
295/2:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
data = {"Frame":["Ornate", "Classical","Modern","Wood","Cardboard"], "Price":[15,12.5,10,5,1],"Sales":[100,200,150,300,"N/A"]}
new_data = pd.DataFrame(data)
new_data
295/3:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
295/4:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
art = [{"Painting":"Mona Lisa (Knockoff)","Popularity":"Very Popular","Price": 25},
       {"Painting":"Van Gogh (Knockoff)","Popularity":"Popular","Price": 20},
       {"Painting":"Starving Artist","Popularity":"Average","Price": 10},
       {"Painting":"Toddler Drawing","Popularity":"Not Popular","Price": 1}]
art_new = pd.DataFrame(art).head()
295/5:
# Import Dependencies
import pandas as pd
295/6:
# DataFrame should have 3 columns: Frame, Price, and Sales AND 5 rows of data
data = {"Frame":["Ornate", "Classical","Modern","Wood","Cardboard"], "Price":[15,12.5,10,5,1],"Sales":[100,200,150,300,"N/A"]}
new_data = pd.DataFrame(data)
new_data
295/7:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
art = [{"Painting":"Mona Lisa (Knockoff)","Popularity":"Very Popular","Price": 25},
       {"Painting":"Van Gogh (Knockoff)","Popularity":"Popular","Price": 20},
       {"Painting":"Starving Artist","Popularity":"Average","Price": 10},
       {"Painting":"Toddler Drawing","Popularity":"Not Popular","Price": 1}]
art_new = pd.DataFrame(art).head()
295/8:
# Use a different method of creating DataFrames to
# Create a DataFrame for an art gallery that contains three columns - "Painting", "Price", and "Popularity"
# and has 4 rows of data
art = [{"Painting":"Mona Lisa (Knockoff)","Popularity":"Very Popular","Price": 25},
       {"Painting":"Van Gogh (Knockoff)","Popularity":"Popular","Price": 20},
       {"Painting":"Starving Artist","Popularity":"Average","Price": 10},
       {"Painting":"Toddler Drawing","Popularity":"Not Popular","Price": 1}]
art_new = pd.DataFrame(art)
art_new
292/39: new["clinic_north"].describe()
292/40: new["clinic_north"].unique()
296/1:
# The value_counts method counts unique values in a column
count = data_file_pd["Last Name"].value_counts()
count
297/1:
# Dependencies
import pandas as pd
297/2:
# Save path to data set in a variable
data_file = "Resources/dataSet.csv"
297/3:
# Use Pandas to read data
data_file_pd = pd.read_csv(data_file)
data_file_pd.head()
297/4:
# Display a statistical overview of the DataFrame
data_file_pd.describe()
297/5:
# Reference a single column within a DataFrame
data_file_pd["Amount"].head()
297/6:
# Reference multiple columns within a DataFrame
data_file_pd[["Amount", "Gender"]].head()
297/7:
# The mean method averages the series
average = data_file_pd["Amount"].mean()
average
297/8:
# The sum method adds every entry in the series
total = data_file_pd["Amount"].sum()
total
297/9:
# The unique method shows every element of the series that appears only once
unique = data_file_pd["Last Name"].unique()
unique
297/10:
# The value_counts method counts unique values in a column
count = data_file_pd["Last Name"].value_counts()
count
297/11:
# Calculations can also be performed on Series and added into DataFrames as new columns
thousands_of_dollars = data_file_pd["Amount"]/1000
data_file_pd["Thousands of Dollars"] = thousands_of_dollars

data_file_pd.head()
298/1:
# Dependencies
import pandas as pd
298/2:
# Save path to data set in a variable
data_file = "Resources/dataSet.csv"
298/3:
# Use Pandas to read data
data_file_pd = pd.read_csv(data_file)
data_file_pd.head()
298/4:
# Display a statistical overview of the DataFrame
data_file_pd.describe()
298/5:
# Reference a single column within a DataFrame
data_file_pd["Amount"].head()
298/6:
# Reference multiple columns within a DataFrame
data_file_pd[["Amount", "Gender"]].head()
298/7:
# The mean method averages the series
average = data_file_pd["Amount"].mean()
average
298/8:
# The sum method adds every entry in the series
total = data_file_pd["Amount"].sum()
total
298/9:
# The unique method shows every element of the series that appears only once
unique = data_file_pd["Last Name"].unique()
unique
298/10:
# The value_counts method counts unique values in a column
data_file_pd["Last Name"].value_counts()
298/11:
# Calculations can also be performed on Series and added into DataFrames as new columns
thousands_of_dollars = data_file_pd["Amount"]/1000
data_file_pd["Thousands of Dollars"] = thousands_of_dollars

data_file_pd.head()
299/1:
# Collecting a summary of all numeric data
training_data.describe()
299/2:
# Import Dependencies
import pandas as pd
import random
299/3:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head(10)
299/4:
# Collecting a summary of all numeric data
training_data.describe()
299/5:
# Finding the names of the trainers
training_data["Trainers"].value_counts()
301/1:
# Import Dependencies
import pandas as pd
import random
301/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head(10)
301/3:
# Collecting a summary of all numeric data
training_data.describe()
301/4:
# Finding the names of the trainers
training_data["Trainers"].value_counts()
302/1:
# Import Dependencies
import pandas as pd
import random
302/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head(10)
302/3:
# Collecting a summary of all numeric data
training_data.describe()
302/4:
# Finding the names of the trainers
training_data["Trainer"].value_counts()
302/5: # Finding how many students each trainer has
302/6: # Finding the average weight of all students
302/7: # Finding the combined weight of all students
302/8: # Converting the membership days into weeks and then adding a column to the DataFrame
303/1:
# Import Dependencies
import pandas as pd
import random
303/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head(10)
303/3:
# Collecting a summary of all numeric data
training_data.describe()
303/4:
# Finding the names of the trainers
training_data["Trainer"].unique()
303/5: # Finding how many students each trainer has
303/6: # Finding the average weight of all students
303/7: # Finding the combined weight of all students
303/8: # Converting the membership days into weeks and then adding a column to the DataFrame
303/9:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head()
303/10:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data
303/11:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head()
303/12:
# Finding how many students each trainer has
training_data["Trainer"].value_counts()
303/13:
# Finding how many students each trainer has
training_data["Trainer"].value_counts()
303/14:
# Finding the average weight of all students
training_data["Weight"].mean()
303/15:
# Finding the combined weight of all students
training_data["Weight"].sum()
303/16:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_data["Membership(Days)"]
training_data["Weeks"] = weeks
training_data.head()
303/17:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_data["Membership(Days)"]/7
training_data["Weeks"] = weeks
training_data.head()
305/1:
import pandas as pd
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
trining_data.head()
305/2:
import pandas as pd
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head()
305/3: training_data.columns
305/4: training_data.rows
305/5:
new = training_data[["Membership(Days)","Name","Trainer","Weight"]]
new.head()
305/6: training_data[["Membership(Days)","Name","Trainer","Weight"]]
305/7:
training_data["Weight"].rename(columns={"Weight":"Weight(in Pounds)"})
training_data.head()
305/8:
df = training_data["Weight"].rename(columns={"Weight":"Weight(in Pounds)"})
df.head()
305/9:
training_data.rename(columns={"Weight":"Weight(in Pounds)"})
training_data.head()
305/10:
training_data.rename(columns={"Weight":"Weight in Pounds"})
training_data.head()
305/11:
new = training_data.rename(columns={"Weight":"Weight in Pounds"})
new.head()
305/12:
new = training_data.replace(columns={"Weight":"Weight in Pounds"})
new.head()
305/13:
new = training_data.rename(columns={"Weight":"Weight in Pounds"})
new.head()
306/1:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
306/2:
# import dependencies
import pandas as pd
306/3:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
306/4:
# Rename columns to clean up the look
hey_arnold.rename(columns=={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
hey_arnold
306/5:
# Rename columns to clean up the look
new = hey_arnold.rename(columns=={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
306/6:
# import dependencies
import pandas as pd
306/7:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
306/8:
# Rename columns to clean up the look
new = hey_arnold.rename(columns=={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
306/9:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
306/10:
# import dependencies
import pandas as pd
306/11:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
306/12:
# Rename columns to clean up the look
new = hey_arnold.rename(columns=={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
306/13:
# Rename columns to clean up the look
hey_arnold.rename(columns={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
hey_arnold
306/14:
# Rename columns to clean up the look
new = hey_arnold.rename(columns={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
306/15:
# Organize columns into a more logical order
new[["Condition","Height","Hair_color","Actor"]]
307/1:
# import dependencies
import pandas as pd
307/2:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
307/3:
# Rename columns to clean up the look
new = hey_arnold.rename(columns={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
307/4:
# Organize columns into a more logical order
new[["Condition","Height","Hair_color","Actor"]]
307/5:
# Organize columns into a more logical order
new[["Condition","Height","Hair_color","Actor"]]
new
308/1:
# import dependencies
import pandas as pd
308/2:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
308/3:
# Rename columns to clean up the look
new = hey_arnold.rename(columns={"Character_in_show":"Actor","color_of_hair":"Hair_Color","Football_Shaped_Head":"Condition"})
new
308/4:
# Organize columns into a more logical order
new[["Condition","Height","Hair_color","Actor"]]
new
308/5:
# Organize columns into a more logical order
new[["Condition","Height","Hair_Color","Actor"]]
310/1:
# Import Dependencies
import pandas as pd
310/2:
# Make a reference to the books.csv file path

file_path = "books.csv"
df = pd.DataFrame(file_path)
# Import the books.csv file as a DataFrame
310/3:
# Import Dependencies
import pandas as pd
310/4:
# Make a reference to the books.csv file path

file_path = "books.csv"
df = pd.read_csv(file_path)
# Import the books.csv file as a DataFrame
310/5:
# Import Dependencies
import pandas as pd
310/6:
# Make a reference to the books.csv file path

file_path = "Resources","books.csv"
df = pd.read_csv(file_path)
# Import the books.csv file as a DataFrame
310/7:
# Import Dependencies
import pandas as pd
310/8:
# Make a reference to the books.csv file path

file_path = "../Resources/books.csv"
df = pd.read_csv(file_path)
# Import the books.csv file as a DataFrame
310/9:
# Import Dependencies
import pandas as pd
310/10:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
# Import the books.csv file as a DataFrame
310/11:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
df.head()
# Import the books.csv file as a DataFrame
310/12:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
new = df.[["isbn","original_publication_year","original_title","authors","ratings_1","ratings_2","ratings_3","ratings_4","ratings_5"]]
new.head()
310/13:
# Import Dependencies
import pandas as pd
310/14:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
df.head()
# Import the books.csv file as a DataFrame
310/15:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
new = df[["isbn","original_publication_year","original_title","authors","ratings_1","ratings_2","ratings_3","ratings_4","ratings_5"]]
new.head()
310/16:
# Rename the headers to be more explanatory
new = new.rename(columns={"isbn":"ISBN","original_publication_year":"Publication Year","original_title":"Title","authors":"Authors","rating_1":"one star review",
                         "rating_2":"two star review","rating_3":"three star review","rating_4":"four star review","rating_5":"five star review"})
new.head()
310/17:
# Rename the headers to be more explanatory
new = new.rename(columns={"isbn":"ISBN","original_publication_year":"Publication Year","original_title":"Title","authors":"Authors","ratings_1":"one star review",
                         "ratings_2":"two star review","ratings_3":"three star review","ratings_4":"four star review","ratings_5":"five star review"})
new.head()
310/18:
# Push the remade DataFrame to a new CSV file
new to.csv("new/clean_data.csv", header=True)
310/19:
# Push the remade DataFrame to a new CSV file
new.to_csv("new/clean_data.csv", header=True)
311/1:
# Import Dependencies
import pandas as pd
311/2:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
df.head()
# Import the books.csv file as a DataFrame
311/3:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
new = df[["isbn","original_publication_year","original_title","authors","ratings_1","ratings_2","ratings_3","ratings_4","ratings_5"]]
new.head()
311/4:
# Rename the headers to be more explanatory
new = new.rename(columns={"isbn":"ISBN","original_publication_year":"Publication Year","original_title":"Title","authors":"Authors","ratings_1":"one star review",
                         "ratings_2":"two star review","ratings_3":"three star review","ratings_4":"four star review","ratings_5":"five star review"})
new.head()
311/5:
# Push the remade DataFrame to a new CSV file
new.to_csv("new/clean_data.csv", header=True)
312/1:
# Import Dependencies
import pandas as pd
312/2:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
df.head()
# Import the books.csv file as a DataFrame
312/3:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
new = df[["isbn","original_publication_year","original_title","authors","ratings_1","ratings_2","ratings_3","ratings_4","ratings_5"]]
new.head()
312/4:
# Rename the headers to be more explanatory
new = new.rename(columns={"isbn":"ISBN","original_publication_year":"Publication Year","original_title":"Title","authors":"Authors","ratings_1":"one star review",
                         "ratings_2":"two star review","ratings_3":"three star review","ratings_4":"four star review","ratings_5":"five star review"})
new.head()
312/5:
# Push the remade DataFrame to a new CSV file
new.to_csv("new.txt", header=True)
312/6:
# Push the remade DataFrame to a new CSV file
new.to_csv("new.txt", header=True)
313/1:
# Import Dependencies
import pandas as pd
313/2:
# Make a reference to the books.csv file path

file_path = "./Resources/books.csv"
df = pd.read_csv(file_path)
df.head()
# Import the books.csv file as a DataFrame
313/3:
# Remove unecessary columns from the DataFrame and save the new DataFrame
# Only keep: "isbn", "original_publication_year", "original_title", "authors",
# "ratings_1", "ratings_2", "ratings_3", "ratings_4", "ratings_5"
new = df[["isbn","original_publication_year","original_title","authors","ratings_1","ratings_2","ratings_3","ratings_4","ratings_5"]]
new.head()
313/4:
# Rename the headers to be more explanatory
new = new.rename(columns={"isbn":"ISBN","original_publication_year":"Publication Year","original_title":"Title","authors":"Authors","ratings_1":"one star review",
                         "ratings_2":"two star review","ratings_3":"three star review","ratings_4":"four star review","ratings_5":"five star review"})
new.head()
313/5:
# Push the remade DataFrame to a new CSV file
new.to_csv("new_output/untitled.txt", header=True)
316/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
316/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
316/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
316/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
316/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
316/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
316/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
316/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
316/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
316/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
316/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
316/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
316/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
316/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
316/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
316/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
316/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
316/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
316/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
316/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
316/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
316/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
316/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_size_range
316/24: #################################SCORES BY SCHOOL TYPE###################################
316/25:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
316/26:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
316/27:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
#type_avg_math = school_type["math_score"].mean()
#type_reading_math
# type_avg_reading = school_type["reading_score"].mean()
# type_percent_math = school_type
316/28:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
school_type.dtype
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
#type_avg_math = school_type["math_score"].mean()
#type_reading_math
# type_avg_reading = school_type["reading_score"].mean()
# type_percent_math = school_type
316/29:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
school_type.dtype()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
#type_avg_math = school_type["math_score"].mean()
#type_reading_math
# type_avg_reading = school_type["reading_score"].mean()
# type_percent_math = school_type
317/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
317/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
317/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
317/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
317/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
317/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
317/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
317/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
317/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
317/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
317/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
317/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
317/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
317/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
317/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
317/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
317/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
317/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
317/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
317/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
317/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
317/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
317/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_size_range
317/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
school_type.dtype()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
#type_avg_math = school_type["math_score"].mean()
#type_reading_math
# type_avg_reading = school_type["reading_score"].mean()
# type_percent_math = school_type
317/25:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
size_overall_passing = (type_percent_math + type_percent_reading)/2
317/26:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_type
317/27:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
size_overall_passing = (type_percent_math + type_percent_reading)/2
317/28:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_type
318/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
318/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
318/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
318/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
318/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
318/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
318/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
318/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
318/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
318/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
318/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
318/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
318/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
318/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
318/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
318/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
318/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
318/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
318/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
318/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
318/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
318/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
318/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_size_range
318/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
size_overall_passing = (type_percent_math + type_percent_reading)/2
318/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_type
319/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
319/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
319/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
319/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
319/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
319/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
319/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
319/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
319/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
319/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
319/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
319/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
319/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
319/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
319/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
319/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
319/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
319/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
319/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
319/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
319/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
319/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
319/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
319/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
size_overall_passing = (type_percent_math + type_percent_reading)/2
319/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_school_type
320/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
320/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
320/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
320/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
320/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
320/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
320/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
320/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
320/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
320/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
320/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
320/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
320/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
320/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
320/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
320/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
320/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
320/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
320/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
320/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
320/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
320/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
320/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
320/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
size_overall_passing = (type_percent_math + type_percent_reading)/2
320/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
321/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
321/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
321/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
321/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
321/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
321/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
321/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
321/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
321/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
321/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
321/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
321/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
321/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
321/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
321/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
321/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
321/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
321/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
321/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
321/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
321/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
321/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
321/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
321/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
type_overall_passing = (type_percent_math + type_percent_reading)/2
321/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
322/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
322/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
322/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
322/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
average_math = grouped_school_summary["math_score"].mean()
#average_math
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
322/5:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
322/6:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
322/7:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
322/8:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
322/9:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing 
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
322/10:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing 
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
322/11:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
type_overall_passing = (type_percent_math + type_percent_reading)/2
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
324/1:
#import dependencies
import pandas as pd
import numpy as np

school = "schools_complete.csv"
students = "students_complete.csv"

schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
324/2:
#CREATING DF FOR ALL THE SCHOOLS

schools_total = schools_data["school_name"].count()

students_total = students_data["student_name"].count()

budget_total = schools_data["budget"].sum()

avg_math = students_data["math_score"].mean()

avg_reading= students_data["reading_score"].mean()

stu_math_70 = students_data.loc[students_data["math_score"]>=70]

percent_math = (stu_math_70["math_score"].count()/students_total)*100

stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100

overall_percent_score =  (percent_math + percent_reading)/2

all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
all_schools_df
324/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL

school_summary = complete_school_data.set_index("school_name")

school_summary.head()

grouped_school_summary = school_summary.groupby(["school_name"])

grouped_school_summary.head()
324/4:
school_type = schools_data.set_index("school_name")["type"]

total_students = grouped_school_summary["Student ID"].count()

total_school_budget = schools_data.set_index("school_name")["budget"]

per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]

average_math = grouped_school_summary["math_score"].mean()

average_reading = grouped_school_summary["reading_score"].mean()

numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100

overall_passing_rate = (final_percent_math + final_percent_reading)/2

for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})

for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
324/5:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
324/6:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
324/7:
#################################MATH SCORES BY GRADE###################################


math_grade_9th = students_data.loc[students_data["grade"]=="9th"]

math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()


math_grade_10th = students_data.loc[students_data["grade"]=="10th"]

math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()


math_grade_11th = students_data.loc[students_data["grade"]=="11th"]

math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()


math_grade_12th = students_data.loc[students_data["grade"]=="12th"]

math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th

math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
324/8:
##################################READING SCORES BY GRADE###################################


reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]

reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()

reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]

reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()


reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]

reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()

reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]

reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th

reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
324/9:
#################################SCORES BY SCHOOL SPENDING###################################


bins_to_group = [0, 585, 615, 645, 675]

group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]

complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)

student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()

spending_avg_math = student_spending_range["math_score"].mean()

spending_avg_reading = student_spending_range["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100

spending_overall_passing = (spending_percent_math + spending_percent_reading)/2

final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
324/10:
#################################SCORES BY SCHOOL SIZE###################################

bins_to_group_size = [0, 1000, 2000, 5000]

group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]

complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)

school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()

size_avg_math = school_size_range["math_score"].mean()

size_avg_reading = school_size_range["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100

size_overall_passing = (size_percent_math + size_percent_reading)/2

final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
324/11:
#################################SCORES BY SCHOOL TYPE###################################

school_type = complete_school_data.groupby("type")
type_avg_math = school_type["math_score"].mean()

type_avg_reading = school_type["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100

type_overall_passing = (type_percent_math + type_percent_reading)/2
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
325/1:
# Dependencies
import pandas as pd
325/2:
# Load in File from resources
filepath = "./Resources/movie_scores.csv"
# 'movie_scores.csv'
325/3:
# Read and display with pandas
new = pd.read_csv(filepath)
new.head()
325/4:
# List all the columns the table provides
new.columns
325/5:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
new_1 = new[["Film",'IMDB','IMDB_norm','IMDB_norm_round', 'IMDB_user_vote_count']]
new_1.head9
325/6:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
new_1 = new[["Film",'IMDB','IMDB_norm','IMDB_norm_round', 'IMDB_user_vote_count']]
new_1.head()
326/1:
# Dependencies
import pandas as pd
326/2:
# Load in File from resources
filepath = "./Resources/movie_scores.csv"
# 'movie_scores.csv'
326/3:
# Read and display with pandas
new = pd.read_csv(filepath)
new.head()
326/4:
# List all the columns the table provides
new.columns
326/5:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
new_1 = new[["Film",'IMDB','IMDB_norm','IMDB_norm_round', 'IMDB_user_vote_count']]
new_1.head()
326/6:
# only care about Imdb, so create a new table that takes the Film and all the columns relating to IMDB
new_1 = new[["FILM",'IMDB','IMDB_norm','IMDB_norm_round', 'IMDB_user_vote_count']]
new_1.head()
329/1: import pandas as pd
329/2:
data = { "customer_id": [112, 403, 999, 543, 123],
    "name": ["John", "Kelly", "Sam", "April", "Bobbo"],
    "email": ["jman@gmail", "kelly@aol.com", "sports@school.edu", "April@yahoo.com", "HeyImBobbo@msn.com"}
df = pd.DataFrame(data)
df
329/3: import pandas as pd
329/4:
data = {"customer_id": [112, 403, 999, 543, 123],"name": ["John", "Kelly", "Sam", "April", "Bobbo"],"email": ["jman@gmail", "kelly@aol.com", "sports@school.edu", "April@yahoo.com", "HeyImBobbo@msn.com"}
df = pd.DataFrame(data)
df
329/5:
data = {"customer_id": [112, 403, 999, 543, 123],"name": ["John", "Kelly", "Sam", "April", "Bobbo"],"email": ["jman@gmail", "kelly@aol.com", "sports@school.edu", "April@yahoo.com", "HeyImBobbo@msn.com"]}
df = pd.DataFrame(data)
df
329/6:
data_1 = {"customer_id": [403, 112, 543, 999, 654],
    "item": ["soda", "chips", "TV", "Laptop", "Cooler"],
    "cost": [3.00, 4.50, 600, 900, 150]}
df_1 = pd.DataFrame(data_1)
df_1
329/7:
merge = pd.merge(df, df_1,on="customer_id")
merge
329/8:
merge = pd.merge(df, df_1,on="customer_id",how="inner")
merge
329/9:
merge = pd.merge(df, df_1,on="customer_id",how="outer")
merge
329/10:
merge = pd.merge(df, df_1,on="customer_id",how="left")
merge
329/11:
merge = pd.merge(df, df_1,on="customer_id",how="right")
merge
331/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
331/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df
331/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
331/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
331/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
331/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
331/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
331/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
331/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
331/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
331/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
331/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
331/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
331/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
331/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
331/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
331/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
331/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
331/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
331/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
331/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
331/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
331/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
331/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
type_overall_passing = (type_percent_math + type_percent_reading)/2
331/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
334/1:
import pandas as pd
# Mapping lets you format an entire DataFrame
file = "Resources/sample_data.csv"
file_df = pd.read_csv(file)
file_df.head()
334/2:
import pandas as pd
# Mapping lets you format an entire DataFrame
file = "..//Resources//sample_data.csv"
file_df = pd.read_csv(file)
file_df.head()
334/3:
import pandas as pd
# Mapping lets you format an entire DataFrame
file = "../Resources/sample_data.csv"
file_df = pd.read_csv(file)
file_df.head()
334/4:
import pandas as pd
# Mapping lets you format an entire DataFrame
file = "../Resources/sample_data.csv"
file_df = pd.read_csv(file)
file_df.head()
331/26:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
343/1: purchase_data.head()
343/2:
# Dependencies and Setup
import pandas as pd
import numpy as np

# Raw data file
file_to_load = "Resources/purchase_data.csv"

# Read purchasing file and store into pandas data frame
purchase_data = pd.read_csv(file_to_load)
343/3: purchase_data.head()
343/4:
purchase_data.head()
purchase_data["SN"].count()
343/5:
purchase_data.head()
purchase_data["SN"].count()
343/6: purchase_data["Item Name"].unique()
344/1:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
345/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
345/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
345/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
345/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
345/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
345/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
345/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
345/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
345/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
345/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
345/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
345/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
345/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
345/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
345/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
345/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
345/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
345/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
345/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
# spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
345/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
345/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
345/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
345/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
345/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
type_overall_passing = (type_percent_math + type_percent_reading)/2
345/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
345/26:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["Total Budget"].map("${:,.2f}".format)
complete_school_data.head()
345/27:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["Total Budget"].map("${:,.2f}".format)
complete_school_data.head()
345/28:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
345/29:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
grouped_school_summary["budget"] = grouped_school_summary["budget"].map("${:,.2f}".format)
grouped_school_summary.head()
345/30:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
345/31:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
345/32:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
grouped_school_summary["budget"] = grouped_school_summary["budget"].map("${:,.2f}".format)
grouped_school_summary.head()
345/33:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
school_summary.head()
school_summary["budget"] = school_summary["budget"].map("${:,.2f}".format)
school_summary.head()
345/34:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
345/35:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
345/36:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
school_summary.head()
school_summary["budget"] = school_summary["budget"].map("${:,.2f}".format)
school_summary.head()
345/37:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
345/38:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
school_summary.head()
345/39:
school_summary["budget"] = school_summary["budget"].map("${:,.2f}".format)
school_summary.head()
347/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
347/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
347/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
school_summary.head()
347/4:
school_summary["budget"] = school_summary["budget"].map("${:,.2f}".format)
school_summary.head()
347/5:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
347/6:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
school_summary.head()
347/7:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
347/8:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
347/9:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
348/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
348/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
348/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
348/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
348/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
348/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
348/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
348/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
348/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
348/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
348/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
348/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
348/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
348/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
348/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
348/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
348/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
348/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
348/19:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
348/20:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
348/21:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
348/22:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
349/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
349/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
349/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
349/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
349/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
349/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
349/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
349/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
349/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
349/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
349/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
349/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
349/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
349/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
349/15:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format(u"5.0"))
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format(u"5.0"))
for_each_school.head()
350/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
350/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
350/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
350/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
350/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
350/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
350/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
350/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
350/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
350/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
350/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
350/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
350/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
350/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format(u"5.0"))
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format(u"5.0"))
for_each_school.head()
350/15:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.dtype
350/16:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
#for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
#for_each_school.head()
#for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.dtype
350/17:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
#for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
#for_each_school.head()
#for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
type(for_each_school)
350/18:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
#for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
#for_each_school.head()
#for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school
350/19:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
350/20:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
350/21:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
350/22:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
350/23:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
350/24:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
351/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
351/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
351/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
351/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
351/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
351/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
351/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
351/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
351/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
351/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
351/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
351/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
351/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
351/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school.head()
351/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
351/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
351/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
351/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
351/19:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
352/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
352/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
352/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
352/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
352/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
352/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
352/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
352/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
352/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
352/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
352/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
352/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
352/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
352/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school.head()
352/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
352/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
352/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
352/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
352/19:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
352/20:
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
352/21: average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
352/22: average_spending_range = complete_school_data["budget"]/int(complete_school_data["size"])|
352/23: average_spending_range = complete_school_data["budget"]/int(complete_school_data["size"])
352/24: average_spending_range = complete_school_data["budget"].astype(int)/complete_school_data["size"]
352/25: average_spending_range = complete_school_data["budget"].astype(float64)/complete_school_data["size"]
352/26: average_spending_range = complete_school_data["budget"].astype('float64')/complete_school_data["size"]
352/27: complete_school_data['budget']
352/28: complete_school_data['budget'].format('$')
352/29: complete_school_data['budget']
352/30: complete_school_data['budget'].strip('$')
352/31: complete_school_data['budget'].strip('$',2)
352/32: complete_school_data['budget']
352/33:
complete_school_data['budget_converted'] = complete_school_data['budget'].replace("$"," ")
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].astype('float64')
352/34:
complete_school_data['budget_converted'] = complete_school_data['budget'].replace("$"," ")
complete_school_data['budget_converted']
352/35:
complete_school_data['budget_converted'] = complete_school_data['budget'].replace("$"," ")
complete_school_data['budget_converted']
352/36:
complete_school_data['budget_converted'] = complete_school_data['budget'].replace("\$"," ")
complete_school_data['budget_converted']
352/37:
complete_school_data['budget_converted'] = complete_school_data['budget'].replace('$',' ')
complete_school_data['budget_converted']
352/38:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted']
352/39:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'].astype('float64')
352/40:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'].astype('int64')
352/41:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'].astype('float')
352/42:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace(',','')
complete_school_data['budget_converted'].astype('float')
352/43:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace(',','')
complete_school_data['budget_converted']
352/44:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budgetd_converte'].str.replace(',','')
complete_school_data['budget_converted']
352/45:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budgetd_converted'].str.replace(',','')
complete_school_data['budget_converted']
352/46:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].str.replace(',','')
complete_school_data['budget_converted']
352/47:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].str.replace(',','')
complete_school_data['budget_converted'].astype('float')
352/48: average_spending_range = complete_school_data["budget"].astype('float64')/complete_school_data["size"]
352/49: average_spending_range = complete_school_data["budget_converted"]/complete_school_data["size"]
352/50: complete_school_data["size"].dtype
352/51:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].str.replace(',','')
complete_school_data['budget_converted'].astype('int64')
352/52:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].str.replace(',','')
complete_school_data['budget_converted'].astype('int')
352/53:
complete_school_data['budget_converted'] = complete_school_data['budget'].str.replace('\$','')
complete_school_data['budget_converted'] = complete_school_data['budget_converted'].str.replace(',','')
complete_school_data['budget_converted'].astype('float')
352/54: average_spending_range = complete_school_data["budget_converted"]/complete_school_data["size"].astype('float64')
352/55: average_spending_range = complete_school_data["budget_converted"]/(complete_school_data["size"].astype('float64'))
352/56: complete_school_data['budget_converted']
352/57: complete_school_data['budget_converted']/complete_school_data["size"]
352/58:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
352/59:
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
352/60:
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df.head()
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
352/61:
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
352/62:
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
352/63:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
352/64:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df.head()
352/65:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
352/66:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
352/67:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
352/68:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
352/69:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
352/70:
average_math = grouped_school_summary["math_score"].mean()
#average_math
352/71:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
352/72:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
352/73:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
352/74:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
352/75:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
352/76:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
352/77:
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
352/78:
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
352/79:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school.head()
352/80:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
352/81:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
352/82:
#################################SCORES BY SCHOOL SPENDING###################################
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]

complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)

student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()

spending_avg_math = student_spending_range["math_score"].mean()

spending_avg_reading = student_spending_range["reading_score"].mean()


# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
352/83:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
352/84:
students_spending_range["budget"] = for_each_school["budget"].map("${:,.2f}".format)
students_spending_range.head()
352/85:
students_spending_range["budget"] = students_spending_range["budget"].map("${:,.2f}".format)
students_spending_range.head()
352/86:
student_spending_range["budget"] = student_spending_range["budget"].map("${:,.2f}".format)
student_spending_range.head()
354/1:
#import dependencies
import pandas as pd
import numpy as np

#set path to csv/data files 
school = "schools_complete.csv"
students = "students_complete.csv"

#read the files
schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################

#merge both data tables:new_df=pd.merge(leftdf,rightdf,how="left/inner/right/outer",on=column_name/[column_name_leftdf,column_name_rightdf])
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
354/2:
#CREATING DF FOR ALL THE SCHOOLS
# The count method counts every entry in that particular series: var=df["column_name"].count()
schools_total = schools_data["school_name"].count()
#print(schools_total)

# counts the total no. of students in all the schools
students_total = students_data["student_name"].count()
#print(students_total)

# The sum method adds every entry in that particular series: var=df["column_name"].sum() for all the schools 
budget_total = schools_data["budget"].sum()
#print(budget_total)

# The mean method calculates the average of all the entries in that particular series: var=df["column_name"].mean() for all the schools 
avg_math = students_data["math_score"].mean()
#print(avg_math)

#similarly for the reading to for all the schools
avg_reading= students_data["reading_score"].mean()
#print(avg_reading)

#percentage of students with >=70 in math in all the schools = (no.of students having score greater than 70 in all the schools/students_total)*100
#no.of students having score greater than 70 - lists all the students with score >=70 by indexing math_score
stu_math_70 = students_data.loc[students_data["math_score"]>=70]
#Final percent calculation = counts the no. of students with the score>=70 and divides by the total no. of students *100
percent_math = (stu_math_70["math_score"].count()/students_total)*100
#print(percent_math)

#similarly for reading
stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100
#print(percent_reading)

#overall score for all the schools
overall_percent_score =  (percent_math + percent_reading)/2
#print(overall_percent_score)

#creating a new DF for all schools in the district by assigning the values calculated above to the keys in a dictionary***********************************************(Why assign lists when there's just 1 value?)
all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
#Since its a total count for all schools, there is just 1 row to display
#use Map to format the the entire df
all_schools_df.head()
354/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL
#school_summary df has to be created by indexing school_name, based on which, all the other columns(values) are defined 
#just gives an overview of how the school_name is indexed and how it looks after indexing
school_summary = complete_school_data.set_index("school_name")
#check code
school_summary.head()
## The object returned is a "GroupBy" object and cannot be viewed normally
grouped_school_summary = school_summary.groupby(["school_name"])
#In order to be visualized, a data function must be used
grouped_school_summary.head()
354/4:
# now we need to index the data by the school_name and group the values of school_type, total_students,
# total_budget, per_student_budget, average math score, average reading score, %passing math, 
# %passing reading and overall passing rate OF EACH SCHOOL BASED ON THE SCHOOL_NAME INDEX!!!!
#aim at this point is to create a DF to hold all the above results
#Firstly tackling the school-type by assigning the schools with the type using the plain schools_data df
school_type = schools_data.set_index("school_name")["type"]
#school_type
354/5:
#next counting the total students in each school using the groupby df
total_students = grouped_school_summary["Student ID"].count()
#total_students
354/6:
#total_school_budget of each school using the plain old schools_data df
total_school_budget = schools_data.set_index("school_name")["budget"]
#total_school_budget
354/7:
#budget for each student in each school = total budget for each school/no.of students in that particular school using schools_data df
per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]
#per_student_budget
354/8:
average_math = grouped_school_summary["math_score"].mean()
#average_math
354/9:
average_reading = grouped_school_summary["reading_score"].mean()
#average_reading
354/10:
#percentage of students passing math = (no.of students with mathscores>=70 in each school/total no. of students in each school)*100 
numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
#final_percent_math
354/11:
#similarly, percentage of students passing reading = (no.of students with readingscores>=70 in each school/total no. of students in each school)*100 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100
#final_percent_reading
354/12:
#overall passing rate = Average of the final_percent_math and final_percent_reading
overall_passing_rate = (final_percent_math + final_percent_reading)/2
#overall_passing_rate
354/13:
#creating another DF for each school in the district- assigning the values as lists(rows) to the keys(colum_names)
for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})
#To display the columns in the new DF, use [["","",""]]
for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
354/14:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school.head()
354/15:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
354/16:
#################################MATH SCORES BY GRADE###################################

#Assign individual grade levels
math_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the math score
math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()
#math_grade_9th
#Assign individual grade levels
math_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the math score
math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()
#math_grade_10th
#Assign individual grade levels
math_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the math score
math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()
#math_grade_11th
#Assign individual grade levels
math_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the math score
math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
354/17:
##################################READING SCORES BY GRADE###################################

#Assign individual grade levels
reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()
#reading_grade_9th
#Assign individual grade levels
reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()
#reading_grade_10th
#Assign individual grade levels
reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()
#reading_grade_11th
#Assign individual grade levels
reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]
#groupby the school name and calculate the mean for the reading score
reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th
#Create a dictionary to set the keys as grades 9, 10, 11 and 12 with corresponding mean values calculated above
reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
354/18:
#################################SCORES BY SCHOOL SPENDING###################################

# Create a table that breaks down school performances based on average Spending Ranges (Per Student). Use 4 reasonable bins to group school spending. Include in the table each of the following:
# Average Math Score
# Average Reading Score
# % Passing Math
# % Passing Reading
# Overall Passing Rate (Average of the above two)
#create 4 bins with 5 values
bins_to_group = [0, 585, 615, 645, 675]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]
#Cut the initial merged complete_school_data DF for avg_spending_range based on the bins and group_names created
complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)
#create a group based off of the binsbby assigning the group to a new DF variable
student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()
354/19:
#The above table shows the indexing of spending_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created spending_range DF
spending_avg_math = student_spending_range["math_score"].mean()
#spending_avg_math
spending_avg_reading = student_spending_range["reading_score"].mean()
# spending_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100
#spending_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100
#spending_percent_reading
#overall passing rate = Average of the spending_percent_math and spending_percent_reading
spending_overall_passing = (spending_percent_math + spending_percent_reading)/2
#spending_overall_passing
354/20:
final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
354/21:
#################################SCORES BY SCHOOL SIZE###################################

#Repeat the above breakdown, but this time group schools based on a reasonable approximation of school size (Small, Medium, Large)
#create 4 bins with 5 values
bins_to_group_size = [0, 1000, 2000, 5000]
#create a list of group name(1 count less than the bin) for each bin range
group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]
#Cut the initial merged complete_school_data DF for size_range based on the bins and group_names created
complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)
#create a group based off of the binsbby assigning the group to a new DF variable
school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()
354/22:
#The above table shows the indexing of size_range but with all the columns. We need specific columns of avg_math, avg_read and their percents and overall
#so now we need to calculate avg_math, avg_read and their percents and overall based on the above newly created size_range DF
size_avg_math = school_size_range["math_score"].mean()
# size_avg_math
size_avg_reading = school_size_range["reading_score"].mean()
# size_avg_reading
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100
#size_percent_math
#percentage of students passing reading = (no.of students with readingscores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100
#size_percent_reading
#overall passing rate = Average of the size_percent_math and size_percent_reading
size_overall_passing = (size_percent_math + size_percent_reading)/2
#size_overall_passing
354/23:
final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
354/24:
#################################SCORES BY SCHOOL TYPE###################################
#Perform the same operations as above, based on school type

school_type = complete_school_data.groupby("type")
#school_type.head()
#Now, basic calculations of avg math, avg reading and their percents and overall based on school type is to be done
#type_avg_math
type_avg_math = school_type["math_score"].mean()
#type_reading_math
type_avg_reading = school_type["reading_score"].mean()
#Type_percent_math
#percentage of students passing math = (no.of students with mathscores>=70/total no. of students)*100 
numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100
#percentage of students passing in reading = (no.of students with readingcores>=70/total no. of students)*100 
numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100
#overall passing rate = Average of the type_percent_math and type_percent_reading
type_overall_passing = (type_percent_math + type_percent_reading)/2
354/25:
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
354/26:
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
354/27:
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
354/28:
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
354/29:
student_spending_range["budget"] = student_spending_range["budget"].map("${:,.2f}".format)
student_spending_range.head()
354/30:
final_school_size_range["budget"] = final_school_size_range["budget"].map("${:,.2f}".format)
final_school_size_range.head()
353/1:
#Formatting the tables for $ and commas
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
353/2:
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
356/1:
#import dependencies
import pandas as pd
import numpy as np

school = "schools_complete.csv"
students = "students_complete.csv"

schools_data = pd.read_csv(school)
students_data = pd.read_csv(students)

#################################DISTRICT SUMMARY###################################
complete_school_data = pd.merge(schools_data,students_data, how='left',on=['school_name','school_name'])
complete_school_data.head()
356/2:
#CREATING DF FOR ALL THE SCHOOLS

schools_total = schools_data["school_name"].count()

students_total = students_data["student_name"].count()

budget_total = schools_data["budget"].sum()

avg_math = students_data["math_score"].mean()

avg_reading= students_data["reading_score"].mean()

stu_math_70 = students_data.loc[students_data["math_score"]>=70]

percent_math = (stu_math_70["math_score"].count()/students_total)*100

stu_reading_70 = students_data.loc[students_data["reading_score"]>=70]
percent_reading = (stu_reading_70["reading_score"].count()/students_total)*100

overall_percent_score =  (percent_math + percent_reading)/2

all_schools_df = pd.DataFrame({"Total Schools":[schools_total], "Total Students": [students_total],
                               "Total Budget": [budget_total], "Average Math Score": [avg_math], "Average Reading Score": [avg_reading],
                              "%Passing Math": [percent_math], "%Passing Reading": [percent_reading], "%Overall Passing Rate": [overall_percent_score]})
all_schools_df
356/3:
#################################SCHOOL SUMMARY###################################

#CREATING DF FOR EACH INDIVIDUAL SCHOOL

school_summary = complete_school_data.set_index("school_name")

school_summary.head()

grouped_school_summary = school_summary.groupby(["school_name"])

grouped_school_summary.head()
356/4:
school_type = schools_data.set_index("school_name")["type"]

total_students = grouped_school_summary["Student ID"].count()

total_school_budget = schools_data.set_index("school_name")["budget"]

per_student_budget = total_school_budget/schools_data.set_index("school_name")["size"]

average_math = grouped_school_summary["math_score"].mean()

average_reading = grouped_school_summary["reading_score"].mean()

numerator_value_math = students_data.loc[students_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("school_name")["Student ID"].count()
final_percent_math = (numerator_math/total_students)*100
 
numerator_value_reading = students_data.loc[students_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("school_name")["Student ID"].count()
final_percent_reading = (numerator_reading/total_students)*100

overall_passing_rate = (final_percent_math + final_percent_reading)/2

for_each_school = pd.DataFrame({"School Type": school_type,"Total Students": total_students, "Total_School_Budget":total_school_budget,
                           "Per Student Budget": per_student_budget,"Average Math Score":average_math,"Average Reading Score":average_reading,"%Passing Math":final_percent_math,
                           "%Passing Reading":final_percent_reading,"Overall Passing Rate":overall_passing_rate})

for_each_school = for_each_school[["School Type", "Total Students", "Total_School_Budget", "Per Student Budget",
                           "Average Math Score","Average Reading Score","%Passing Math","%Passing Reading","Overall Passing Rate" ]]
for_each_school
356/5:
#################################TOP PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nlargest(5, ["Overall Passing Rate"])
for_each_school
356/6:
#################################BOTTOM PERFORMING SCHOOLS(BY PASSING RATE)###################################

for_each_school = for_each_school.nsmallest(5, ["Overall Passing Rate"], keep="last")
for_each_school
356/7:
#################################MATH SCORES BY GRADE###################################


math_grade_9th = students_data.loc[students_data["grade"]=="9th"]

math_grade_9th = math_grade_9th.groupby("school_name")["math_score"].mean()


math_grade_10th = students_data.loc[students_data["grade"]=="10th"]

math_grade_10th = math_grade_10th.groupby("school_name")["math_score"].mean()


math_grade_11th = students_data.loc[students_data["grade"]=="11th"]

math_grade_11th = math_grade_11th.groupby("school_name")["math_score"].mean()


math_grade_12th = students_data.loc[students_data["grade"]=="12th"]

math_grade_12th = math_grade_12th.groupby("school_name")["math_score"].mean()
math_grade_12th

math_grades_combined = {"9th":math_grade_9th, "10th":math_grade_10th, "11th":math_grade_11th,"12th":math_grade_12th}
final_math_grade = pd.DataFrame(math_grades_combined)
final_math_grade = final_math_grade[["9th","10th","11th","12th"]]
final_math_grade
356/8:
##################################READING SCORES BY GRADE###################################


reading_grade_9th = students_data.loc[students_data["grade"]=="9th"]

reading_grade_9th = reading_grade_9th.groupby("school_name")["reading_score"].mean()

reading_grade_10th = students_data.loc[students_data["grade"]=="10th"]

reading_grade_10th = reading_grade_10th.groupby("school_name")["reading_score"].mean()


reading_grade_11th = students_data.loc[students_data["grade"]=="11th"]

reading_grade_11th = reading_grade_11th.groupby("school_name")["reading_score"].mean()

reading_grade_12th = students_data.loc[students_data["grade"]=="12th"]

reading_grade_12th = reading_grade_12th.groupby("school_name")["reading_score"].mean()
reading_grade_12th

reading_grades_combined = {"9th":reading_grade_9th, "10th":reading_grade_10th,"11th":reading_grade_11th,"12th":reading_grade_12th}
final_reading_grade = pd.DataFrame(reading_grades_combined)
final_reading_grade = final_reading_grade[["9th","10th","11th","12th"]]
final_reading_grade
356/9:
#################################SCORES BY SCHOOL SPENDING###################################


bins_to_group = [0, 585, 615, 645, 675]

group_name_for_bins = ["<$585","$585-615", "$615-645", "$645-675"]
average_spending_range = complete_school_data["budget"]/complete_school_data["size"]

complete_school_data["spending_range"] = pd.cut(average_spending_range, bins_to_group, labels = group_name_for_bins)

student_spending_range = complete_school_data.groupby("spending_range")
student_spending_range.max()

spending_avg_math = student_spending_range["math_score"].mean()

spending_avg_reading = student_spending_range["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("spending_range")["Student ID"].count()
spending_percent_math = (numerator_math/(student_spending_range["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("spending_range")["Student ID"].count()
spending_percent_reading = (numerator_reading/(student_spending_range["Student ID"].count())) * 100

spending_overall_passing = (spending_percent_math + spending_percent_reading)/2

final_student_spending_range = pd.DataFrame({"Average Math Score": spending_avg_math, "Average Reading Score": spending_avg_reading,
                                            "%Passing Math": spending_percent_math, "%Passing Reading": spending_percent_reading, "Overall Passing Rate": spending_overall_passing})
final_student_spending_range
356/10:
#################################SCORES BY SCHOOL SIZE###################################

bins_to_group_size = [0, 1000, 2000, 5000]

group_name_for_sizebins = ["Small (<1000)", "Medium (1000-2000)", "Large (2000-5000)"]

complete_school_data["size_range"] = pd.cut(complete_school_data["size"], bins_to_group_size, labels = group_name_for_sizebins)

school_size_range = complete_school_data.groupby("size_range")
school_size_range.max()

size_avg_math = school_size_range["math_score"].mean()

size_avg_reading = school_size_range["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("size_range")["Student ID"].count()
size_percent_math = (numerator_math/(school_size_range["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("size_range")["Student ID"].count()
size_percent_reading = (numerator_reading/(school_size_range["Student ID"].count())) * 100

size_overall_passing = (size_percent_math + size_percent_reading)/2

final_school_size_range = pd.DataFrame({"Average Math Score": size_avg_math, "Average Reading Score": size_avg_reading,
                                            "%Passing Math": size_percent_math, "%Passing Reading": size_percent_reading, "Overall Passing Rate": size_overall_passing})
final_school_size_range
356/11:
#################################SCORES BY SCHOOL TYPE###################################

school_type = complete_school_data.groupby("type")
type_avg_math = school_type["math_score"].mean()

type_avg_reading = school_type["reading_score"].mean()

numerator_value_math = complete_school_data.loc[complete_school_data["math_score"]>=70]
numerator_math = numerator_value_math.groupby("type")["Student ID"].count()
type_percent_math = (numerator_math/(school_type["Student ID"].count())) * 100

numerator_value_reading = complete_school_data.loc[complete_school_data["reading_score"]>=70]
numerator_reading = numerator_value_reading.groupby("type")["Student ID"].count()
type_percent_reading = (numerator_reading/(school_type["Student ID"].count())) * 100

type_overall_passing = (type_percent_math + type_percent_reading)/2
final_school_type = pd.DataFrame({"Average Math Score": type_avg_math, "Average Reading Score": type_avg_reading,
                                            "%Passing Math": type_percent_math, "%Passing Reading": type_percent_reading, "Overall Passing Rate": type_overall_passing})
final_school_type
356/12:
#Formatting the tables for $ and commas
complete_school_data["budget"] = complete_school_data["budget"].map("${:,.2f}".format)
complete_school_data.head()
356/13:
all_schools_df["Total Budget"] = all_schools_df["Total Budget"].map("${:,.2f}".format)
all_schools_df["Total Students"] = all_schools_df["Total Students"].map("{:,}".format)
all_schools_df.head()
356/14:
for_each_school["Total_School_Budget"] = for_each_school["Total_School_Budget"].map("${:,.2f}".format)
for_each_school.head()
for_each_school["Per Student Budget"] = for_each_school["Per Student Budget"].map("${:,.2f}".format)
for_each_school.head()
359/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
#df = df.rename(df)
#df["Sold in Bulk?"]
print(df)
359/2:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
#df = df.rename(df)
#df["Sold in Bulk?"]
df
359/3:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
#df = df.rename(df)
#df["Sold in Bulk?"]
df
help(df.rename)
359/4:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
#help(df.rename)
359/5:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(index=str, columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
#help(df.rename)
359/6:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(index=str, columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
help(df.rename)
359/7:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = 'Yes'
df
#help(df.rename)
359/8:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
359/9:
import string
df["Product Description"]=df["Product Description"].apply(upper)
df
359/10:
import string
df["Product Description"]=df.Product Description.apply(upper)
df
359/11:
import string
df["Product Description"]=df.("Product Description").apply(upper)
df
359/12:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
359/13:
import string
df["Product Description"]=df.("Product Description").apply(upper)
df
359/14:
from string import upper
df["Product Description"]=df.("Product Description").apply(upper)
df
359/15:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
359/16:
from string import upper
df["Product Description"]=df.("Product Description").apply(upper)
df
359/17:
from string import upper
df["Product Description"]=df.("Product Description").apply(upper)
df
359/18:
from string import upper
df["Product Description"]=df.["Product Description"].apply(upper)
df
359/19:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
359/20:
from string import upper
df["Product Description"]=df.["Product Description"].apply(upper)
df
359/21:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
359/22:
from string import upper
df["Product Description"]=df.["Product Description"].apply(upper)
df
359/23:
from string import upper
df["Product Description"]=df."Product Description".apply(upper)
df
359/24:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
363/1:
import matplotlib.pyplot as plt
days =[0,1,2,3,4,5,6]
money_spent = [10,12,12,10,14,22,24]
plt.plot(days,money_spent)
plt.show()
363/2:
import pandas as pd
import matplotlib.pyplot as plt
days =[0,1,2,3,4,5,6]
money_spent = [10,12,12,10,14,22,24]
plt.plot(days,money_spent)
plt.show()
363/3:
x = raange(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1)
plt.plot(x,y2)
plt.show
363/4:
x = range(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1)
plt.plot(x,y2)
plt.show
363/5:
x = range(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1, color="pink",markers = 'o')
plt.plot(x,y2,color="gray",markers = 'o')
plt.show
363/6:
import pandas as pd
import matplotlib.pyplot as plt
days =[0,1,2,3,4,5,6]
money_spent = [10,12,12,10,14,22,24]
plt.plot(days,money_spent)
plt.show()
363/7:
x = range(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1, color="pink",markers = "o")
plt.plot(x,y2,color="gray",markers = "o")
plt.show
363/8:
x = range(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1, color="pink",marker = "o")
plt.plot(x,y2,color="gray",marker = "o")
plt.show
363/9:
days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
plt.bar(range(len(days_in_year)),days_in_year)
plt.show()
363/10:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(sales,drinks)
plt.show()
363/11:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(drinks,sales)
plt.show()
363/12:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
plt.show()
363/13:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks[0,1,2,3,4,5]
ax.xlabels["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]

plt.show()
363/14:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.xlabels["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]

plt.show()
363/15:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.xlabels(drinks)

plt.show()
363/16:
import pandas as pd
import matplotlib.pyplot as plt
days =[0,1,2,3,4,5,6]
money_spent = [10,12,12,10,14,22,24]
plt.plot(days,money_spent)
plt.show()
363/17:
x = range(8)
y1 = [12,24,36,48,60,72,84,96]
y2 = ["a","b","c","d","e","f","g","h"]

plt.plot(x,y1, color="pink",marker = "o")
plt.plot(x,y2,color="gray",marker = "o")
plt.show
363/18:
days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
plt.bar(range(len(days_in_year)),days_in_year)
plt.show()
363/19:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.xlabels(drinks)

plt.show()
363/20:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.set_xtickxlabels(drinks)

plt.show()
363/21:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.set_xticksxlabels(drinks)

plt.show()
363/22:
import pandas as pd
import matplotlib.pyplot as plt
days =[0,1,2,3,4,5,6]
money_spent = [10,12,12,10,14,22,24]
plt.plot(days,money_spent)
plt.show()
363/23:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.set_xtickxlabels(drinks)

plt.show()
363/24:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
ax = plt.subplot()
ax.set_xticks(range(6))
ax.set_xticklabels(drinks)

plt.show()
363/25:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(range(len(drinks)),sales)
plt.show()
363/26:
drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]
plt.bar(drinks,sales)
plt.show()
367/1:
import numpy as np
import matplotlib.plyplot as plt
x_axis=(range(12))
average temperature per month in Fahrenheit: `[39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, average temperature per month in Fahrenheit)
plt.show()
367/2:
import numpy as np
import matplotlib.plyplot as plt
x_axis=(range(12))
average temperature per month in Fahrenheit: [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, average temperature per month in Fahrenheit)
plt.show()
367/3:
import numpy as np
import matplotlib.plyplot as plt
x_axis=(range(12))
average temperature per month in Fahrenheit: [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, average temperature per month in Fahrenheit)
plt.show()
367/4:
import numpy as np
import matplotlib.plyplot as plt
x_axis=(range(12))
average temperature per month in Fahrenheit= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, average temperature per month in Fahrenheit)
plt.show()
367/5:
import numpy as np
import matplotlib.plyplot as plt
x_axis=(range(12))
avg_temp= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, avg_temp)
plt.show()
367/6:
import numpy as np
import matplotlib.pyplot as plt
x_axis=(range(12))
avg_temp= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, avg_temp)
plt.show()
367/7:
import numpy as np
import matplotlib.pyplot as plt
x_axis=np.arange(1,12,1)
avg_temp= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, avg_temp)
plt.show()
367/8:
import numpy as np
import matplotlib.pyplot as plt
x_axis=np.arange(1,12,1)
x_axis
avg_temp= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, avg_temp)
plt.show()
367/9:
import numpy as np
import matplotlib.pyplot as plt
x_axis=np.arange(1,13,1)
x_axis
avg_temp= [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]
plt.plot(x_axis, avg_temp)
plt.show()
367/10:
#C = (F - 32) \* 0.56
cel = ([avg_temp]-32)\*0.56
cel
plt.plot(x_axis,cel)
plt.show()
367/11:
#C = (F - 32) \* 0.56
cel = (([avg_temp]-32)\*0.56)
cel
plt.plot(x_axis,cel)
plt.show()
367/12:
#C = (F - 32) * 0.56
cel = (([avg_temp]-32)*0.56)
cel
plt.plot(x_axis,cel)
plt.show()
367/13:
#C = (F - 32) * 0.56
cel = ((x-32)*0.56 for x in avg_temp)
cel
plt.plot(x_axis,cel)
plt.show()
367/14:
#C = (F - 32) * 0.56
cel = ((x-32)*0.56 for x in avg_temp)
cel
# plt.plot(x_axis,cel)
# plt.show()
367/15:
#C = (F - 32) * 0.56
cel = (x-32)*0.56 for x in avg_temp
cel
# plt.plot(x_axis,cel)
# plt.show()
367/16:
#C = (F - 32) * 0.56
cel = [(x-32)*0.56 for x in avg_temp]
cel
# plt.plot(x_axis,cel)
# plt.show()
367/17:
#C = (F - 32) * 0.56
cel = [(x-32)*0.56 for x in avg_temp]
cel
plt.plot(x_axis,cel)
plt.show()
367/18:
plt.plot(x_axis, avg_temp)
plt.plot(x_axis,cel)
plt.show()
368/1: help(plt.hlines)
368/2: help(plt.hlines)
368/3:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
368/4: %matplotlib notebook
368/5:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
368/6:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
368/7:
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
368/8:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
368/9:
#function *[1,1] - *[,] unpacks the values in the list in python, which is also used in matplot lib 
#values in the legends is identified by the handles
368/10:
# Adds a legend and sets its location to the lower right
plt.legend(loc="lower right")
368/11:
# Saves an image of our chart so that we can view it in a folder
plt.savefig("../Images/lineConfig.png")
plt.show()
368/12: #help(plt.plot)
368/13:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
368/14:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
368/15:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
368/16:
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
368/17:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
368/18:
#function *[1,1] - *[,] unpacks the values in the list in python, which is also used in matplot lib 
#values in the legends is identified by the handles
368/19:
# Adds a legend and sets its location to the lower right
plt.legend(loc="lower right")
368/20:
# Saves an image of our chart so that we can view it in a folder
plt.savefig("../Images/lineConfig.png")
plt.show()
368/21:
# Saves an image of our chart so that we can view it in a folder
#plt.savefig("../Images/lineConfig.png")
plt.show()
369/1:
import matplotlib.pyplot as plt
import numpy as np
369/2:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
plt.hlines(0,0,10, alpha = 0.25)
369/3:
# Set x axis and variables
x_axis = np.arange(0, 10, 2)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
plt.hlines(0,0,10, alpha = 0.25)
369/4:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
# sin = np.sin(x_axis)
# cos = np.cos(x_axis)
plt.hlines(0,0,10, alpha = 0.25)
369/5:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
# sin = np.sin(x_axis)
# cos = np.cos(x_axis)
#plt.hlines(0,0,10, alpha = 0.25)
369/6:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
# sin = np.sin(x_axis)
# cos = np.cos(x_axis)
#plt.hlines(0,0,10, alpha = 0.25)
369/7:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
# sin = np.sin(x_axis)
# cos = np.cos(x_axis)
plt.hlines(0,0,10, alpha = 0.25)
369/8:
sine_handle, = plt.plot(x_axis, sin)
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos)
369/9: plt.legend(loc="lower right")
369/10:
sine_handle, = plt.plot(x_axis, sin)
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos)
plt.legend(loc="lower right")
371/1:
import matplotlib.pyplot as plt
import numpy as np
x_axis = np.arange(0,10,0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
plt.hlines(0,0,10, alpha = 0.25)
371/2:
plt.plt(x_axis,sin)
plt.plot(x_axis,cos)
plt.show()
371/3:
plt.plot(x_axis,sin)
plt.plot(x_axis,cos)
plt.show()
371/4:
plt.plt(x_axis,sin)
plt.plot(x_axis,cos)
plt.show()
371/5:
plt.plot(x_axis,sin)
plt.plot(x_axis,cos)
plt.show()
371/6:
plt.plot(x_axis,sin, linestyle ="--",marker ="o")
plt.plot(x_axis,cos,linestyle ="--",marker ="^")
plt.show()
371/7:
plt.plot(x_axis,sin, linewidth ="2", linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth ="2",color="y",marker ="^")
plt.show()
371/8:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.show()
371/9:
plt.title("First plot")
plt.xtitle("Numbers")
plt.ytitle("Decimals")
371/10:
plt.title("First plot")
plt.xlabel("Numbers")
plt.ytitle("Decimals")
371/11:
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
371/12:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.show()
371/13:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")

plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")plt.show()
371/14:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
plt.show()
371/15:
plt.xlim(0, 10)
plt.ylim(-1, 1)
371/16:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
plt.xlim(0, 10)
plt.ylim(-1, 1)
plt.show()
371/17:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
# plt.xlim(0, 10)
# plt.ylim(-1, 1)
plt.show()
371/18: help(plt.xlim)
371/19:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
plt.xlim(0, 10)
plt.ylim(-1, 1)
plt.show()
371/20:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
plt.xlim(0, 10)
plt.ylim(-1, 1)
plt.grid()
plt.show()
371/21: help(plt.grid)
371/22:
plt.plot(x_axis,sin, linewidth =2, linestyle ="--",marker ="o",color="r")
plt.plot(x_axis,cos,linestyle ="--",linewidth =2,color="y",marker ="^")
plt.title("First plot")
plt.xlabel("Numbers")
plt.ylabel("Decimals")
plt.xlim(0, 10)
plt.ylim(-1, 1)
plt.grid(alpha = 0.5)
plt.show()
370/1: )helpplt.savefig)
370/2: help(plt.savefig)
371/23: help(plt.savefig)
371/24: help(plt.show)
373/1:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.plot(x_axis,users)
plt.show()
373/2:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users)
plt.show()
373/3: help(plt.bar)
373/4:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, bottom = 2)
plt.show()
373/5:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, bottom = 20)
plt.show()
373/6:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users)
plt.show()
373/7:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, aligh = "edge")
plt.show()
373/8:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge")
plt.show()
373/9:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",orientation ="horizontal")
plt.show()
373/10:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.barh(x_axis,users, align = "edge",orientation ="horizontal")
plt.show()
373/11:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.barh(x_axis,users, align = "edge")
plt.show()
373/12:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge")
plt.show()
373/13:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.barh(x_axis,users, align = "edge")
plt.show()
373/14:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.barh(x_axis,users, align = "edge",labels=["Java", "C++", "Python", "Ruby", "Clojure"])
plt.show()
373/15:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",labels=["Java", "C++", "Python", "Ruby", "Clojure"])
plt.show()
373/16:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label=["Java", "C++", "Python", "Ruby", "Clojure"])
plt.show()
373/17:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.barh(x_axis,users, align = "edge",tick_label = names )
plt.show()
373/18:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.show()
373/19: #help(plt.bar)
373/20:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "center",tick_label = names )
plt.xlim()
plt.show()
373/21:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "center",tick_label = names )
plt.xlim(0,len(names))
plt.show()
373/22:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.show()
373/23:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(10,len(names))
plt.show()
373/24:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(3,len(names))
plt.show()
373/25:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0.25,len(names))
plt.show()
373/26:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0.1,len(names))
plt.show()
373/27:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.show()
373/28:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names),2)
plt.show()
373/29:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names),4)
plt.show()
373/30:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.show()
373/31:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,len(users))
plt.show()
373/32:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users))
plt.show()
373/33:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)5000)
plt.show()
373/34:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.show()
373/35:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+500)
plt.show()
373/36:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.show()
373/37:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()
373/38:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.title("Popularity of Programming Languages",0.5)
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()
373/39:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()
373/40: #help(plt.title)
373/41: help(plt.title)
373/42:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.title("Popularity of Programming Languages",loc="right")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()
373/43:
import matplotlib.pyplot as plt
import numpy as np
users = [13000, 26000, 52000, 30000, 9000]
names = ["Java", "C++", "Python", "Ruby", "Clojure"]
x_axis = np.arange(len(users))
plt.bar(x_axis,users, align = "edge",tick_label = names )
plt.xlim(0,len(names))
plt.ylim(0,max(users)+5000)
plt.title(0,0.5,"Popularity of Programming Languages",loc="right")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()
373/44: help(plt.set_title)
373/45: help(set_title)
380/1: %matplotlib notebook
380/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
380/3:
# Set x axis and variables
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
380/4:
# Draw a horizontal line with 0.25 transparency - hlines
plt.hlines(0, 0, 10, alpha=0.25)
380/5:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
380/6:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine_handle, = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
380/7:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine_handle, = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
380/8:
# Assign plots to tuples that stores result of plot

# Each point on the sine chart is marked by a blue circle
sine = plt.plot(x_axis, sin, marker ='o', color='blue', label="Sine")
# Each point on the cosine chart is marked by a red triangle
cosine = plt.plot(x_axis, cos, marker='^', color='red', label="Cosine")
383/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
383/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
383/3:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
383/4:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
383/5:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
383/6:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
383/7:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
combined_unemployed_data
384/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
384/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
384/3:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
384/4:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
combined_unemployed_data
384/5:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

# Collect the years where data was collected
years = average_unemployment.keys()
384/6:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

average_unemployment
384/7:
# Collect the years where data was collected
years = average_unemployment.keys()
years
384/8:
# Plot the world average as a line chart
world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
                        color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
384/9:
average_unemployment.plot(label="World Average")
combined_unemployed_data.loc['USA', "2010":"2014"].plot(label="United States")
plt.legend()
plt.show()
385/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
385/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
385/3:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code", "2010": "Year 2010"})
combined_unemployed_data.head()
385/4:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
combined_unemployed_data
385/5:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

average_unemployment
385/6:
# Collect the years where data was collected
years = average_unemployment.keys()
years
385/7:
# Plot the world average as a line chart
world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
                        color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
385/8:
average_unemployment.plot(label="World Average")
combined_unemployed_data.loc['USA', "2010":"2014"].plot(label="United States")
plt.legend()
plt.show()
386/1:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
386/2:
# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2011.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2012-2014.csv")

# Merge our two data frames together
#DF = pd.merge(leftdf,rightdf, on= column_name to be merged,how=inner(by default)
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
#suffix - to be used when there are different columns and different conditions
combined_unemployed_data.head()
386/3:
#Cleaning data
# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code"})
combined_unemployed_data.head()
386/4:
# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")
combined_unemployed_data
386/5:
# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data.mean()

average_unemployment
386/6:
# Collect the years where data was collected
years = average_unemployment.keys()
years
386/7:
# Plot the world average as a line chart
world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
                        color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()
386/8:
average_unemployment.plot(label="World Average")
combined_unemployed_data.loc['USA', "2010":"2014"].plot(label="United States")
plt.legend()
plt.show()
387/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "data/city_data.csv"
ride_data_to_load = "data/ride_data.csv"

# Read the City and Ride Data

# Combine the data into a single dataset

# Display the data table for preview
382/1:
# Import the necessary modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
387/2:
plt.plot(x,y)
plt.show()
387/3:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "../data/city_data.csv"
ride_data_to_load = "../data/ride_data.csv"

# Read the City and Ride Data

# Combine the data into a single dataset

# Display the data table for preview
x =[1,2,3,4]
y = [1,2,3,4]
387/4:
plt.plot(x,y)
plt.show()
387/5:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "../data/city_data.csv"
ride_data_to_load = "../data/ride_data.csv"

# Read the City and Ride Data

# Combine the data into a single dataset

# Display the data table for preview
387/6:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "../data/city_data.csv"
ride_data_to_load = "../data/ride_data.csv"

# Read the City and Ride Data
city_data = pd.read_csv(city_data_to_load)
ride_data = pd.read_csv(ride_data_to_load)
# Combine the data into a single dataset

# Display the data table for preview
387/7:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
city_data = pd.read_csv(city_data_to_load)
ride_data = pd.read_csv(ride_data_to_load)
# Combine the data into a single dataset

# Display the data table for preview
387/8:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
city_data = pd.read_csv(city_data_to_load)
ride_data = pd.read_csv(ride_data_to_load)

city_data.head()
# Combine the data into a single dataset

# Display the data table for preview
387/9:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
city_data = pd.read_csv(city_data_to_load)
ride_data = pd.read_csv(ride_data_to_load)

city_data.head()
ride_data.head()
# Combine the data into a single dataset

# Display the data table for preview
387/10: ride_data.head()
387/11:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
city_data = pd.read_csv(city_data_to_load)
ride_data = pd.read_csv(ride_data_to_load)

city_data.head()
387/12:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/13: rd.head()
387/14:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")

# Display the data table for preview

# Obtain the x and y coordinates for each of the three city types

# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure
387/15:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
crd

# Display the data table for preview

# Obtain the x and y coordinates for each of the three city types

# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure
387/16:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
crd.head()

# Display the data table for preview

# Obtain the x and y coordinates for each of the three city types

# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure
387/17:
# Obtain the x and y coordinates for each of the three city types
type.dtype
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
plt.show()
387/18:
# Obtain the x and y coordinates for each of the three city types
type.valuecount()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
plt.show()
387/19:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/20: rd.head()
387/21:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/22:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/23: rd.head()
387/24:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/25:
# Obtain the x and y coordinates for each of the three city types
type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/26:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd
#type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/27:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd.head()
#type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/28:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd.head()
crd["City_Type"].valuecounts()
#type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/29:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd.head()
City_Types.valuecounts()
#type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/30:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd.head()
help(dtype)
#type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/31:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/32: rd.head()
387/33:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/34:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
crd.head()

City_Type.valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/35:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()

crd["City_Type"].valuecounts()
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/36:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/37: rd.head()
387/38:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/39:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()

crd["City_Type"].valuecounts()
387/40:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
387/41:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/42: rd.head()
387/43:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/44:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
387/45: crd["City_Type"].valuecounts()
387/46:
#crd["City_Type"].valuecounts()
help(crd.valuecounts)
387/47:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
387/48:
#crd["City_Type"].valuecounts()
help(crd.valuecounts)
387/49: crd["City_Type"].value_counts()
387/50: crd["City_Type"].value_counts()
387/51:
uc = crd["city"].loc(crd["City_Type"]=="Urban")
uc
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/52:
uc = crd.loc(crd["City_Type"]=="Urban")
uc
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/53:
uc = crd.loc(crd["City_Type"]=="Urban")
uc
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/54:
uc = crd["City_Type"].loc(crd["City_Type"]=="Urban")

# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/55:
uc = crd.loc[crd["City_Type"]=="Urban"]

# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/56:
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc


# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/57:
uc = crd.loc[(crd["City_Type"]=="Urban")]
#suc


# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/58:

uc = crd.loc[(crd["City_Type"]=="Urban")]
uc
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]






# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/59:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/60: rd.head()
387/61:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/62:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
387/63: crd["City_Type"].value_counts()
387/64: uc
387/65:
#Calculate average fare for each city type
avg_fare = uc.groupby["City_Type"]
387/66:
#Calculate average fare for each city type
avg_fare = uc.groupby("City_Type")
387/67:
#Calculate average fare for each city type
avg_fare = uc.groupby(["City_Type"])["Fare"].mean()
387/68:
#Calculate average fare for each city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
387/69:
#Calculate average fare for each city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
avg_fare.count()
387/70:
#Calculate average fare for each city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
avg_fare
387/71:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
387/72:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
387/73:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["CIty_Type"])["driver_count"].count()
total_drivers
387/74:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
387/75: rd.head()
387/76:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
387/77:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
387/78: crd["City_Type"].value_counts()
387/79:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["CIty_Type"])["driver_count"].count()
total_drivers
387/80:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
total_drivers
387/81:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
total_drivers
387/82:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
#total_drivers
387/83:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
387/84:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
#total_drivers
390/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
390/2: rd.head()
390/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
390/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
390/5: crd["City_Type"].value_counts()
390/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
390/7:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
#total_drivers
390/8:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
390/9:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
390/10:
# Show Figure
plt.
390/11:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
#total_drivers = uc.groupby(["City_Type"])["driver_count"].count()
#total_drivers
390/12:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].sum()
total_drivers
390/13:
#Calculate average fare for each city type
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for each city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count per city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].sum()
total_drivers
390/14:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare = uc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for urban city type
total_rides = uc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers = uc.groupby(["City_Type"])["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare = suc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for suburban city type
total_rides = suc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers = suc.groupby(["City_Type"])["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare = rc.groupby(["City_Type"])["fare"].mean()
#avd_fare
#Calculate total rides for rural city type
total_rides = rc.groupby(["City_Type"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers = rc.groupby(["City_Type"])["driver_count"].sum()
#total_drivers
390/15:
x_axis = [["avg_fare1","avg_fare2","avg_fare3"]]
x_axis
390/16:
x_axis = [["avg_fare1","avg_fare2","avg_fare3"]]
x_axis.series
390/17:
x_axis = [["avg_fare1","avg_fare2","avg_fare3"]]
x_axis
390/18:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["City_Type"])["fare"].mean()
#avg_fare
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby(["City_Type"]).mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
390/19:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["City_Type"])["fare"].mean()
avg_fare
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby(["City_Type"]).mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
390/20:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("City_Type")["fare"].mean()
avg_fare
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
391/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
391/2: rd.head()
391/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
391/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
391/5: crd["City_Type"].value_counts()
391/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
391/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("City_Type")["fare"].mean()
avg_fare
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
391/8:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("City_Type")["fare"].mean()
avg_fare1
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
392/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
392/2: rd.head()
392/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
392/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
392/5: crd["City_Type"].value_counts()
392/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
392/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("City_Type")["fare"].mean()
avg_fare1
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
392/8:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
392/9:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
392/10:
# Show Figure
plt.
392/11:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("City")["fare"].mean()
avg_fare1
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
392/12:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
avg_fare1
# #Calculate total rides for urban city type
# total_rides1 = uc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count urban city type
# total_drivers1 = uc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #SUBURBAN CITY TYPE
# #avg fare for suburban city type
# avg_fare2 = suc.groupby("City_Type").mean()["fare"]
# #avg_fare
# #Calculate total rides for suburban city type
# total_rides2 = suc.groupby("City_Type")["ride_id"].count()
# #total_rides
# #Calculate driver count suburban city type
# total_drivers2 = suc.groupby("City_Type")["driver_count"].sum()
# #total_drivers

# #RURAL CITY TYPE
# #avg fare for rural city type
# avg_fare3 = rc.groupby(["City_Type"])["fare"].mean()
# #avg_fare
# #Calculate total rides for rural city type
# total_rides3 = rc.groupby(["City_Type"])["ride_id"].count()
# #total_rides
# #Calculate driver count rural city type
# total_drivers3 = rc.groupby(["City_Type"])["driver_count"].sum()
# #total_drivers
392/13:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
393/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
393/2: rd.head()
393/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
393/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
393/5: crd["City_Type"].value_counts()
393/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
393/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
393/8:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
393/9:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
393/10:
# Show Figure
plt.
393/11: city_index = crd.set_index("city")["City_Type"]
393/12:
city_index = crd.set_index("city")["City_Type"]
city_index
393/13:
city_index = crd.set_index("city")
city_index
393/14:
city_index = crd.set_index("city")
city_index
393/15:
city_index = crd.set_index("city")
#city_index
393/16:
final_df = pd.DataFrame({"Average Fare":["avg_fare1","avg_fare2","avg_fare1"],"Total Rides":["total_rides1","total_rides2","total_rides3"],
                        "Total Drivers per city":["total_drivers1","total_drivers2","total_drivers3"]})
final_df
393/17:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
394/2: rd.head()
394/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
394/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
394/5: crd["City_Type"].value_counts()
394/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
394/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1,avg_fare2,avg_fare3]
final_total_rides = [total_rides1,total_rides2,total_rides3]
final_total_drivers = [total_drivers1,total_drivers2,total_drivers3]
394/8:
city_index = crd.set_index("city")
#city_index
394/9:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/10:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
394/11:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
394/12:
# Show Figure
plt.
394/13:
final_df = pd.DataFrame({"Average Fare":[total_avg_fare],"Total Rides":[final_total_rides],
                        "Total Drivers per city":]final_total_drivers]})
final_df
394/14:
final_df = pd.DataFrame({"Average Fare":[total_avg_fare],"Total Rides":[final_total_rides],
                        "Total Drivers per city":[final_total_drivers]})
final_df
394/15:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/16:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[[avg_fare1,avg_fare2,avg_fare3]]
final_total_rides = [total_rides1,total_rides2,total_rides3]
final_total_drivers = [total_drivers1,total_drivers2,total_drivers3]
394/17:
city_index = crd.set_index("city")
#city_index
394/18:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/19:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[[avg_fare1,avg_fare2,avg_fare3]]
final_total_rides = [[total_rides1,total_rides2,total_rides3]]
final_total_drivers = [[total_drivers1,total_drivers2,total_drivers3]]
394/20:
city_index = crd.set_index("city")
#city_index
394/21:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/22:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1]+[avg_fare2]+[avg_fare3]
final_total_rides = [[total_rides1,total_rides2,total_rides3]]
final_total_drivers = [[total_drivers1,total_drivers2,total_drivers3]
394/23:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1]+[avg_fare2]+[avg_fare3]
final_total_rides = [total_rides1]+[total_rides2]+[total_rides3]
final_total_drivers = [total_drivers1]+[total_drivers2]+[total_drivers3]
394/24:
city_index = crd.set_index("city")
#city_index
394/25:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
394/26:
city_index = crd.set_index("city")["City_Type"]
#city_index
394/27:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
395/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
395/2: rd.head()
395/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
395/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
395/5: crd["City_Type"].value_counts()
395/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
395/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby("city")["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby("city")["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby("city").mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby("city")["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby("city")["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1]+[avg_fare2]+[avg_fare3]
final_total_rides = [total_rides1]+[total_rides2]+[total_rides3]
final_total_drivers = [total_drivers1]+[total_drivers2]+[total_drivers3]
395/8:
city_index = crd.set_index("city")["City_Type"]
#city_index
395/9:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
395/10:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
395/11:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
395/12:
# Show Figure
plt.
395/13:
city_index = crd.set_index("city")["City_Type"]
city_index
395/14: city_index = crd.set_index("city")["City_Type"]
395/15:
city_index = crd.set_index("city")["City_Type"]
city_index
395/16:
city_index = crd.set_index("city")
city_index
395/17:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["city"])["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby(["city"])["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby(["city"]).mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby(["city"])["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1]+[avg_fare2]+[avg_fare3]
final_total_rides = [total_rides1]+[total_rides2]+[total_rides3]
final_total_drivers = [total_drivers1]+[total_drivers2]+[total_drivers3]
396/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
396/2: rd.head()
396/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
396/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
396/5: crd["City_Type"].value_counts()
396/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
396/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["city"])["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby(["city"])["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby(["city"]).mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby(["city"])["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1]+[avg_fare2]+[avg_fare3]
final_total_rides = [total_rides1]+[total_rides2]+[total_rides3]
final_total_drivers = [total_drivers1]+[total_drivers2]+[total_drivers3]
396/8:
city_index = crd.set_index("city")
city_index
396/9:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
396/10:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
396/11:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
396/12:
# Show Figure
plt.
396/13:
city_index = crd.set_index("city")
city_index.head()
396/14:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
396/15:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["city"])["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby(["city"])["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby(["city"]).mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby(["city"])["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1,avg_fare2,avg_fare3]
final_total_rides = [total_rides1,total_rides2,total_rides3]
final_total_drivers = [total_drivers1,total_drivers2,total_drivers3]
396/16:
city_index = crd.set_index("city")
city_index.head()
396/17:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
396/18:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
396/19:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
396/20:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.set_index("city")
396/21:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.set_index("city")
crd.head()
396/22:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.set_index("city")
396/23:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
397/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
397/2: rd.head()
397/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.set_index("city")
397/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
397/5: crd["City_Type"].value_counts()
397/6:
#Create DF's for each city type- urban, suburban and rural
uc = crd.loc[(crd["City_Type"]=="Urban")]
suc = crd.loc[(crd["City_Type"]=="Suburban")]
rc = crd.loc[(crd["City_Type"]=="Rural")]
397/7:
#Calculate average fare for each city type
#URBAN CITY TYPE
#avg fare for urban city type
avg_fare1 = uc.groupby(["city"])["fare"].mean()
#avg_fare1
#Calculate total rides for urban city type
total_rides1 = uc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count urban city type
total_drivers1 = uc.groupby(["city"])["driver_count"].sum()
#total_drivers

#SUBURBAN CITY TYPE
#avg fare for suburban city type
avg_fare2 = suc.groupby(["city"]).mean()["fare"]
#avg_fare
#Calculate total rides for suburban city type
total_rides2 = suc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count suburban city type
total_drivers2 = suc.groupby(["city"])["driver_count"].sum()
#total_drivers

#RURAL CITY TYPE
#avg fare for rural city type
avg_fare3 = rc.groupby(["city"])["fare"].mean()
#avg_fare
#Calculate total rides for rural city type
total_rides3 = rc.groupby(["city"])["ride_id"].count()
#total_rides
#Calculate driver count rural city type
total_drivers3 = rc.groupby(["city"])["driver_count"].sum()
#total_drivers
total_avg_fare =[avg_fare1,avg_fare2,avg_fare3]
final_total_rides = [total_rides1,total_rides2,total_rides3]
final_total_drivers = [total_drivers1,total_drivers2,total_drivers3]
397/8:
final_df = pd.DataFrame({"Average Fare":total_avg_fare,"Total Rides":final_total_rides,
                        "Total Drivers per city":final_total_drivers})
final_df
397/9:
# Build the scatter plots for each city types

# Incorporate the other graph properties

# Create a legend

# Incorporate a text label regarding circle size

# Save Figure


# Show plot
#plt.show()
397/10:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
397/11:
# Show Figure
plt.
397/12:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
avg_fare



# #Create DF's for each city type- urban, suburban and rural
# uc = crd.loc[(crd["City_Type"]=="Urban")]
# suc = crd.loc[(crd["City_Type"]=="Suburban")]
# rc = crd.loc[(crd["City_Type"]=="Rural")]
397/13:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
397/14:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")[ride_id].count()
total_rides
397/15:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
total_rides
397/16:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
397/17:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
total_drivers
397/18:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
397/19:
plotting_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
plotting_df
397/20:
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df
397/21:
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
397/22:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.show()
397/23:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
398/1:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
398/2:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
398/3: rd.head()
398/4:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
398/5:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
398/6: crd["City_Type"].value_counts()
398/7:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
398/8:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City_Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City_Type"]=="Suburban"]
rc = crd.loc[final_df["City_Type"]=="Rural"]
398/9:
crd["City_Type"].value_counts()
crd = crd.set_index("city")["City_Type"]
crd
398/10:
crd["City_Type"].value_counts()
crd = crd.set_index("city")["City_Type"]
crd.value_counts()
398/11:
crd["City_Type"].value_counts()
crd = crd.set_index("city")["City_Type"]
crd["City_Type"].value_counts()
398/12:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
398/13: rd.head()
398/14:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
398/15:
crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new["City_Type"].value_counts()
398/16:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
398/17:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City_Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City_Type"]=="Suburban"]
rc = crd.loc[final_df["City_Type"]=="Rural"]
398/18:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new["City_Type"].value_counts()
398/19:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
398/20:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
398/21: rd.head()
398/22:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
398/23: rd.head()
398/24: rd.head()
398/25:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
398/26:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
398/27:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new["City_Type"].value_counts()
398/28:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new["City_Type"].value_counts()
398/29:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
#crd_new["City_Type"].value_counts()
398/30:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new
#crd_new["City_Type"].value_counts()
398/31:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
#crd_new
crd_new["City_Type"].value_counts()
398/32:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
#crd_new
crd_new["city"].value_counts()
398/33:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
#crd_new
crd_new.value_counts()
398/34:
crd["City_Type"].value_counts()
#crd_new = crd.set_index("city")["City_Type"]
#crd_new
#crd_new.value_counts()
398/35:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
crd_new
#crd_new.value_counts()
398/36:
#crd["City_Type"].value_counts()
crd_new = crd.set_index("city")["City_Type"]
#crd_new
crd_new.value_counts()
399/1:
#crd["City_Type"].value_counts()
cd_new = cd.set_index("city")["City_Type"]
#crd_new
cd_new.value_counts()
399/2:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
399/3: rd.head()
399/4:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
399/5:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
399/6:
#crd["City_Type"].value_counts()
cd_new = cd.set_index("city")["City_Type"]
#crd_new
cd_new.value_counts()
399/7:
#crd["City_Type"].value_counts()
cd_new = cd.set_index("city")["type"]
#crd_new
cd_new.value_counts()
399/8:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City_Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City_Type"]=="Suburban"]
rc = crd.loc[final_df["City_Type"]=="Rural"]
399/9:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
399/10: rd.head()
399/11:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
399/12:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
399/13:
#set city as index in the cd(city data) set for the city type 
cd_new = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
cd_new.value_counts()
399/14:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
399/15:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City_Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City_Type"]=="Suburban"]
rc = crd.loc[final_df["City_Type"]=="Rural"]
399/16:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":"City_Type"})
final_df.head()
399/17:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":"type"})
final_df.head()
399/18:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":City_Type})
final_df.head()
399/19:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type": type })
final_df.head()
399/20:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers})
final_df.head()
399/21:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
399/22:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/23:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
type(new_city)
399/24:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
type(avg_fare)
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
399/25:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/26:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
type(avg_fare)
# total_rides = crd.groupby("city")["ride_id"].count()
# #total_rides
# total_drivers = crd.groupby("city")["driver_count"].count()
# #total_drivers
# final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
# final_df.head()
399/27:
#Calculate average fare for each city type
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
399/28:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City_Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City_Type"]=="Suburban"]
rc = crd.loc[final_df["City_Type"]=="Rural"]
399/29:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
rc = crd.loc[final_df["City Type"]=="Rural"]
399/30:
#DF's for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
uc
# suc = final_df.loc[final_df["City Type"]=="Suburban"]
# rc = crd.loc[final_df["City Type"]=="Rural"]
399/31:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
type(uc)
# suc = final_df.loc[final_df["City Type"]=="Suburban"]
# #suc
# rc = crd.loc[final_df["City Type"]=="Rural"]
# #rc
399/32:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
399/33:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)

cd.head()
399/34: rd.head()
399/35:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
399/36:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
399/37:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/38:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
399/39:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
399/40:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
399/41:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
cd
399/42:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
399/43:
#set city as index in the cd(city data) set for the city type 
new_city = crd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/44:
#set city as index in the cd(city data) set for the city type 
new_city = crd.set_index("city")["City_Type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/45:
#set city as index in the cd(city data) set for the city type 
new_city = rd.set_index("city")["City_Type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/46:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["City_Type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/47:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
399/48:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
399/49:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
uc
#suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
#rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
399/50:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
400/1: plt.scatter(uc[total_rides],uc[avg_fare],marker="o",color="#d4623", label = "Urban")
400/2:
plt.scatter(uc[total_rides],uc[avg_fare],marker="o",color="#d4623", label = "Urban")
plt.show()
401/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
401/2: rd.head()
401/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
401/4:
# Obtain the x and y coordinates for each of the three city types
crd = crd.rename(columns = {"type":"City_Type"})
#crd.head()
401/5:
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
new_city.value_counts()
401/6:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
avg_fare = crd.groupby("city")["fare"].mean()
#avg_fare
total_rides = crd.groupby("city")["ride_id"].count()
#total_rides
total_drivers = crd.groupby("city")["driver_count"].count()
#total_drivers
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
401/7:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = crd.loc[final_df["City Type"]=="Rural"]
#rc
401/8:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
401/9:
plt.scatter(uc[total_rides],uc[avg_fare],marker="o",color="#d4623", label = "Urban")
plt.show()
401/10:
plt.scatter(uc[total_rides],uc[avg_fare],s = uc[total_drivers]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
401/11:
plt.scatter(uc[total_rides],uc[avg_fare],s = uc[total_drivers]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
401/12:
plt.scatter(uc["total_rides"],uc["avg_fare"],s = uc["total_drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
402/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
402/2: rd.head()
402/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
402/4:
# # Obtain the x and y coordinates for each of the three city types
# crd = crd.rename(columns = {"type":"City_Type"})
# #crd.head()
402/5:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
402/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
402/7:
plt.scatter(uc["total_rides"],uc["avg_fare"],s = uc["total_drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
402/8:
plt.scatter(uc[total_rides],uc["avg_fare"],s = uc["total_drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
403/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
403/2: rd.head()
403/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
403/4:
# # Obtain the x and y coordinates for each of the three city types
# crd = crd.rename(columns = {"type":"City_Type"})
# #crd.head()
403/5:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
403/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
403/7:
plt.scatter(uc["total_rides"],uc["avg_fare"],s = uc["total_drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
403/8:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
404/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
404/2: rd.head()
404/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
404/4:
# # Obtain the x and y coordinates for each of the three city types
# crd = crd.rename(columns = {"type":"City_Type"})
# #crd.head()
404/5:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
404/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
404/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",color="#d4623", label = "Urban")
plt.show()
404/8:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="#d4623", label = "Urban")
plt.show()
404/9:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", label = "Urban")
plt.show()
404/10:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="r", label = "Urban")
plt.show()
404/11:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", label = "Urban")
plt.show()
404/12:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black" label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black" label = "Urban")
plt.show()
404/13:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.show()
404/14:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")

plt.show()
404/15:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")

plt.show()
404/16:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.show()
404/17:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.show()
404/18:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.show()
404/19:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.show()
404/20:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(Urban,Suburban,Rural)
plt.show()
404/21:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend([Urban,Suburban,Rural])
plt.show()
404/22:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*5, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.show()
404/23:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*3, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.show()
404/24:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.show()
404/25:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(0,0,text="Note: Circle size correlates with driver count per city")
plt.show()
404/26:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(0,0,text="Note: Circle size correlates with driver count per city")
plt.show()
404/27:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(0.5,0.5,text="Note: Circle size correlates with driver count per city")
plt.show()
404/28:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(1.5,1.5,text="Note: Circle size correlates with driver count per city")
plt.show()
404/29:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(20,20,text="Note: Circle size correlates with driver count per city")
plt.show()
404/30:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,text="Note: Circle size correlates with driver count per city")
plt.show()
405/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
405/2: rd.head()
405/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
405/4:
# # Obtain the x and y coordinates for each of the three city types
# crd = crd.rename(columns = {"type":"City_Type"})
# #crd.head()
405/5:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
405/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
405/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,text="Note: Circle size correlates with driver count per city")
plt.show()
405/8: # Save Figure
405/9:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
405/10:
# Show Figure
plt.
405/11:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
406/2: rd.head()
406/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
406/4:
# # Obtain the x and y coordinates for each of the three city types
# crd = crd.rename(columns = {"type":"City_Type"})
# #crd.head()
406/5:
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
406/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
406/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="r", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/8: # Save Figure
406/9:
# Calculate Type Percents

# Build Pie Chart

# Save Figure
406/10:
# Show Figure
plt.
406/11:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="orange", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/12:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="b", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/13:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="y", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/14:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="lightyellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/15:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="brightyellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/16:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.Text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/17:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(50,50,"Note: Circle size correlates with driver count per city")
plt.show()
406/18:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(20,20,"Note: Circle size correlates with driver count per city")
plt.show()
406/19:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(45,400,"Note: Circle size correlates with driver count per city")
plt.show()
406/20:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(45,40,"Note: Circle size correlates with driver count per city")
plt.show()
406/21:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(45,40,"Note:" 
         "Circle size correlates with driver count per city")
plt.show()
406/22:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(45,40,"Note:"/n "Circle size correlates with driver count per city")
plt.show()
406/23:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(45,40,"Note:Circle size correlates with driver count per city")
plt.show()
406/24:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(4,40,"Note:Circle size correlates with driver count per city")
plt.show()
406/25:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(40,40,"Note:Circle size correlates with driver count per city")
plt.show()
406/26:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
plt.text(42,40,"Note:Circle size correlates with driver count per city")
plt.show()
406/27:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city")
plt.show()
406/28:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Urban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Urban")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
plt.show()
406/29:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
plt.savefig
plt.show()
406/30:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig(../Pyber/Pyber.bmp)
plt.show()
406/31:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig(./Pyber/Pyber.bmp)
plt.show()
406/32:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig(/Pyber/Pyber.bmp)
plt.show()
406/33:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig(../Images/Pyber.bmp)
plt.show()
406/34:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/Pyber.bmp")
plt.show()
406/35:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/Pyber.jpg")
plt.show()
406/36:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/Pyber.png)
plt.show()
406/37:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/Pyber.png")
plt.show()
406/38:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
408/1:
# Calculate Type Percents
#set index for the city
crd.set_index("city",inplace=True)
crd

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
        shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
        wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/2:
# Calculate Type Percents
#set index for the city
crd.set_index("city",inplace=True)
crd

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/3:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
408/4:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
408/5:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
408/6:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
408/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
408/8:
# Calculate Type Percents
#set index for the city
crd.set_index("city",inplace=True)
crd

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/9:
# Calculate Type Percents
#set index for the city; inplace -modifies the DF in place and does not create a new one
crd.set_index("city",inplace=True)
crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/10:
# Calculate Type Percents
#set index for the city; inplace -modifies the DF in place and does not create a new one
crd.set_index("type",inplace=True)
crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/11:
# Calculate Type Percents
#set index for the city; inplace -modifies the DF in place and does not create a new one
crd.set_index("city",inplace=True)
#crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/12:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

#plt.pie(         , explode=    ,labels=   ,colors="",)
# Save Figure
408/13:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"])
plt.show()
# Save Figure
408/14:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"])
plt.axis("equal")
plt.show()
# Save Figure
408/15:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],color=["y","lightblue","coral"])
plt.axis("equal")
plt.show()
# Save Figure
409/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
409/2:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
409/3:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
409/4:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
409/5:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
409/6:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],color=["yellow","lightblue","coral"])
plt.axis("equal")
plt.show()
# Save Figure
409/7:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],c=["yellow","lightblue","coral"])
plt.axis("equal")
plt.show()
# Save Figure
409/8:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["yellow","lightblue","coral"])
plt.axis("equal")
plt.show()
# Save Figure
409/9:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","yellow"])
plt.axis("equal")
plt.show()
# Save Figure
409/10:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"])
plt.axis("equal")
plt.show()
# Save Figure
409/11:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], shadow=True)
plt.axis("equal")
plt.show()
# Save Figure
409/12:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle=,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/13:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 30,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/14:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 60,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/15:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/16:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.2f%%")
plt.axis("equal")
plt.legend()
plt.show()
# Save Figure
409/17:
# Calculate Type Percents
# #set index for the city; inplace -modifies the DF in place and does not create a new one
# crd.set_index("city",inplace=True)
# #crd
group_fare = crd.groupby("type")
total_fares = group_fare.sum()["fare"]
total_fares
explode=[0,0,0.3]


# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/18:
group_rides = crd.groupby("type")
total_rides = group_fare.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/19:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/20:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 60,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/21:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/22:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%2.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/23:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%3.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/24:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.2f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/25:
# Show Figure
plt.
409/26:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/27:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 130,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/28:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/29:
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/30:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/31:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# # Build Pie Chart
# #plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
# #         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
# #         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

# plt.pie(total_fares,explode=explode,labels=,colors=["coral","lightblue","gold"], 
#         shadow=True,startangle= 80,autopct = "%1.1f%%")
# plt.axis("equal")
# plt.show()
# # Save Figure
409/32:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/33:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.axis("equal")
plt.show()
# Save Figure
409/34:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type")
plt.axis("equal")
plt.show()
# Save Figure
409/35:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/36:
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/37:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Urban","Suburban","Rural"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/38:
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/39:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=[["Rural","Suburban","Urban],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/40:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=[["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
409/41:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
411/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
411/2:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
411/3:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
411/4:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
411/5:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
412/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
412/2:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
412/3:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
412/4:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
412/5:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
#plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
412/6:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
412/7:
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
412/8:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
plt.show()
# Save Figure
414/1:
# Dependencies
import requests
import json
414/2:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
414/3:
# Print the response object to the console
print(requests.get(url))
414/4:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
help(requests.get)
414/5:
# Pretty Print the output of the JSON
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
415/1:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
print(requests.get(url))
415/2:
# Dependencies
import requests
import json
415/3:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
print(requests.get(url))
415/4: print(requests.get(url).json)
415/5: print(requests.get(url).json())
415/6:
# Pretty print JSON for all launchpads
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
415/7:
# Pretty print JSON for a specific launchpad
# response1 = requests.get(url).json
# print(json.dumps(response,indent=4,sort_keys=True))

help(json.dumps)
415/8:
# Pretty print JSON for a specific launchpad
response[1]["id"]
#help(json.dumps)
415/9:
# Pretty print JSON for a specific launchpad
response[1]["details"]
#help(json.dumps)
415/10:
# Pretty print JSON for a specific launchpad
response[1]["padid"]
#help(json.dumps)
415/11:
# Pretty print JSON for a specific launchpad
response[1]["latitude"]
#help(json.dumps)
415/12:
# Pretty print JSON for a specific launchpad
response[1]["latitude"]
#help(json.dumps)
417/1:
# Dependencies
import requests
import json
417/2:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
print(requests.get(url))
417/3: print(requests.get(url).json())
417/4:
# Pretty print JSON for all launchpads
response = requests.get(url).json()
#response is an object
print(json.dumps(response, indent=4, sort_keys=True))
417/5:
# Pretty print JSON for a specific launchpad
response[1]["latitude"]
#help(json.dumps)
417/6:
# Pretty print JSON for a specific launchpad
response[1]["location"]
#help(json.dumps)
417/7:
# Pretty print JSON for a specific launchpad
response[1]["location"]
#help(json.dumps)
type(response)
417/8:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
print(requests.get(url))
typr(url)
417/9:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
print(requests.get(url))
type(url)
417/10:
# Pretty print JSON for all launchpads
response = requests.get(url +"/falcon1").json() 
#The above returns a list
#response is an object
print(json.dumps(response, indent=4, sort_keys=True))
#s in dumps meanstake a python datatype and convert it into a string
417/11:
# Dependencies
import requests
import json
417/12:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url))
type(url)
417/13: print(requests.get(url).json())
417/14:
# Pretty print JSON for all launchpads
response = requests.get(url +"/falcon1").json() 
#The above returns a list
#response is an object
print(json.dumps(response, indent=4, sort_keys=True))
#s in dumps meanstake a python datatype and convert it into a string
416/1:
# Dependencies
import requests
import json
418/1:
# Dependencies
import requests
import json
418/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
418/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
418/4:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
418/5:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
418/6:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
418/7:
# Print the character and the number of films that they were in
# YOUR CODE HERE
data["name"]["films"]
418/8:
# Print the character and the number of films that they were in
# YOUR CODE HERE
print(data["name"]["films"])
418/9:
# Dependencies
import requests
import json
418/10:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
418/11:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
418/12:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
418/13:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
418/14:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
418/15:
# Print the character and the number of films that they were in
# YOUR CODE HERE
print(data["name"]["films"])
418/16:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data)["films"]
418/17:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data)["films"]
print(films_name)
420/1:
# Dependencies
import requests
import json
420/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
420/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
420/4:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
420/5:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
420/6:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
420/7:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data)["films"]
print(films_name)
420/8:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(films_name)
420/9:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by{"name"} {is {films_name})
420/10:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by[]"name"] {is {films_name})
420/11:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by ["name"] {is {films_name})
421/1:
# Dependencies
import requests
import json
421/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
421/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
421/4:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
421/5:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
421/6:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
421/7:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by ["name"] {is {films_name})
421/8:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by {is {films_name})
421/9:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f "The total no. of films by {is {films_name})
421/10:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f"The total no. of films by is {films_name})
422/1:
# Dependencies
import requests
import json
422/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
422/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
422/4:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
422/5:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
422/6:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
422/7:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f"The total no. of films by is {films_name})
422/8:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f"The total no. of films by is {films_name}"")
422/9:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
print(f"The total no. of films by is {films_name}")
422/10:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
name = data["name"]
print(f"The total no. of films by{name} is {films_name}")
422/11:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
name = data["name"]
print(f"The total no. of films by {name} is {films_name}")
423/1:
# Base URL for GET requests to retrieve number/date facts
base_url = "http://numbersapi.com"
423/2:
# Ask the user what kind of data they would like to search for
requests.get(base_url+"/5").text
423/3:
# Dependencies
import requests
import json
423/4:
# Base URL for GET requests to retrieve number/date facts
base_url = "http://numbersapi.com"
423/5:
# Ask the user what kind of data they would like to search for
requests.get(base_url+"/5").text
423/6:
# Ask the user what kind of data they would like to search for
requests.get(base_url+"/8/14/date").text
423/7:
# Ask the user what kind of data they would like to search for
requests.get(base_url+"/8/14/year").text
423/8:
# Ask the user what kind of data they would like to search for
requests.get(base_url+"2017/year").text
423/9:
# Ask the user what kind of data they would like to search for
requests.get(base_url+ "/2017/year").text
423/10:
# Ask the user what kind of data they would like to search for
#returns faact on year 2017
requests.get(base_url+ "/2017/year").text
#returns math trivia
requests.get(base_url+ "/5/math").text
#returns fact on date
requests.get(base_url+ "/8/14/date").text
423/11:
# Ask the user what kind of data they would like to search for
#returns faact on year 2017
requests.get(base_url+ "/2017/year").text
#returns math trivia
requests.get(base_url+ "/5/math").text
#returns fact on date
print(requests.get(base_url+ "/8/14/date").text)
423/12:
# Create code to return a number fact
user_input = input("what type of are you interested in? [Trivia, Math, Date,Year])
423/13:
# Create code to return a number fact
user_input = input("what type of are you interested in? [Trivia, Math, Date,Year]"")
423/14:
# Create code to return a number fact
user_input = input("what type of are you interested in? [Trivia, Math, Date,Year]")
423/15: user_input
423/16:
# Create code to return a number fact
user_input = input("what type of are you interested in? [Trivia, Math, Date,Year]")
423/17: user_input
423/18: if user_input.lower().strip(=="date":)
424/1:
import requests
import json
424/2:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
424/3:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
api_key = "&apikey=trilogy"
424/4:
# Performing a GET request similar to the one we executed
# earlier
response = requests.get(url + "Aliens" + api_key)
print(response.url)
424/5:
# Performing a GET request similar to the one we executed
# earlier
def get_movie(movie):
    print(url+movie+api_key)
response = requests.get(url + "Aliens" + api_key)
print(response.url)
424/6: data[Director]
424/7: data'[Director']
424/8: data['Director']
425/1:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?apikey=trilogy&t="
api_key = "&apikey=trilogy"
425/2:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?apikey=trilogy&t="
api_key = "apikey=2417c4c0"
425/3:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?apikey=2417c4c0&t="
api_key = "apikey=2417c4c0"
425/4:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
425/5:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
425/6:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
data = response.json()
pprint(data)
425/7:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4c0"
425/8:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
data = response.json()
pprint(data)
425/9:
data = response.json()
pprint(data)
425/10:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4c0"
425/11:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
425/12:
data = response.json()
pprint(data)
425/13:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4c0"
425/14:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
425/15:
data = response.json()
pprint(data)
426/1:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4c0"
426/2:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
426/3:
data = response.json()
pprint(data)
426/4: # What was the movie Gladiator rated?
426/5: # What year was 50 First Dates released?
426/6: # Who wrote Moana?
426/7: # What was the plot of the movie Sing?
426/8:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4cO"
426/9:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
426/10:
data = response.json()
pprint(data)
426/11:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4cO"
426/12:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
426/13:
data = response.json()
pprint(data)
426/14:
# Dependencies
import requests
from pprint import pprint

url = "http://www.omdbapi.com/?&t="
api_key = "apikey=2417c4cO"
426/15:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
426/16:
data = response.json()
pprint(data)
426/17:
# Dependencies
import requests
from pprint import pprint
# http://www.omdbapi.com/?apikey=[yourkey]&

url = "http://www.omdbapi.com/?&t="
api_key = "&apikey=2417c4cO"
426/18:
# Who was the director of the movie Aliens?
response = requests.get(url + "Aliens" + api_key)
print(response.url)
426/19:
data = response.json()
pprint(data)
426/20:
# Dependencies
import requests
from pprint import pprint
# http://www.omdbapi.com/?apikey=[yourkey]&

url = "http://www.omdbapi.com/?apikey="
api_key = "&apikey=2417c4cO"
426/21:
# Who was the director of the movie Aliens?
response = requests.get(url + api_key + '&t=aliens')
print(response.url)
426/22:
data = response.json()
pprint(data)
426/23:
# Dependencies
import requests
from pprint import pprint
# http://www.omdbapi.com/?apikey=[yourkey]&

url = "http://www.omdbapi.com/?apikey="
api_key = "&apikey=2417c4c0"
426/24:
# Who was the director of the movie Aliens?
response = requests.get(url + api_key + '&t=aliens')
print(response.url)
426/25:
data = response.json()
pprint(data)
426/26:
# Dependencies
import requests
from pprint import pprint
# http://www.omdbapi.com/?apikey=[yourkey]&

url = "http://www.omdbapi.com/?apikey="
api_key = "&apikey=2417c4c0"
426/27:
# Who was the director of the movie Aliens?
response = requests.get(url + api_key)
print(response.url)
426/28:
data = response.json()
pprint(data)
426/29:
# Dependencies
import requests
from pprint import pprint
# http://www.omdbapi.com/?apikey=[yourkey]&

url = "http://www.omdbapi.com/?apikey="
api_key = "2417c4c0"
426/30:
# Who was the director of the movie Aliens?
response = requests.get(url + api_key + '&aliens')
print(response.url)
426/31:
data = response.json()
pprint(data)
426/32:
# Who was the director of the movie Aliens?
response = requests.get(url + api_key + '&t=aliens')
print(response.url)
426/33:
data = response.json()
pprint(data)
429/1: import config
429/2: api_key
429/3:
# Dependencies
import requests
from pprint import pprint
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
429/4: from config import api_key
429/5: api_key
430/1:
# Dependencies
import requests
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
params = {"api_key":api_key,
         "q":""}

# Store a search term
query = "articles"

# Search for articles published between a begin and end date


# Build url
430/2:
# Dependencies
import requests
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
params = {"api_key":api_key,
         "q":""}

# Store a search term
query = "articles"
query_url = url + "api-key=" + api_key + "&q=" + query
articles = requests.get(query_url).json()
# Search for articles published between a begin and end date


# Build url
430/3:
# Dependencies
import requests
from config import api_key

url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
params = {"api_key":api_key,
         "q":""}

# Store a search term
query = "articles"
query_url = url + "api-key=" + api_key + "&q=" + query
articles = requests.get(query_url).json()
print(articles)
# Search for articles published between a begin and end date


# Build url
414/6:
# Dependencies
import requests
import json
help(requests)
414/7:
# Dependencies
import requests
import json
help(json)
414/8:
# Dependencies
import requests
import json
#help(json)
414/9:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
#help(requests.get)
432/1:
import requests
import json
432/2: url = "https://api.spacexdata.com/v2/launchpads"
432/3:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
432/4: print(requests.get(url))
432/5: requests.get(url).json
432/6: print(requests.get(url).json)
432/7: requests.get(url).json()
432/8: print(requests.get(url).json())
432/9: print(requests.get(url).json())
432/10: requests.get(url).json()
433/1:
s = requests.get(url).json()
print(json.dumps(s))
433/2:
import requests
import json
433/3:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
433/4: print(requests.get(url))
433/5: requests.get(url).json()
433/6:
s = requests.get(url).json()
print(json.dumps(s))
433/7:
s = requests.get(url).json()
json.dumps(s)
433/8:
s = requests.get(url).json()
json.dumps(s,indent=3, sort_keys=True)
433/9:
s = requests.get(url).json()
json.dumps(s,indent=4, sort_keys=True)
433/10:
s = requests.get(url).json()
print(json.dumps(s,indent=4, sort_keys=True))
414/10:
# Pretty Print the output of the JSON
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
help(json.dumps)
414/11:
# Pretty Print the output of the JSON
response = requests.get(url).json()
#print(json.dumps(response, indent=4, sort_keys=True))
help(json.dumps)
414/12:
# Pretty Print the output of the JSON
response = requests.get(url).json()
#print(json.dumps(response, indent=4, sort_keys=True))
#help(json.dumps)
433/11:
#requests.get(url).json()
requests.api
433/12:
#requests.get(url).json()
requests.status_codes
433/13:
#requests.get(url).json()
requests.status_codes
433/14:
#requests.get(url).json()
help(requests.status_codes)
433/15:
#requests.get(url).json()
(helprequests.get)
433/16:
#requests.get(url).json()
help(requests.get)
433/17: help(request.get)
433/18: help(requests.get)
433/19: requests.get(url).text
433/20:
s = requests.get(url).json()
print(json.dumps(s,indent=4, sort_keys=True))
type(s)
433/21:
s = requests.get(url).json()
#print(json.dumps(s,indent=4, sort_keys=True))
type(s)
433/22:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get)
433/23:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url))
433/24:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(import json)
433/25:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(importjson)
433/26:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(import json)
433/27: print(requests.get(url))
433/28:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url).json())
433/29:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url))
433/30:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url).json)
433/31:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url).json())
433/32:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(s)
433/33:
url = "https://api.spacexdata.com/v2/launchpads"
requests.get(url)
type(requests.get(url).text)
433/34: requests.get(url).text()
434/1:
import requests
import json
434/2:
url = https://api.spacexdata.com/v2/rockets/falcon1
requests.get(url).json()
434/3:
url = "https://api.spacexdata.com/v2/rockets/falcon1"
requests.get(url).json()
434/4:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
434/5:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
print(requests.get(url).json())
434/6:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
print(requests.get(url).json())
response = requests.get(url).json()
434/7:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
print(requests.get(url).json())
response = requests.get(url).json()
print(json(response, indent = 4, sort_keys=True))
434/8:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
print(requests.get(url).json())
response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
434/9:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
#print(requests.get(url).json())
response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
434/10:
import requests
import json
434/11:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
434/12:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
# print(json.dumps(response, indent = 4, sort_keys=True))
434/13:
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url).json())
# response = requests.get(url).json()
# print(json.dumps(response, indent = 4, sort_keys=True))
435/1: response = requests.get(url+"/falcon1").json()
435/2:
response = requests.get(url+"/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
435/3:
import requests
import json
435/4:
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url).json())
# response = requests.get(url).json()
# print(json.dumps(response, indent = 4, sort_keys=True))
435/5:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
435/6: response[1]
435/7: response[0]
435/8:
import requests
import json
435/9:
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url).json())
# response = requests.get(url).json()
# print(json.dumps(response, indent = 4, sort_keys=True))
435/10:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
435/11: response[0]
435/12:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
435/13:
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
435/14:
import requests
import json
435/15:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
435/16:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
435/17: response[0]["description"]
435/18: response[1]["description"]
435/19:
import requests
import json
435/20:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
435/21:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
435/22: response[1]["description"]
436/1:
# Dependencies
import requests
import json
436/2:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url))
type(url)
436/3: print(requests.get(url).json())
438/1:
# Dependencies
import requests
import json
438/2:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/rockets"
print(requests.get(url))
type(url)
438/3:
# Pretty print JSON for all launchpads
response = requests.get(url +"/falcon1").json() 
#The above returns a list
#response is an object
print(json.dumps(response, indent=4, sort_keys=True))
#s in dumps meanstake a python datatype and convert it into a string
438/4:
# Pretty print JSON for a specific launchpad
response[1]["location"]
#help(json.dumps)
438/5:
# Pretty print JSON for a specific launchpad
response[1]["location"]
#help(json.dumps)
help(reaponse)
438/6:
# Pretty print JSON for a specific launchpad
#response[1]["location"]
#help(json.dumps)
help(reaponse)
438/7:
# Pretty print JSON for a specific launchpad
#response[1]["location"]
#help(json.dumps)
help(response)
438/8:
# Pretty print JSON for a specific launchpad
response[1]["location"]
#help(json.dumps)
#help(response)
438/9:
# Pretty print JSON for a specific launchpad
response[1]["location"]
print(response)
#help(json.dumps)
#help(response)
438/10:
# Pretty print JSON for a specific launchpad
response[1]["location"]
print(response.json())
#help(json.dumps)
#help(response)
437/1: type(response)
437/2: #response[1]["description"]
437/3: type(response)
437/4:
import requests
import json
437/5:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
print(json.dumps(response, indent = 4, sort_keys=True))
437/6:
url = "https://api.spacexdata.com/v2/rockets"
#print(requests.get(url).json())
# response = requests.get(url).json()
#print(json.dumps(response, indent = 4, sort_keys=True))
437/7:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
437/8: type(response)
437/9:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
type9response)
437/10:
response = requests.get(url + "/falcon1").json()
print(json.dumps(response,indent=4,sort_keys=True))
type(response)
437/11:
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
type(response)
437/12: response[1]["description"]
437/13:
url = "https://api.spacexdata.com/v2/rockets/falcon1"
#print(requests.get(url).json())
# response = requests.get(url).json()
#print(json.dumps(response, indent = 4, sort_keys=True))
437/14:
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
type(response)
437/15: response[1]["description"]
437/16: response["description"]
437/17: type(response)
441/1:
import requests
import json
441/2:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
441/3:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
#requests.get(url).json()
441/4:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
response = requests.get(url).json()
print(response,indent=4,sort_keys=True)
441/5:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
441/6: response["description"]
441/7: response["thrust"]["ibf"]
441/8: response["ibf"]
441/9: response["diameter"]
441/10: response["diameter"]["feet"]
441/11: response["second_stage"]["pay_loads"]["diameter"]["meters"]
441/12: response["second_stage"]["payloads"]["diameter"]["meters"]
441/13: response["second_stage"]["payloads"]["composite_fairing"["diameter"]["meters"]
441/14: response["second_stage"]["payloads"]["composite_fairing"]["diameter"]["meters"]
441/15: len(response)
441/16: type(len(response))
441/17: help(len(response))
441/18: (len(response))
419/1:
# It is also possible to perform some
# analyses on values stored within the JSON object
number_payloads = len(response_json["payload_weights"])
print(f"There are {number_payloads} payloads.")
419/2:
# Dependencies
import requests
import json
419/3:
# Performing a GET Request and saving the 
# API's response within a variable
url = "https://api.spacexdata.com/v2/rockets/falcon9"
response = requests.get(url)
response_json = response.json()
print(json.dumps(response_json, indent=4, sort_keys=True))
419/4:
# It is possible to grab a specific value 
# from within the JSON object
print(response_json["cost_per_launch"])
419/5:
# It is also possible to perform some
# analyses on values stored within the JSON object
number_payloads = len(response_json["payload_weights"])
print(f"There are {number_payloads} payloads.")
441/19: len(payload_weights)
441/20: len(response)(payload_weights)
441/21: len(response)[payload_weights]
441/22: len(response)["payload_weights"]
441/23: payload = len(response)["payload_weights"]
441/24:
payload = len(response)["payload_weights"]
print(payload)
441/25:
import requests
import json
441/26:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
441/27: response["description"]
441/28: response["second_stage"]["payloads"]["composite_fairing"]["diameter"]["meters"]
441/29: (len(response))
441/30:
payload = len(response)["payload_weights"]
#print(payload)
441/31:
payload = len(response["payload_weights"])
#print(payload)
441/32:
payload = len(response["payload_weights"])
print(payload)
441/33: payload[0]["lb"]
441/34: lb = payload[0]["lb"]
441/35:
import requests
import json
441/36:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
441/37: response["description"]
441/38: response["second_stage"]["payloads"]["composite_fairing"]["diameter"]["meters"]
441/39: (len(response))
441/40:
payload = len(response["payload_weights"])
print(payload)
441/41: lb = payload[0]["lb"]
441/42: lb = response["payload"][0]["lb"]
441/43:
import requests
import json
441/44:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
requests.get(url).json()
response = requests.get(url).json()
print(json.dumps(response,indent=4,sort_keys=True))
441/45: response["description"]
441/46: response["second_stage"]["payloads"]["composite_fairing"]["diameter"]["meters"]
441/47: (len(response))
441/48:
payload = len(response["payload_weights"])
print(payload)
441/49: lb = response["payload"][0]["lb"]
441/50: lb = response["payload_weights"][0]["lb"]
441/51:
lb = response["payload_weights"][0]["lb"]
print(lb)
441/52: print("The second payload weight is {lb}")
441/53: print(f"The second payload weight is {lb}")
446/1:
import pandas as pd
import matplotlib as plt
import requests
import json
446/2:
url = "https://swapi.co/api/people/"
print(url)
446/3:
character id = "4"
url_1 = url + character id
url_1
446/4:
character_id = "4"
url_1 = url + character_id
url_1
446/5: requests.get(url_1)
446/6: print(requests.get(url_1))
446/7: requests.get(url_1).json()
446/8: response = requests.get(url_1).json()
446/9:
response = requests.get(url_1).json()
response
446/10: type(response)
446/11: response["name"]
446/12: response["films"]
446/13:
response = requests.get(url_1).json()
print(json.dumps(response,indent=4,sort_keys=True))
446/14: type(response)
446/15: type(response)
446/16: response["name"]
446/17: response["films"]
446/18: response["Species"]
446/19: response["species"]
446/20: len(response)
446/21: len(response)["films"]
446/22: len(response["films"])
446/23:
name = response["name"]
film_data = len(response["films"])
print("name has acted in film_data films")
446/24:
name = response["name"]
film_data = len(response["films"])
print("name has acted in [film_data] films")
446/25:
name = response["name"]
film_data = len(response["films"])
print("name has acted in {film_data} films")
446/26:
name = response["name"]
film_data = len(response["films"])
print("}name} has acted in {film_data} films")
446/27:
name = response["name"]
film_data = len(response["films"])
print("{name} has acted in {film_data} films")
446/28:
name = response["name"]
film_data = len(response["films"])
print(f"{name} has acted in {film_data} films")
446/29: response["species"]
446/30: response["starships"][0]
446/31: starship_url = response["starships"][0]
447/1:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_startship_url = data["starship"][0]
print(first_starship_url)
447/2:
# Dependencies
import requests
import json
447/3:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
447/4:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
447/5:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
447/6:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
447/7:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
447/8:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
name = data["name"]
print(f"The total no. of films by {name} is {films_name}")
447/9:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_startship_url = data["starship"][0]
print(first_starship_url)
447/10:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_startship_url = data["starships"][0]
print(first_starship_url)
447/11:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
print(first_starship_url)
447/12:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url)
print(first_starship_url)
447/13:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url)
print(news_url)
447/14:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url)
print(new_url)
447/15:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url)
new_url
447/16:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url)
new_url
response = new_url
print(json.dumps(response, indent=4,sort_keys=True))
447/17:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url).json()
new_url
response = new_url
print(json.dumps(response, indent=4,sort_keys=True))
447/18:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url).json()
new_url
# response = new_url
# print(json.dumps(response, indent=4,sort_keys=True))
447/19:
#Bonus
for film in data["films"]:
    requests.get(film).json()
print(film)
447/20:
#Bonus
films = []
for film in data["films"]:
    new_film = requests.get(film).json()
    films.append(new_film)
print(film)
447/21:
#Bonus
films = []
for film in data["films"]:
    new_film = requests.get(film).json()
    film_title=new_film["title"]
    films.append(film_title)
print(film)
447/22:
#Bonus
films = []
for film in data["films"]:
    new_film = requests.get(film).json()
    #film_title=new_film["title"]
    films.append(new_film)
print(films)
447/23:
#Bonus
films = []
for film in data["films"]:
    new_film = requests.get(film).json()
    film_title=new_film["title"]
    films.append(new_film)
print(films)
447/24:
#Bonus
films = []

for film in data["films"]:
    new_film = requests.get(film).json()
    film_title=new_film["title"]
    films.append(new_film)
print(films)
447/25:
# Dependencies
import requests
import json
447/26:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
447/27:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
447/28:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
447/29:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
447/30:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
447/31:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
name = data["name"]
print(f"The total no. of films by {name} is {films_name}")
447/32:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url).json()
new_url
# response = new_url
# print(json.dumps(response, indent=4,sort_keys=True))
447/33:
#Bonus
films = []

for film in data["films"]:
    new_film = requests.get(film).json()
    film_title=new_film["title"]
    films.append(new_film)
print(films)
450/1:
# Dependencies
import requests
import json
450/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
450/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
450/4:
# Perform a get request for this character
response = requests.get(url)
print(response.url)
450/5:
# Storing the JSON response within a variable
data = response.json()
print(json.dumps(data, indent=4, sort_keys=True))
450/6:
# Collecting the name of the character collected
# YOUR CODE HERE
data["name"]
450/7:
# Print the character and the number of films that they were in
# YOUR CODE HERE
films_name = len(data["films"])
name = data["name"]
print(f"The total no. of films by {name} is {films_name}")
450/8:
# Figure out what their first starship was and print the ship
# YOUR CODE HERE
first_starship_url = data["starships"][0]
new_url = requests.get(first_starship_url).json()
new_url
# response = new_url
# print(json.dumps(response, indent=4,sort_keys=True))
450/9:
#Bonus
films = []

for film in data["films"]:
    new_film = requests.get(film).json()
    film_title=new_film["title"]
    films.append(new_film)
print(films)
450/10:
# Print what their first ship was
# YOUR CODE HERE
450/11:
#Bonus
# films = []

# for film in data["films"]:
#     new_film = requests.get(film).json()
#     film_title=new_film["title"]
#     films.append(new_film)
# print(films)
450/12:
#Bonus
# films = []

# for film in data["films"]:
#     new_film = requests.get(film).json()
#     film_title=new_film["title"]
#     films.append(new_film)
# print(films)
films = []

for film in data['films']:
    cur_film = requests.get(film).json()
    film_title = cur_film["title"]
    films.append(film_title)
    
print(f"{character_name} was in:")
print(films)
450/13:
#Bonus
# films = []

# for film in data["films"]:
#     new_film = requests.get(film).json()
#     film_title=new_film["title"]
#     films.append(new_film)
# print(films)
films = []

for film in data['films']:
    cur_film = requests.get(film).json()
    film_title = cur_film["title"]
    films.append(film_title)
    
#print(f"{character_name} was in:")
print(films)
452/1:
import json
import requests
452/2: url = "http://numbersapi.com"
452/3:
url = "http://numbersapi.com"
requests.get(url)
452/4: input("What type of number are you interested in? [Trivia, Math, Date, Year]  ")
452/5:
# requests.get(url + ""/2017/year").text
# requests.get(url+"/5/Math").text
# requests.get(url+"/5").text
print(requests.get(url+"/8/14/Date").text)
457/1:
import requests
import json
457/2:
url = "http://numbersapi.com/"
url
457/3:
url = "http://numbersapi.com/"
print(url)
457/4:
url = "http://numbersapi.com/"
url
457/5: input("What type of number do u wann input?  [Trivia, Math, Date, Year]")
457/6: input("What type of number do u wann input?  [Trivia, Math, Date, Year]     ")
458/1:
import requests
import json
458/2:
url = "http://numbersapi.com/"
url
458/3: a = input("What type of number do u wanna input?  [Trivia, Math, Date, Year]")
458/4: num_needed = a
458/5:
a = input("What type of number do u wanna input?  [Trivia, Math, Date, Year]")
num_needed = a
458/6:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    requests.get(f"{url}{month}{day}/{a.lower()}?json").json()
458/7:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(f"{url}{month}{day}/{a.lower()}?json").json()
    print(response)
458/8:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(f"{url}{month}{day}/{a.lower()}?json").json()
    print(response.json)
458/9:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(f"{url}{month}{day}/{a.lower()}?json").json()
    print(response)
458/10:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(f"{url}{month}{day}/{a.lower()}?json").json()
    print(response["text"])
458/11:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(url+month+day+"/"+a.lower()+ "?json").json()
    print(response["text"])
# else:
#     number = input("What number would you like to search for?   ")
#     response = requests.get()
458/12:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(url+month+day+"/"+a.lower()+ "?json").json()
    print(response["text"])
else:
    number = input("What number would you like to search for?   ")
    response = requests.get(url+number+"/"+a.lower()+"?json").json()
    print(response["text"])
458/13:
import requests
import json
458/14:
url = "http://numbersapi.com/"
url
458/15:
a = input("What type of number do u wanna input?  [Trivia, Math, Date, Year]")
num_needed = a
458/16:
if(a.lower()=="date"):
    month =input("What month do you req info for?   ")
    day = input("What day would you like to search for?   ")
    response = requests.get(url+month+day+"/"+a.lower()+ "?json").json()
    print(response["text"])
else:
    number = input("What number would you like to search for?   ")
    response = requests.get(url+number+"/"+a.lower()+"?json").json()
    print(response["text"])
462/1:
import requests
import json
from pprint import pprint
460/1:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
460/2:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
api_key = "&apikey=trilogy"
requests.
460/3:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
460/4:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
460/5:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
api_key = "&apikey=trilogy"
460/6:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
460/7: dir(response)
460/8: json_data = response.json()
460/9: json_data
460/10:
import json 
print(json.dumps(response.json(),indent=4,sort_keys=True))
460/11:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
460/12:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?apikey=trilogy&t="
#http://www.omdbapi.com/?apikey=[yourkey]&
#api_key = "&apikey=trilogy"
460/13:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
460/14: dir(response)
460/15: json_data = response.json()
460/16: json_data
460/17:
import json 
print(json.dumps(response.json(),indent=4,sort_keys=True))
460/18:
# Converting the response to JSON, and printing the result.
data = response.json()
pprint(data)
460/19:
# Print a few keys from the response JSON.
print(f"Movie was directed by {data['Director']}.")
print(f"Movie was released in {data['Country']}.")
460/20: dir(object_e)
460/21: json_data = object_e.json()
460/22: json_data
460/23:
import json 
print(json.dumps(response.json(),indent=4,sort_keys=True))
460/24:
# Converting the response to JSON, and printing the result.
data = response.json()
pprint(data)
460/25:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
460/26:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
api_key = "&apikey=trilogy"
460/27:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
460/28: dir(object_e)
460/29: json_data = object_e.json()
460/30: json_data
460/31:
import json 
print(json.dumps(response.json(),indent=4,sort_keys=True))
460/32:
# Converting the response to JSON, and printing the result.
data = response.json()
pprint(data)
460/33:
# Print a few keys from the response JSON.
print(f"Movie was directed by {data['Director']}.")
print(f"Movie was released in {data['Country']}.")
460/34:
import json 
print(json.dumps(object_e.json(),indent=4,sort_keys=True))
460/35:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
460/36:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
api_key = "&apikey=trilogy"
460/37:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
460/38: dir(object_e)
460/39: json_data = object_e.json()
460/40: json_data
460/41:
import json 
print(json.dumps(object_e.json(),indent=4,sort_keys=True))
460/42:
# Converting the response to JSON, and printing the result.
data = response.json()
pprint(data)
460/43:
# Converting the response to JSON, and printing the result.
data = object_e.json()
pprint(data)
464/1:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens")
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
464/2:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
464/3:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
#url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
url = "http://www.omdbapi.com/?apikey=trilogy"
#api_key = "&apikey=trilogy"
464/4:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens")
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
464/5:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens")
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
print(json.dumps(object_e.json(,indent=4, sort_keys=True))
464/6:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens")
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
print(json.dumps(object_e.json(),indent=4, sort_keys=True))
464/7:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
464/8:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
#url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
url = "http://www.omdbapi.com/?apikey=trilogy"
#api_key = "&apikey=trilogy"
464/9:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens")
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
print(json.dumps(object_e.json(),indent=4, sort_keys=True))
464/10:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens").json()
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
print(json.dumps(object_e.json(),indent=4, sort_keys=True))
464/11:
# New Dependency! Use this to pretty print the JSON
# https://docs.python.org/3/library/pprint.html
from pprint import pprint
import requests
464/12:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
#url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
url = "http://www.omdbapi.com/?apikey=trilogy"
#api_key = "&apikey=trilogy"
464/13:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url+"&t=Aliens").json()
#object_e = requests.get(url + "Aliens" + api_key)
print(object_e)
print(json.dumps(object_e.json(),indent=4, sort_keys=True))
464/14:
import json 
print(json.dumps(object_e.json(),indent=4,sort_keys=True))
464/15:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
464/16:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
#url = "http://www.omdbapi.com/?t="
#http://www.omdbapi.com/?apikey=[yourkey]&
url = "http://www.omdbapi.com/?apikey=trilogy"
#api_key = "&apikey=trilogy"
464/17:
# Note that the ?t= is a query param for the t-itle of the
# movie we want to search for.
url = "http://www.omdbapi.com/?t="
api_key = "&apikey=trilogy"
464/18:
# Performing a GET request similar to the one we executed
# earlier
object_e = requests.get(url + "Aliens" + api_key)
print(object_e.url)
464/19: dir(object_e)
464/20: json_data = object_e.json()
464/21: json_data
464/22:
import json 
print(json.dumps(object_e.json(),indent=4,sort_keys=True))
464/23:
# Converting the response to JSON, and printing the result.
data = object_e.json()
pprint(data)
464/24:
# Print a few keys from the response JSON.
print(f"Movie was directed by {data['Director']}.")
print(f"Movie was released in {data['Country']}.")
464/25:
#json_data = object_e.json()
object_e
464/26:
#json_data = object_e.json()
object_e.json()
464/27:
# Converting the response to JSON, and printing the result.
data = object_e.json()
print(data)
464/28:
# Converting the response to JSON, and printing the result.
data = object_e.json()
print(data)
464/29:
# Converting the response to JSON, and printing the result.
data = object_e.json()
pprint(data)
464/30:
# Converting the response to JSON, and printing the result.
data = object_e.json()
print(data)
465/1:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json()
print(f'The director of Aliens was {movie["Director"]}.')
type(movie)
465/2:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="
465/3:
# Who was the director of the movie Aliens?
movie = requests.get(url + "Aliens").json()
print(f'The director of Aliens was {movie["Director"]}.')
type(movie)
466/1:
# Dependencies
import random
import json
import requests
466/2:
# Let's get the JSON for 100 posts sequentially.
url = "http://jsonplaceholder.typicode.com/posts/"
466/3:
# Create an empty list to store the responses
response_json = []
466/4:
# Create random indices representing
# a user's choice of posts
indices = random.sample(list(range(1, 100)), 10)
indices
random.sample
466/5:
# Create random indices representing
# a user's choice of posts
indices = random.sample(list(range(1, 100)), 10)
indices
random.sample
466/6:
# Create random indices representing
# a user's choice of posts
indices = random.sample(list(range(1, 100)), 10)
indices
#random.sample
466/7:
# Make a request for each of the indices
for x in range(len(indices)):
    print(f"Making request number: {x} for ID: {indices[x]}")

    # Get one of the posts
    post_response = requests.get(url + str(indices[x]))

    # Save post's JSON
    response_json.append(post_response.json())
467/1:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
    responses
#     responses.append(movie_data)
#     print(f'The director of {movie} is {movie_data["Director"]}')
467/2:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
    responses
#     responses.append(movie_data)
#     print(f'The director of {movie} is {movie_data["Director"]}')
467/3: # responses
467/4:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
#     responses.append(movie_data)
#     print(f'The director of {movie} is {movie_data["Director"]}')
467/5:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
    movie_data
#     responses.append(movie_data)
#     print(f'The director of {movie} is {movie_data["Director"]}')
467/6:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
    print(movie_data)
#     responses.append(movie_data)
#     print(f'The director of {movie} is {movie_data["Director"]}')
467/7:
# Dependencies
import requests

url = "http://www.omdbapi.com/?apikey=trilogy&t="

movies = ["Aliens", "Sing", "Moana"]

responses = [];

for movie in movies:
    movie_data = requests.get(url + movie).json()
    print(movie_data)
#     responses.append(movie_data)
print(f'The director of {movie} is {movie_data["Director"]}')
471/1:
# Dependencies
import json

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
471/2:
# Dependencies
import os
import json

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
471/3: json_data
471/4: print(json_data["title"])
471/5: print(json_data["items"]["title"])
471/6: print(json_data["data"]["items"][5]["title"])
471/7: print(json_data["data"]["items"])
471/8: print(json_data[5]["title"])
471/9: print(json_data["items"][5]["title"])
471/10: print(json_data["items"]["title"])
471/11: print(json_data["title"])
471/12: print(json_data
471/13: response = json_data["items"][5]["title"]
471/14: response = json_data[5]["title"]
471/15: data_items = json_data["data"]["items"].json()
471/16:
data_items = json_data["data"]["items"]
data_items
471/17: items =  json_data["items"][0]["title"]
471/18: items =  json_data[0]["title"]
471/19: items =  json_data["data_items"][0]["title"]
471/20:
data_items = json_data["data"]["items"]
data_items.json()
471/21:
data_items = json_data["data"]["items"]
data_items
471/22: items =  json_data["data_items"][0]["title"]
471/23:
# Dependencies
import json

# Load JSON
filepath = os.path.join("..", "Resources", "youtube_response.json")
with open(filepath) as jsonfile:
    json_data = json.load(jsonfile)
471/24: json_data
471/25:
data_items = json_data["data"]["items"]
data_items
471/26: items =  json_data["data_items"][0]["title"]
476/1:
# Specify the URL
url = 'http://nyt-mongo-scraper.herokuapp.com/api/headlines'
# Make request and store response
response = requests.get(url)
json_file = response.json()
476/2:
# Dependencies
import json
import requests
476/3:
# Specify the URL
url = 'http://nyt-mongo-scraper.herokuapp.com/api/headlines'
# Make request and store response
response = requests.get(url)
json_file = response.json()
476/4:
# JSON-ify response
json_file
476/5:
# Specify the URL
url = 'http://nyt-mongo-scraper.herokuapp.com/api/headlines/?'
# Make request and store response
response = requests.get(url)
json_file = response.json()
476/6:
# Print first and last articles
first_article = requests.get(url + "saved")
print(first_article)
476/7:
# Print first and last articles
first_article = requests.get(url + "saved")
first_article
476/8:
# Print first and last articles
first_article = requests.get(url + "saved")
first_article
476/9:
# Print first and last articles
first_article = requests.get(url + "saved")
first_article.json()
476/10:
# Dependencies
import json
import requests 
from pprint import pprint
476/11:
# Specify the URL
url = 'http://nyt-mongo-scraper.herokuapp.com/api/headlines/?'
# Make request and store response
response = requests.get(url)
json_file = response.json()
476/12:
# JSON-ify response
json_file
476/13:
# Print first and last articles
first_article = requests.get(url + "saved")
json_data = first_article.json()
json_data
476/14:
# Print first and last articles
first_article = requests.get(url + "saved")
json_data = first_article[0].json()
json_data
476/15:
#Print the number of responses received.
json_file[0]
476/16: json_file[-1]
476/17: len(json_file)
473/1:
# Dependencies
import json
import requests
from config import api_key
473/2:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/3:
# Dependencies
import json
import requests
from config import api_key
473/4:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/5:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
473/6:
# Dependencies
import json
import requests
from config import api_key
473/7:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/8:
# Dependencies
import json
import requests
from config import api_key
473/9:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/10:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
473/11:
# Dependencies
import json
import requests
from config import api_key
473/12:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/13:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
473/14:
# Dependencies
import json
import requests
from config import 43ea6db4d138b3a3bf0abd932401ec1f
473/15:
# Dependencies
import json
import requests
from config import api_key
473/16:
# Dependencies
import json
import requests
from config import api_key
473/17:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/18:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
473/19:
# Dependencies
import json
import requests
from config import api_key
473/20:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/21:
# Dependencies
import json
import requests
from config import api_key
473/22:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + api_key + "&q=" + city
473/23:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
478/1:
# Dependencies
import json
import requests
from config import api_key
478/2:
# Dependencies
import json
import requests
from config import api_key
478/3:
# Dependencies
import json
import requests
from config import My_api_key
478/4:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + 25bc90a1196e6f153eece0bc0b0fc9eb + "&q=" + city
478/5:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + My_api_key + "&q=" + city
478/6:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")
479/1:
# Dependencies
import requests
from config import My_api_key
479/2:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"

# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
480/1:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q" = Bujumbura, "units"="metric"}
# Get weather data
weather_response = requests.get(url+params)
weather_json = weather_response.json()
480/2:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q" = "Bujumbura", "units"="metric"}
# Get weather data
weather_response = requests.get(url+params)
weather_json = weather_response.json()
480/3:
# Dependencies
import requests
from config import My_api_key
480/4:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q" = "Bujumbura", "units"="metric"}
# Get weather data
weather_response = requests.get(url+params)
weather_json = weather_response.json()
481/1:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q" = "Bujumbura", "units"="metric"}
# Get weather data
weather_response = requests.get(url,params=params)
weather_json = weather_response.json()
481/2:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q":"Bujumbura", "units":"metric"}
# Get weather data
weather_response = requests.get(url,params=params)
weather_json = weather_response.json()
481/3:
# Dependencies
import requests
from config import My_api_key
481/4:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q":"Bujumbura", "units":"metric"}
# Get weather data
weather_response = requests.get(url,params=params)
weather_json = weather_response.json()
481/5:
# Build query URL and request your results in Celsius
url = "http://api.openweathermap.org/data/2.5/weather?"
params= {"appid": My_api_key, "q":"Bujumbura", "units":"metric"}
# Get weather data
weather_response = requests.get(url,params=params)
weather_json = weather_response.json()
weather_json
481/6:
# Get temperature from JSON response
weather["main"]["temp"]
481/7:
# Get temperature from JSON response
weather_json["main"]["temp"]
482/1:
# Dependencies
import csv
import matplotlib.pyplot as plt
import requests
import pandas as pd
from config import My_api_key
482/2:
# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL
query_url = f"{url}appid={My_api_key}&units={units}&q="
482/3:
cities = ["Paris", "London", "Oslo", "Beijing"]

# set up lists to hold reponse info
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    params = {"appid": My_api_key,"q": city,"units": units}
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    temp.append(response['main']['temp'])

print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")
482/4:
# create a data frame from cities, lat, and temp
weather_dict = {
    "city": cities,
    "lat": lat,
    "temp": temp
}
weather_data = pd.DataFrame(weather_dict)
weather_data.head()
482/5:
# Build a scatter plot for each data type
plt.scatter(weather_data["lat"], weather_data["temp"], marker="o")

# Incorporate the other graph properties
plt.title("Temperature in World Cities")
plt.ylabel("Temperature (Celsius)")
plt.xlabel("Latitude")
plt.grid(True)

# Save the figure
plt.savefig("TemperatureInWorldCities.png")

# Show plot
plt.show()
483/1:
#list of tv show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", "The Flash", "Vikings", "Shameless", "Arrow", "Peaky Blinders", "Dirk Gently"]
url = "https://www.tvmaze.com/search/show?q="

print
# make iterative requests to TVmaze search endpoint
ratings = []
networks = []

# make iterative requests to TVmaze search endpoint
base_url = "http://api.tvmaze.com/search/shows?q="
for show in tv_shows:
    params = {"q": show}
    data = requests.get(base_url, params).json()
    titles.append(data[0]['show']['name'])
    ratings.append(data[0]['show']['rating']['average'])
    try:
        networks.append(data[0]['show']['network']['name'])
    except:
        networks.append("N/A")
483/2:
#Dependencies
import requests
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
483/3:
#list of tv show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", "The Flash", "Vikings", "Shameless", "Arrow", "Peaky Blinders", "Dirk Gently"]
url = "https://www.tvmaze.com/search/show?q="

print
# make iterative requests to TVmaze search endpoint
ratings = []
networks = []

# make iterative requests to TVmaze search endpoint
base_url = "http://api.tvmaze.com/search/shows?q="
for show in tv_shows:
    params = {"q": show}
    data = requests.get(base_url, params).json()
    titles.append(data[0]['show']['name'])
    ratings.append(data[0]['show']['rating']['average'])
    try:
        networks.append(data[0]['show']['network']['name'])
    except:
        networks.append("N/A")
483/4:
#list of tv show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", "The Flash", "Vikings", "Shameless", "Arrow", "Peaky Blinders", "Dirk Gently"]
url = "https://www.tvmaze.com/search/show?q="

# make iterative requests to TVmaze search endpoint
titles = []
ratings = []
networks = []

# make iterative requests to TVmaze search endpoint
base_url = "http://api.tvmaze.com/search/shows?q="
for show in tv_shows:
    params = {"q": show}
    data = requests.get(base_url, params).json()
    titles.append(data[0]['show']['name'])
    ratings.append(data[0]['show']['rating']['average'])
    try:
        networks.append(data[0]['show']['network']['name'])
    except:
        networks.append("N/A")
483/5:
# create dataframe
tv_shows={"Title":name,"Ratings":ratings,"Network": networks}
483/6:
# create dataframe
tv_shows={"Title":name,"Ratings":ratings,"Network": networks}
tv_data = pd.DataFrame(tv_shows)
483/7:
# create dataframe
tv_shows={"Title":name,"Ratings":ratings,"Network": networks}
tv_data = pd.DataFrame(tv_shows)
tv_data.head()
483/8:
# create dataframe
tv_shows={"Title":titles,"Ratings":ratings,"Network": networks}
tv_data = pd.DataFrame(tv_shows)
tv_data.head()
484/1:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}

print(students["Jezebel"])

print("This line will never print.")
485/1:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x={[0,1]
# Try to access key that doesn't exist
try:
   x[2]
   students["Jezebel"]
except KeyError:
    print("Oops, that key doesn't exist.")
except
   print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
485/2:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x={[0,1]
# Try to access key that doesn't exist
try:
   x[2]
   students["Jezebel"]
except KeyError:
    print("Oops, that key doesn't exist.")
except
   print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
485/3:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x={[0,1]
# Try to access key that doesn't exist
try:
    x[2]
    students["Jezebel"]
except KeyError:
    print("Oops, that key doesn't exist.")
except
   print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
485/4:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x=[0,1]
# Try to access key that doesn't exist
try:
    x[2]
    students["Jezebel"]
except KeyError:
    print("Oops, that key doesn't exist.")
except
   print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
485/5:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x=[0,1]
# Try to access key that doesn't exist
try:
    x[2]
    students["Jezebel"]
except KeyError:
    print("Oops, that key doesn't exist.")
except:
   print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
485/6:
students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}
x=[0,1]
# Try to access key that doesn't exist
try:
    students["Jezebel"]
    x[2]
except KeyError:
    print("Oops, that key doesn't exist.")
except:
    print("general handling")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")
486/1:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except:
    print("I think her name was + " + name + "?")

# print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/2:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except:ZeroDivisionError
    print("I think her name was + " + name + "?")

# print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/3:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
excep ZeroDivisionError:
    print("I think her name was + " + name + "?")

# print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/4:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print("I think her name was + " + name + "?")

# print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/5:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print("I think her name was + " + name + "?")
except 
    print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/6:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print(ZeroDivisionError)
    
    print("I think her name was + " + name + "?")
except 
    print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/7:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print(ZeroDivisionError)
    
    print("I think her name was + " + name + "?")
 
    print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/8:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print(DivisionError)
    
    print("I think her name was + " + name + "?")
 
    print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/9:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print(DivisionError)
    
try:
    print("I think her name was + " + name + "?")
except:
 
    print("Your name is a nonsense number. Look: " + int("Gabriel"))

# print("You made it through the gauntlet--the message survived!")
486/10:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print(DivisionError)
486/11:
# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print("DivisionError")
486/12:
try:
    print("I think her name was + " + name + "?")
except:
486/13:

    print("I think her name was + " + name + "?")
486/14: print("I think her name was + " + name + "?")
486/15:
try:
    print("I think her name was + " + name + "?")
except nameerror
     print("Nameerror")
487/1:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
487/2:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
487/3:
# Create settings dictionary with information we're interested in
settings = {"units": "metric", "appid": api_key}
487/4:
# Get current weather
current_weather_paris = owm.get_current("Paris", **settings)
print(f"Current weather object for Paris: {current_weather_paris}.")
487/5:
summary = ["name", "main.temp"]

data = current_weather_paris(*summary)
print(f"The current weather summary for Paris is: {data}.")
487/6:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
487/7:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
487/8:
# Create settings dictionary with information we're interested in
settings = {"units": "metric", "appid": api_key}
487/9:
# Get current weather
current_weather_paris = owm.get_current("Paris", **settings)
print(f"Current weather object for Paris: {current_weather_paris}.")
487/10:
summary = ["name", "main.temp"]

data = current_weather_paris(*summary)
print(f"The current weather summary for Paris is: {data}.")
488/1:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
488/2:
# Create settings dictionary with information we're interested in
settings = {"units": "metric", "appid": api_key}
488/3:
# Get current weather
current_weather_paris = owm.get_current("Paris", **settings)
print(f"Current weather object for Paris: {current_weather_paris}.")
488/4:
summary = ["name", "main.temp"]

data = current_weather_paris(*summary)
print(f"The current weather summary for Paris is: {data}.")
486/16:
try:
    print("I think her name was + " + name + "?")
486/17: print("I think her name was + " + name + "?")
486/18:
try:
    print("I think her name was + " + name + "?")
except NameError:
    print("NameError")
486/19: print("Your name is a nonsense number. Look: " + int("Gabriel"))
486/20:
try:
    print("Your name is a nonsense number. Look: " + int("Gabriel"))
except ValueError:
    print("ValueError")
486/21: print("You made it through the gauntlet--the message survived!")
490/1:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
490/2:
# Dependencies
import openweathermapy.core as owm

#config
from config import api_key
490/3:
# Create settings dictionary with information we're interested in
#settings var same as params
settings = {"units": "metric", "appid": api_key}
owm.get_current("Paris",units=settings["units", appid = api_key])
490/4:
# Dependencies
import openweathermapy.core as owm
#alias lib to 
#config
from config import api_key
490/5: #!pip install openweathermapy
490/6:
# Create settings dictionary with information we're interested in
#settings var same as params
settings = {"units": "metric", "appid": api_key}
owm.get_current("Paris",units=settings["units", appid = api_key])
490/7:
# Create settings dictionary with information we're interested in
#settings var same as params
settings = {"units": "metric", "appid": api_key}
491/1:
# Dependencies
import csv
import matplotlib.pyplot as plt
import openweathermapy as ow
import pandas as pd

# import api_key from config file
from config import api_key
491/2: # Create a settings object with your API key and preferred units
491/3:
# Get data for each city in cities.csv
csvpath = os.path.join('..', 'Resources', 'cities.csv')
491/4:
# Dependencies
import os
import csv
import matplotlib.pyplot as plt
import openweathermapy as ow
import pandas as pd

# import api_key from config file
from config import api_key
491/5:
# Create a settings object with your API key and preferred units
settings = {"units": "metric", "appid": api_key}
491/6:
# Get data for each city in cities.csv
csvpath = os.path.join('..', 'Resources', 'cities.csv')
491/7:
# Get data for each city in cities.csv
csvpath = os.path.join('..', 'Resources', 'cities.csv')
with open(csvpath, newline='') as csvfile:

    # CSV reader specifies delimiter and variable that holds contents
    csvreader = csv.reader(csvfile, delimiter=',')

    print(csvreader)
491/8:
# Get data for each city in cities.csv
csvpath = os.path.join('..', 'Resources', 'cities.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    print(csvreader)
for row in csvreader:
    print(row)
491/9:
# Get data for each city in cities.csv
csvpath = os.path.join('..', 'Resources', 'cities.csv')
with open(csvpath, newline='') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    print(csvreader)
492/1: !pip install citipy
492/2:
# Dependencies
from citipy import citipy
492/3: !pip install citipy
492/4:
# Some random coordinates
coordinates = [(200, 200), (23, 200), (42, 100)]
492/5:
cities = []
for coordinate_pair in coordinates:
    lat, lon = coordinate_pair
    cities.append(citipy.nearest_city(lat, lon))
492/6:
for city in cities:
    country_code = city.country_code
    name = city.city_name
    print(f"The country code of {name} is '{country_code}'.")
493/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
493/2:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
493/3:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
493/4:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
493/5:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
#plt.savefig("../Images/New Bitmap Image.BMP")
plt.show()
493/6:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("../Images/pyber_starter.png")
plt.show()
493/7:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/pyber_starter.png")
plt.show()
493/8:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/pyber_starter.png")
plt.show()
493/9:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/Rides_CityType.png")
plt.show()
493/10:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
493/11:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
493/12:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
493/13:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
493/14:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*10, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*10, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*10, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
493/15:
# Calculate Type Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/Fares_CityType.png")
plt.show()
493/16:
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/Rides_CityType.png")
plt.show()
493/17:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images/Drivers_CityType.png")
plt.show()
502/1: %matplotlib notebook
502/2:
# Dependencies
import matplotlib.pyplot as plt
import numpy as np
502/3:
# Generate the x values from 0 to 10 using a step of 0.1
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)
502/4:
# Add a semi-transparent horizontal line at y = 0
plt.hlines(0, 0, 10, alpha=0.25)
502/5:
# Add labels to the x and y axes
plt.title("Juxtaposed Sine and Cosine Curves")
plt.xlabel("Input (Sampled Real Numbers from 0 to 10)")
plt.ylabel("Value of Sine (blue) and Cosine (red)")
502/6:
# Set your x and y limits - xlim - takes the min and max x values and y takes the min and max y values
plt.xlim(0, 10)
plt.ylim(-1, 1)
502/7:
# Set a grid on the plot
plt.grid()
502/8:
# Save the plot and display it
plt.savefig("../Images/sin_cos_with_markers.png")
plt.show()
502/9: help(plt.savefig)
493/18:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")
#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
493/19:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend("City Type", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
493/20:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title:"City Type", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
493/21:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Type", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
493/22:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
505/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
rd.head()
505/2:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
505/3:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
505/4:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
505/5:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
505/6:
# Calculate Fare Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Fares_CityType.png")
plt.show()
505/7:
#Calculate rides percent
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Rides_CityType.png")
plt.show()
505/8:
# Calculate Driver Percents
group_drivers = crd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Drivers_CityType.png")
plt.show()
505/9:
# Calculate Driver Percents
group_drivers = cd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Drivers_CityType.png")
plt.show()
505/10:
#Calculate rides percent
group_rides = rd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Rides_CityType.png")
plt.show()
505/11:
#Calculate rides percent
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Rides_CityType.png")
plt.show()
505/12:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
505/13: rd.head()
505/14:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
505/15:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
505/16:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
505/17:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
505/18:
# Calculate Fare Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Fares_CityType.png")
plt.show()
505/19:
#Calculate rides percent
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Rides_CityType.png")
plt.show()
505/20:
# Calculate Driver Percents
#consider the city DF here instead of the merged DF to avoid duplicate driver counts
group_drivers = cd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Drivers_CityType.png")
plt.show()
505/21:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="light coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
505/22:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="lightblue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="yellow", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
505/23:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
506/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
506/2: rd.head()
506/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
506/4:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
506/5:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
506/6:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
plt.legend
506/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",c ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
506/8:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",c ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",c ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
506/9:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/1:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
507/2: rd.head()
507/3:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
507/4:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
507/5:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
507/6:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="Light Coral", edgecolors="black", label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="Light Sky Blue", edgecolors="black", label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black", label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/7:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="Light Coral", edgecolors="black", alpha =0.2, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="Light Sky Blue", edgecolors="black", alpha =0.2, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.2, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/8:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="LightCoral", edgecolors="black", alpha =0.2, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.2, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.2, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/9:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="LightCoral", edgecolors="black", alpha =0.1, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.2, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.2, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/10:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="LightCoral", edgecolors="black", alpha =1, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.2, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.2, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/11:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*15, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/12:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*25, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/13:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/14:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size" /n
                "correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/15:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/16:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:Circle size\n correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/17:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right")
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/18:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=2., scatterpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/19:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=2, scatterpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/20:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.2, scatterpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/21:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.4, scatterpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/22:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3, scatterpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/23:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/24:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3, numpoint=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/25:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3, numpoints=1)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/26:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/27:
%matplotlib inline
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# File to Load (Remember to change these)
city_data_to_load = "./data/city_data.csv"
ride_data_to_load = "./data/ride_data.csv"

# Read the City and Ride Data
cd = pd.read_csv(city_data_to_load)
rd = pd.read_csv(ride_data_to_load)
#check DF
cd.head()
507/28: rd.head()
507/29:
# Combine the data into a single dataset
crd = pd.merge(cd,rd,on="city")
# Display the data table for preview
crd.head()
507/30:
# Obtain the x and y coordinates for each of the three city types
#Calculations for each city type - generate series for average fare, total rides and total drivers 
#avg_fare
group_city = crd.groupby("city")
avg_fare = group_city.mean()["fare"]
#total_rides
total_rides = group_city.count()["ride_id"]
#total_drivers
total_drivers = group_city.count()["driver_count"]
#set city as index in the cd(city data) set for the city type 
new_city = cd.set_index("city")["type"]
#check the different values of the cd_new set by value_counts
#new_city.value_counts()
#Create DF for all the above 3 along with the city type
final_df = pd.DataFrame({"Average Fare":avg_fare,"Total Rides":total_rides,"Total Drivers":total_drivers,"City Type":new_city})
final_df.head()
507/31:
# for each city type- urban, suburban and rural
uc = final_df.loc[final_df["City Type"]=="Urban"]
#uc
suc = final_df.loc[final_df["City Type"]=="Suburban"]
#suc
rc = final_df.loc[final_df["City Type"]=="Rural"]
#rc
507/32:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
507/33:
#Total Fares by City Type
# Calculate Fare Percents
group_fares = crd.groupby("type")
total_fares = group_fares.sum()["fare"]
total_fares
explode=[0,0,0.3]
total_fares.index
# labels=["Urban","Suburban","Rural"]
# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_fares,explode=explode,labels=total_fares.index,colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Fares by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Fares_CityType.png")
plt.show()
507/34:
# Total rides by City Type
#Calculate rides percent
group_rides = crd.groupby("type")
total_rides = group_rides.sum()["ride_id"]
total_rides
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_rides,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 80,autopct = "%1.1f%%")
plt.title("% of Total Rides by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Rides_CityType.png")
plt.show()
507/35:
# Total Drivers by City Type
# Calculate Driver Percents
#consider the city DF here instead of the merged DF to avoid duplicate driver counts
group_drivers = cd.groupby("type")
total_drivers = group_drivers.sum()["driver_count"]
total_drivers
explode=[0,0,0.3]

# Build Pie Chart
#plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, 
#         shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, 
#         wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, hold=None, data=None)

plt.pie(total_drivers,explode=explode,labels=["Rural","Suburban","Urban"],colors=["coral","lightblue","gold"], 
        shadow=True,startangle= 120,autopct = "%1.1f%%")
plt.title("% of Total Drivers by City Type",loc="right")
plt.axis("equal")
# Save Figure
plt.savefig("./Images_solution/Drivers_CityType.png")
plt.show()
507/36:
plt.scatter(uc["Total Rides"],uc["Average Fare"],s = uc["Total Drivers"]*20, marker="o",color ="LightCoral", edgecolors="black", alpha =0.8, label = "Urban")
plt.scatter(suc["Total Rides"],suc["Average Fare"],s = suc["Total Drivers"]*15, marker="o",color ="LightSkyBlue", edgecolors="black", alpha =0.8, label = "Suburban")
plt.scatter(rc["Total Rides"],rc["Average Fare"],s = rc["Total Drivers"]*15, marker="o",color ="Gold", edgecolors="black",alpha =0.8, label = "Rural")
plt.grid()
plt.title("Pyber Ride Sharing Data(2016)")
plt.xlabel("Total Number of Rides(Per City)")
plt.ylabel("Average Fare($)")
plt.legend(title="City Types", loc = "upper right", markerscale=0.3)
#DO NOT USE plt.Text, use only plt.text
plt.text(42,40,"Note:\n Circle size correlates with driver count per city.")
#plt.text(>x_axis_max,=y_axis,"text")

#Save fig
plt.savefig("./Images_solution/Pyber_Ride_Sharing_Data.png")
plt.show()
513/1:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df
513/2:
# Dependencies
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Google API Key
from config import gkey
513/3:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df
518/1:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \"address=%s&key=%s" % (target_city, gkey)
518/2:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \"address=%s&key=%s" % (target_city, gkey)
518/3:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
518/4:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \"address=%s&key=%s" % (target_city, gkey)
518/5:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \"address=%s&key=%s" % (target_city, gkey)
518/6:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" 
            \"address=%s&key=%s" % (target_city, gkey)
518/7:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
518/8:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
518/9:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
518/10:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
519/1:
# Print the json (pretty printed)
print(json.dumps(geo_data, indent=4, sort_keys=True))
519/2:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
519/3:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
519/4:
# Print the json (pretty printed)
print(json.dumps(geo_data, indent=4, sort_keys=True))
519/5:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
519/6:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
519/7:
# Print the json (pretty printed)
print(json.dumps(geo_data, indent=4, sort_keys=True))
519/8:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
519/9:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
520/1:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
target_city = "Boise, Idaho"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
520/2:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data)
520/3:
# Print the json (pretty printed)
print(json.dumps(geo_data, indent=4, sort_keys=True))
520/4:
# Extract latitude and longitude
lat = geo_data["results"][0]["geometry"]["location"]["lat"]
lng = geo_data["results"][0]["geometry"]["location"]["lng"]

# Print the latitude and longitude
print("%s: %s, %s" % (target_city, lat, lng))
521/1:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data["results"])
521/2:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
#target_city = "Boise, Idaho"
target_city = "Austin, Texas"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
521/3:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data["results"])
521/4:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey

# Target city
#target_city = "Boise, Idaho"
target_city = "Austin, Texas"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
#https://maps.googleapis.com/maps/api/geocode/json?" \
#"address = (target_city)& key =(gkey)"
#base_url =  https://maps.googleapis.com/maps/api/geocode/json?"
#params = {"address": target_city,
             #"key": gkey}
#geo_data = requests.get(base_url,params=params).json()
521/5:
# Run a request to endpoint and convert result to json
geo_data = requests.get(target_url).json()

# Print the json
print(geo_data["results"])
522/1:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey
522/2: typr(response)
522/3: type(response)
522/4: typeof(response)
522/5: type(response)
522/6:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey
522/7:
# geocoordinates
target_coordinates = "43.6187102, -116.2146068"
target_search = "Chinese"
target_radius = 8000
target_type = "restaurant"

# set up a parameters dictionary
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "type": target_type,
    "key": gkey
}

# base url
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
#geocode(previous) - https://maps.googleapis.com/maps/api/geocode/json?

# run a request using our params dictionary
response = requests.get(base_url, params=params)
522/8: type(response)
522/9: len(places_data["results"])
522/10:
# Dependencies
import requests
import json

# Google developer API key
from config import gkey
522/11:
# geocoordinates
target_coordinates = "43.6187102, -116.2146068"
target_search = "Chinese"
target_radius = 8000
target_type = "restaurant"

# set up a parameters dictionary
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "type": target_type,
    "key": gkey
}

# base url
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
#geocode(previous) - https://maps.googleapis.com/maps/api/geocode/json?

# run a request using our params dictionary
response = requests.get(base_url, params=params)
522/12: type(response)
522/13:
# print the response url, avoid doing for public github repos in order to avoid exposing key
#print(response.url)
522/14:
# convert response to json
places_data = response.json()

# Print the json (pretty printed)
print(json.dumps(places_data, indent=4, sort_keys=True))
522/15:
# Print the name and address of the first restaurant that appears
print(places_data["results"][0]["name"])
print(places_data["results"][0]["vicinity"])
522/16: len(places_data["results"])
522/17:
for result in places_data["results"]:
    print("places")
522/18:
for result in places_data["results"]:
    print ("name")
522/19:
for result in places_data["results"]:
    print (name)
522/20:
for result in places_data["results"]:
    result[name]
522/21:
for result in places_data["results"]:
    print(result["name"])
524/1:
# Create code to answer each of the following questions.
# Hint: You will need multiple target URLs and multiple API requests.

# Dependencies
import requests
import json

# Retrieve Google API key from config.py
from config import gkey
target_city = "Seattle, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
524/2:
# Create code to answer each of the following questions.
# Hint: You will need multiple target URLs and multiple API requests.

# Dependencies
import requests
import json

# Retrieve Google API key from config.py
from config import gkey
target_city = "Seattle, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
524/3:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
geo_code = requests.get(target_url)
target_coordinates = "47.605901, -122.331783"
524/4:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
geo_code = requests.get(target_url)
geo_code
target_coordinates = "47.605901, -122.331783"
524/5:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
geo_code = requests.get(target_url)
geo_code
#target_coordinates = "47.605901, -122.331783"
524/6:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
geo_code = requests.get(target_url)
geo_code.json()
#target_coordinates = "47.605901, -122.331783"
524/7:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url).json()
#target_coordinates = "38.897868, -77036487"
524/8:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url).json()
#target_coordinates = "38.897868, -77036487"
524/9:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url).json()
print(geo_code)
#target_coordinates = "38.897868, -77036487"
524/10:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url)
print(geo_code.json())
#target_coordinates = "38.897868, -77036487"
524/11:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url)
geo_code.json()
#target_coordinates = "38.897868, -77036487"
524/12:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_search = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
524/13:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_search = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
bike_result = response.json()
524/14:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_search = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
bike_result = response.json()
print(json.dumps(bike_reslut, indent=4, sort_keys=True))
524/15:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_search = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
bike_result = response.json()
print(json.dumps(bike_result, indent=4, sort_keys=True))
524/16:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_type = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "type": target_type,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params=params)
bike_result = response.json()
print(json.dumps(bike_result, indent=4, sort_keys=True))
print(bike_result["results"][0]["name"])
print(bike_reslut["resluts"][0]["vicicnity"])
524/17:
print(bike_result["results"][0]["name"])
print(bike_reslut["resluts"][0]["vicicnity"])
524/18:
print(bike_result["results"][0]["name"])
print(bike_result["resluts"][0]["vicicnity"])
527/1:
# Create code to answer each of the following questions.
# Hint: You will need multiple target URLs and multiple API requests.

# Dependencies
import requests
import json

# Retrieve Google API key from config.py
from config import gkey
target_city = "Seattle, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
527/2:
# 1. What are the geocoordinates (latitude/longitude) of Seattle, Washington?
geo_code = requests.get(target_url)
geo_code.json()
target_coordinates = "47.605901, -122.331783"
527/3:
# 2. What are the geocoordinates (latitude/longitude) of The White House?
target_city = "The White House, Washington"

# Build the endpoint URL
target_url = "https://maps.googleapis.com/maps/api/geocode/json?" \
"address=%s&key=%s" % (target_city, gkey)
geo_code = requests.get(target_url)
geo_code.json()
#target_coordinates = "38.897868, -77036487"
527/4:
# 3. Find the name and address of a bike store in Seattle, Washington.
#    Hint: See https://developers.google.com/places/web-service/supported_types
target_coordinates = "47.605901, -122.331783"
target_type = "bicycle_store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "type": target_type,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params=params)
bike_result = response.json()
print(json.dumps(bike_result, indent=4, sort_keys=True))
527/5:
print(bike_result["results"][0]["name"])
print(bike_result["resluts"][0]["vicicnity"])
527/6:
print(bike_result["results"][0]["name"])
print(bike_result["results"][0]["vicicnity"])
527/7:
# 4. Find a balloon store near the White House.
target_coordinates = "47.605901, -122.331783"
target_search = "store"
target_radius = 8000
params = {
    "location": target_coordinates,
    "keyword": target_search,
    "radius": target_radius,
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
balloon_result = response.json()
print(json.dumps(balloon_result, indent=4, sort_keys=True))
527/8:
print(bike_result["results"][0]["name"])
print(bike_result["results"][0]["vicinity"])
527/9:
# 4. Find a balloon store near the White House.
target_coordinates = "47.605901, -122.331783"
target_search = "balloon store"

params = {
    "location": target_coordinates,
    "keyword": target_search,
    "rankby": "distance",
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
balloon_result = response.json()
print(json.dumps(balloon_result, indent=4, sort_keys=True))
527/10:
# 4. Find a balloon store near the White House.
target_coordinates = "47.605901, -122.331783"
target_search = "balloon store"

params = {
    "location": target_coordinates,
    "keyword": target_search,
    "rankby": "distance",
    "key": gkey
}
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
response = requests.get(base_url,params)
balloon_result = response.json()
print(json.dumps(balloon_result, indent=4, sort_keys=True))
525/1:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df.head()
525/2:
# Dependencies
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Google API Key
from config import gkey
525/3:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df.head()
528/1:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Import API key
from config import gkey
525/4:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df
525/5:
types_df = pd.read_csv("../Resources/ethnic_restr.csv")
types_df.head()
528/2:
# Import cities file into the cities_pd DataFrame
cities = pd.read_csv("../Resources/Cities.csv")
cities.head()
#cities_pd = pd.DataFrame
528/3:
cities["lat"] = ""
cities["lng"] = ""
cities["Airport name"] = ""
cities["Airport Address"] = ""
cities["Airport Rating"] = ""
cities.head()
528/4:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities["lat"] = ""
cities["lng"] = ""
cities["Airport name"] = ""
cities["Airport Address"] = ""
cities["Airport Rating"] = ""
cities.head()
528/5:
# Loop through the cities_pd and get the lat/long for each city
# Hint: `requests.get(target_url).json()`
# Hint: `for index, row in cities_pd.iterrows():`
# Hint: `cities_pd.loc`
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
params = {"latitude: "lat",
        "longitude": "lng",
        "type":"International Airports"
        "rankby": "distance",
        "key": gkey,}
for index, row in cities.iterrows():
    city_coord = row["City"]
    response = requests.get(base_url, params=params).json()
    resluts = response["results"]
528/6:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities["Lat"] = ""
cities["Lng"] = ""
cities["Airport name"] = ""
cities["Airport Address"] = ""
cities["Airport Rating"] = ""
cities.head()
528/7:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Import API key
from config import gkey
528/8:
# Import cities file into the cities_pd DataFrame
cities = pd.read_csv("../Resources/Cities.csv")
cities.head()
#cities_pd = pd.DataFrame
529/1:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Import API key
from config import gkey
529/2:
# Import cities file into the cities_pd DataFrame
cities = pd.read_csv("../Resources/Cities.csv")
cities.head()
#cities_pd = pd.DataFrame
529/3:
# Add columns for lat, lng, airport name, airport address, airport rating.
# Hint: Be sure to specify "" as the initial value for each column.
cities["Lat"] = ""
cities["Lng"] = ""
cities["Airport name"] = ""
cities["Airport Address"] = ""
cities["Airport Rating"] = ""
cities.head()
529/4:
# Loop through the cities_pd and get the lat/long for each city
# Hint: `requests.get(target_url).json()`
# Hint: `for index, row in cities_pd.iterrows():`
# Hint: `cities_pd.loc`
base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
params = {"latitude: "lat",
        "longitude": "lng",
        "type":"International Airports"
        "rankby": "distance",
        "key": gkey,}
for index, row in cities.iterrows():
    city_coord = row["City"]
    response = requests.get(base_url, params=params).json()
    resluts = response["results"]
530/1:
# params dictionary to update each iteration
params = {
    "radius": 50000,
    "types": "airport",
    "keyword": "international airport",
    "key": gkey
}

# Use the lat/lng we recovered to identify airports
for index, row in cities_pd.iterrows():
    # get lat, lng from df
    lat = row["Lat"]
    lng = row["Lng"]

    # change location each iteration while leaving original params in place
    params["location"] = f"{lat},{lng}"

    # Use the search term: "International Airport" and our lat/lng
    base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

    # make request and print url
    name_address = requests.get(base_url, params=params)
    
#     print the name_address url, avoid doing for public github repos in order to avoid exposing key
#     print(name_address.url)

    # convert to json
    name_address = name_address.json()
    # print(json.dumps(name_address, indent=4, sort_keys=True))

    # Since some data may be missing we incorporate a try-except to skip any that are missing a data point.
    try:
        cities_pd.loc[index, "Airport Name"] = name_address["results"][0]["name"]
        cities_pd.loc[index, "Airport Address"] = name_address["results"][0]["vicinity"]
        cities_pd.loc[index, "Airport Rating"] = name_address["results"][0]["rating"]
    except (KeyError, IndexError):
        print("Missing field/result... skipping.")
530/2:
# Dependencies
import pandas as pd
import numpy as np
import requests
import json

# Google API Key
from config import gkey
530/3:
# Import cities file as DataFrame
cities_pd = pd.read_csv("../Resources/cities.csv")
cities_pd.head()
530/4:
# Add columns for lat, lng, airport name, airport address, airport rating
# Note that we used "" to specify initial entry.
cities_pd["Lat"] = ""
cities_pd["Lng"] = ""
cities_pd["Airport Name"] = ""
cities_pd["Airport Address"] = ""
cities_pd["Airport Rating"] = ""
cities_pd.head()
530/5:
# create a params dict that will be updated with new city each iteration
params = {"key": gkey}

# Loop through the cities_pd and run a lat/long search for each city
for index, row in cities_pd.iterrows():
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    city = row['City']
    state = row['State']

    # update address key value
    params['address'] = f"{city},{state}"

    # make request
    cities_lat_lng = requests.get(base_url, params=params)
    
    # print the cities_lat_lng url, avoid doing for public github repos in order to avoid exposing key
    # print(cities_lat_lng.url)
    
    # convert to json
    cities_lat_lng = cities_lat_lng.json()

    cities_pd.loc[index, "Lat"] = cities_lat_lng["results"][0]["geometry"]["location"]["lat"]
    cities_pd.loc[index, "Lng"] = cities_lat_lng["results"][0]["geometry"]["location"]["lng"]

# Visualize to confirm lat lng appear
cities_pd.head()
530/6:
# params dictionary to update each iteration
params = {
    "radius": 50000,
    "types": "airport",
    "keyword": "international airport",
    "key": gkey
}

# Use the lat/lng we recovered to identify airports
for index, row in cities_pd.iterrows():
    # get lat, lng from df
    lat = row["Lat"]
    lng = row["Lng"]

    # change location each iteration while leaving original params in place
    params["location"] = f"{lat},{lng}"

    # Use the search term: "International Airport" and our lat/lng
    base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

    # make request and print url
    name_address = requests.get(base_url, params=params)
    
#     print the name_address url, avoid doing for public github repos in order to avoid exposing key
#     print(name_address.url)

    # convert to json
    name_address = name_address.json()
    # print(json.dumps(name_address, indent=4, sort_keys=True))

    # Since some data may be missing we incorporate a try-except to skip any that are missing a data point.
    try:
        cities_pd.loc[index, "Airport Name"] = name_address["results"][0]["name"]
        cities_pd.loc[index, "Airport Address"] = name_address["results"][0]["vicinity"]
        cities_pd.loc[index, "Airport Rating"] = name_address["results"][0]["rating"]
    except (KeyError, IndexError):
        print("Missing field/result... skipping.")
530/7:
# Save Data to csv
cities_pd.to_csv("Airport_Output.csv")

# Visualize to confirm airport data appears
cities_pd.head(10)
532/1: !pip install gmaps
532/2:
import gmaps

# Google developer API key
from config import gkey
532/3: !pip install gmaps
532/4:
coordinates = [
   (40.71, -74.00),
   (30.26, -97.74),
   (46.87, -96.78),
   (47.60, -122.33),
   (32.71, -117.16)
]
533/1: help(gmaps.marker_layer)
533/2:
import gmaps

# Google developer API key
from config import gkey

# Access maps with unique API key
gmaps.configure(api_key=gkey)
533/3:
# Create a list containing coordinates
coordinates = [
    (40.71, -74.00),
    (30.26, -97.74),
    (46.87, -96.78),
    (47.60, -122.33),
    (32.71, -117.16)
]
533/4:
# Customize the size of the figure
figure_layout = {
    'width': '400px',
    'height': '300px',
    'border': '1px solid black',
    'padding': '1px',
    'margin': '0 auto 0 auto'
}
fig = gmaps.figure(layout=figure_layout)
533/5:
# Assign the marker layer to a variable
markers = gmaps.marker_layer(coordinates)
# Add the layer to the map
fig.add_layer(markers)
fig
533/6:
import gmaps

# Google developer API key
from config import gkey

# Access maps with unique API key
gmaps.configure(api_key=gkey)
533/7:
# Create a list containing coordinates
coordinates = [
    (40.71, -74.00),
    (30.26, -97.74),
    (46.87, -96.78),
    (47.60, -122.33),
    (32.71, -117.16)
]
533/8:
# Customize the size of the figure
figure_layout = {
    'width': '400px',
    'height': '300px',
    'border': '1px solid black',
    'padding': '1px',
    'margin': '0 auto 0 auto'
}
fig = gmaps.figure(layout=figure_layout)
533/9:
# Assign the marker layer to a variable
markers = gmaps.marker_layer(coordinates)
# Add the layer to the map
fig.add_layer(markers)
fig
534/1:
import gmaps

# Google developer API key
from config import gkey

# Access maps with unique API key
gmaps.configure(api_key=gkey)
534/2:
# Create a list containing coordinates
coordinates = [
    (40.71, -74.00),
    (30.26, -97.74),
    (46.87, -96.78),
    (47.60, -122.33),
    (32.71, -117.16)
]
534/3:
# Customize the size of the figure
figure_layout = {
    'width': '400px',
    'height': '300px',
    'border': '1px solid black',
    'padding': '1px',
    'margin': '0 auto 0 auto'
}
fig = gmaps.figure(layout=figure_layout)
534/4:
# Assign the marker layer to a variable
markers = gmaps.marker_layer(coordinates)
# Add the layer to the map
fig.add_layer(markers)
fig
535/1:
import gmaps
import pandas as pd

# Google developer API key
from config import gkey

# Configure gmaps
gmaps.configure(api_key=gkey)
535/2:
# Create aiport dataframe
airport_df = pd.read_csv("../Resources/Airport_Output.csv")
airport_df
535/3:
# Create aiport dataframe
airport_df = pd.read_csv("../Resources/Airport_Output.csv")
airport_df.head()
535/4:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations
# Filla NaN values and convert to float
535/5:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations.head()
# Filla NaN values and convert to float
535/6:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations
# Filla NaN values and convert to float
535/7:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations.value_counts()
# Filla NaN values and convert to float
535/8:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations.head()
# Filla NaN values and convert to float
534/5:
import gmaps

# Google developer API key
from config import gkey

# Access maps with unique API key
gmaps.configure(api_key=gkey)
534/6:
# Create a list containing coordinates
coordinates = [
    (40.71, -74.00),
    (30.26, -97.74),
    (46.87, -96.78),
    (47.60, -122.33),
    (32.71, -117.16)
]
534/7:
# Customize the size of the figure
figure_layout = {
    'width': '400px',
    'height': '300px',
    'border': '1px solid black',
    'padding': '1px',
    'margin': '0 auto 0 auto'
}
fig = gmaps.figure(layout=figure_layout)
534/8:
# Assign the marker layer to a variable
markers = gmaps.marker_layer(coordinates)
# Add the layer to the map
fig.add_layer(markers)
fig
535/9:
# Store latitude and longitude in locations
Locations = airport_df[["Lat","Lng"]]
Locations.head()
# Filla NaN values and convert to float
locations_1 = airport_df["Airport Rating"].astype(float)
535/10:
# Plot Heatmap
fig = gmaps.figure(map_type='HYBRID')
heatmap_layer = gmaps.heatmap_layer(locations)
fig.add_layer(heatmap_layer)
fig
535/11:
# Plot Heatmap
fig = gmaps.figure(map_type='HYBRID')
heatmap_layer = gmaps.heatmap_layer(location_1)
fig.add_layer(heatmap_layer)
fig
536/1:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census

# Census API Key
from config import api_key
c = Census(api_key, year=2013)
536/2:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})
census_pd.head()
536/3:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census

# Census API Key
from config import api_key
c = Census(api_key, year=2013)
536/4:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})
census_pd.head()
536/5:
# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["Zipcode", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate"]]

# Visualize
print(len(census_pd))
census_pd.head()
536/6:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})
census_pd.head()
536/7:
# Save as a csv
# Note to avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)
537/1:
# Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from census import Census
import gmaps

# Census & gmaps API Keys
from config import (api_key, gkey)
c = Census(api_key, year=2013)

# Configure gmaps
gmaps.configure(api_key=gkey)
537/2:
# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
# See: https://github.com/CommerceDataService/census-wrapper for library documentation
# See: https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b for labels
census_data = c.acs5.get(("NAME", "B19013_001E", "B01003_001E", "B01002_001E",
                          "B19301_001E",
                          "B17001_002E"), {'for': 'zip code tabulation area:*'})

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column Reordering
census_pd = census_pd.rename(columns={"B01003_001E": "Population",
                                      "B01002_001E": "Median Age",
                                      "B19013_001E": "Household Income",
                                      "B19301_001E": "Per Capita Income",
                                      "B17001_002E": "Poverty Count",
                                      "NAME": "Name", "zip code tabulation area": "Zipcode"})

# Add in Poverty Rate (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * \
    census_pd["Poverty Count"].astype(
        int) / census_pd["Population"].astype(int)

# Final DataFrame
census_pd = census_pd[["Zipcode", "Population", "Median Age", "Household Income",
                       "Per Capita Income", "Poverty Count", "Poverty Rate"]]

# Visualize
print(len(census_pd))
census_pd.head()
537/3:
# Save as a csv
# Note to avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)
537/4: census_state = pd.read_csv("../Resources/state_census_data.csv")
537/5:
census_state = pd.read_csv("../Resources/state_census_data.csv")
census_state
536/8: help(census_df.tolist)
536/9: help(census_pd.tolist)
536/10: help(df.tolist)
536/11: help(DataFrame.tolist)
540/1:
import requests
import json
540/2: url = "https://api.spacexdata.com/v2/launchpads"
540/3: url
540/4: print(url)
540/5: type(url)
540/6: type(print(url))
540/7: requests.get(url)
540/8: response = requests.get(url)
540/9: response = requests.get(url)
540/10:
response = requests.get(url)
response
540/11:
response = requests.get(url)
type(response)
540/12:
i = requests.get(url)
type(i)
540/13: i.json()
540/14: type(i.json())
539/1:
# Pretty Print the output of the JSON
response = requests.get(url).json()
type(print(json.dumps(response, indent=4, sort_keys=True)))
539/2:
# Dependencies
import requests
import json
539/3:
# URL for GET requests to retrieve vehicle data
url = "https://api.spacexdata.com/v2/launchpads"
539/4:
# Print the response object to the console
print(requests.get(url))
539/5:
# Retrieving data and converting it into JSON
print(requests.get(url).json())
539/6:
# Pretty Print the output of the JSON
response = requests.get(url).json()
type(print(json.dumps(response, indent=4, sort_keys=True)))
539/7:
# Pretty Print the output of the JSON
response = requests.get(url).json()
print(json.dumps(response, indent=4, sort_keys=True))
540/15: json.dumps(1, sort_keys = true, indent = 4)
540/16: json.dumps(1, sort_keys = True, indent = 4)
540/17: json.dumps(i, sort_keys = True, indent = 4)
540/18: print(json.dumps(i, sort_keys = True, indent = 4))
540/19:
import requests
import json
540/20: url = "https://api.spacexdata.com/v2/launchpads"
540/21:
i = requests.get(url)
type(i)
540/22: type(i.json())
540/23: print(json.dumps(i, sort_keys = True, indent = 4))
540/24: i.json()
540/25: print(json.dumps(i, sort_keys = True, indent = 4))
540/26:
import requests
import json
540/27: url = "https://api.spacexdata.com/v2/launchpads"
540/28:
i = requests.get(url)
type(i)
540/29: i.json()
540/30: print(json.dumps(i, sort_keys = True, indent = 4))
540/31: print(i.json())
540/32: print(json.dumps(i, sort_keys = True, indent = 4))
540/33:
print(i.json())
print(json.dumps(i, sort_keys = True, indent = 4))
540/34:
i.json()
print(json.dumps(i, sort_keys = True, indent = 4))
540/35:
i.json()
print(json.dumps(i, sort_keys = True, indent = 4))
540/36:
i = requests.get(url)
i
540/37:
i.json()
print(json.dumps(i, sort_keys = True, indent = 4))
540/38:
i.json()
#print(json.dumps(i, sort_keys = True, indent = 4))
540/39:
i.json()
print(json.dumps(i, sort_keys=True,indent=4))
540/40:
i=requests.get(url).json()
#print(json.dumps(i, sort_keys=True,indent=4))
540/41:
i=requests.get(url).json()
print(json.dumps(i, sort_keys=True,indent=4))
540/42:
response = requests.get(url)
response
540/43:
response = requests.get(url)
response
540/44:
response.json()
print(json.dumps(response, sort_keys=True,indent=4))
540/45:
response = requests.get(url)
response
540/46:
response.json()
print(json.dumps(response, sort_keys=True,indent=4))
540/47:
response1 = response.json()
print(json.dumps(response1, sort_keys=True,indent=4))
540/48:
import requests
import json
540/49: url = https://api.spacexdata.com/v2/launchpads
540/50: url = "https://api.spacexdata.com/v2/launchpads"
540/51:
import requests
import json
540/52: url = "https://api.spacexdata.com/v2/launchpads"
540/53:
reaponse = requests.get(url).json()
response
540/54:
reaponse = requests.get(url).json()
print(response)
540/55:
reaponse = requests.get(url).json()
response
540/56: json.dumps(response,indent=4,sort_keys=True)
540/57: print(json.dumps(response,indent=4,sort_keys=True))
540/58: response = requests.get(url).json()
540/59: print(json.dumps(response,indent=4,sort_keys=True))
540/60: json.dumps(response,indent=4,sort_keys=True)
540/61: print(json.dumps(response,indent=4,sort_keys=True))
540/62: response1 = requests.get(url +"kwajalein_atoll").json()
540/63:
response1 = requests.get(url +"kwajalein_atoll").json()
response1
540/64:
response1 = requests.get(url +"ccafs_lc_13").json()
response1
540/65:
response1 = requests.get(url +"/kwajalein_atoll").json()
response1
540/66:
response1 = requests.get(url +"/kwajalein_atoll").json()
print(json.dumps(response1,indent=4,sort_keys=True)
540/67:
response1 = requests.get(url +"/kwajalein_atoll").json()
print(json.dumps(response1,indent=4,sort_keys=True)
540/68:
import requests
import json
540/69: url = "https://api.spacexdata.com/v2/launchpads"
540/70: response = requests.get(url).json()
540/71: print(json.dumps(response,indent=4,sort_keys=True))
540/72:
response1 = requests.get(url +"/kwajalein_atoll").json()
print(json.dumps(response1,indent=4,sort_keys=True)
540/73:
response1 = requests.get(url +"/kwajalein_atoll").json()
print(json.dumps(response1,indent=4,sort_keys=True)
540/74:
response1 = requests.get(url +"/kwajalein_atoll").json()
print(json.dumps(response1,indent=4,sort_keys=True))
543/1:
import requests
import json
543/2: url = "https://api.spacexdata.com/v2/rockets/falcon9"
543/3:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
url
543/4: response = requests.get(url).json()
543/5:
response = requests.get(url).json()
response
543/6:
response = requests.get(url).json()
print(json.dumps(response, indent=4,sort_keys=True)
543/7:
response = requests.get(url).json()
print(json.dumps(response, indent=4,sort_keys=True))
543/8: type(response)
543/9: response[cost_per_launch]
543/10: response["cost_per_launch"]
543/11: response["diameter"]["feet"]
543/12: response["engines"]["thrust_sea_level"]["kN"]
543/13: len(response["payload_weights"])
543/14: len(response["payload_weights"]["id"])
543/15: len(response["payload_weights"]["kg"])
543/16:
import requests
import json
543/17:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
url
543/18:
response = requests.get(url).json()
print(json.dumps(response, indent=4,sort_keys=True))
543/19: type(response)
543/20: response["engines"]["thrust_sea_level"]["kN"]
543/21: len(response["payload_weights"]["kg"])
543/22: (response["payload_weights"]["kg"])
544/1:
import requests
import json
544/2:
url = "https://api.spacexdata.com/v2/rockets/falcon9"
url
544/3:
response = requests.get(url).json()
print(json.dumps(response, indent=4,sort_keys=True))
544/4: type(response)
544/5: response["engines"]["thrust_sea_level"]["kN"]
544/6: (response["payload_weights"]["kg"])
544/7: response["payload_weights"]["kg"]
544/8: response["payload_weights"]["kg"]
544/9: response["payload_weights"]
544/10: response["payload_weights"][1][2]
544/11: response["payload_weights"]("ID")[1][2]
544/12: response["payload_weights"]("ID")[1]["kg"]
544/13: response["payload_weights"][1]["kg"]
544/14:
kg = response["payload_weights"][1]["kg"]
print("the weight is %f kg", kg)
544/15:
kg = response["payload_weights"][1]["kg"]
print("the weight is %%f kg", kg)
544/16:
kg = response["payload_weights"][1]["kg"]
print("the weight is {kg} kg")
544/17:
kg = response["payload_weights"][1]["kg"]
print(f"the weight is {kg} kg")
545/1:
# Dependencies
import requests
import json
545/2:
# URL for GET requests to retrieve Star Wars character data
base_url = "https://swapi.co/api/people/"
545/3:
# Create a url with a specific character id
character_id = '4'
url = base_url + character_id
print(url)
546/1:
import requests
import json
546/2: url = "https://swapi.co/api/people/"
546/3:
url = "https://swapi.co/api/people/"
url
546/4:
id= 4
final_url=url + id
response=requests.get(final_url).json()
546/5:
id= 4
final_url=url + "id"
response=requests.get(final_url).json()
546/6:
id= 4
final_url=url + "id"
response=requests.get(final_url).json()
response
546/7:
id= "4"
final_url=url + id
response=requests.get(final_url).json()
response
546/8: response["name"]
546/9:
films_url=https://swapi.co/api/films/
response=requests.get(films_url).json()
response
546/10:
films_url="https://swapi.co/api/films/""
response=requests.get(films_url).json()
response
546/11:
films_url="https://swapi.co/api/films/"
response=requests.get(films_url).json()
response
546/12:
films_url="https://swapi.co/api/films/4"
response=requests.get(films_url).json()
response
547/1:
import requests
import json
547/2:
url = "http://numbersapi.com"
url
547/3: input = print("Please input a number   ")
547/4: user_input = input("Please input a number   ")
547/5:
user_input = input("Please input a number   ")
user_input
547/6:
import requests
import json
547/7:
url = "http://numbersapi.com"
url
547/8:
user_input = input("Please input a number   ")
user_input
547/9: user_input = input("Please input a number   ")
547/10: user_input = input("Please input a number")
547/11: user_input = input ("Please input a number")
547/12:
user_input =("Please input a number")
type(user_input)
547/13:
user_input =("Please input a number")
type(user_input)
u_i = input(user_input)
547/14:
user =("Please input a number")
type(user_input)
u_i = input(user)
547/15:
import requests
import json
547/16:
url = "http://numbersapi.com"
url
547/17:
user =("Please input a number")
type(user_input)
u_i = input(user)
547/18:
user =("Please input a number    "[Trivia, Math, Number])
type(user_input)
u_i = input(user)
547/19:
import requests
import json
547/20:
url = "http://numbersapi.com"
url
547/21:
user =("Please input a number    "[Trivia, Math, Number])
type(user_input)
u_i = input(user)
547/22:
user =("Please input a number    ","[Trivia, Math, Number]")
type(user_input)
u_i = input(user)
547/23:
import requests
import json
547/24:
url = "http://numbersapi.com"
url
547/25:
user =("Please input a number    ","[Trivia, Math, Number]")
type(user_input)
u_i = input(user)
547/26:
user =("Please input a number    ")
#type(user_input)
u_i = input(user)
547/27:
user =("Please input a number    ")
#type(user_input)
input(user)
547/28:
user =("Please input a number    ")
user
547/29:
user =("Please input a number    ")
input(user)
547/30:
user =("Please input a number    ")
user1 = input(user)
547/31:
user =input ("Please input a number    ")
user1 = input(user)
547/32:
user =input ("Please input a number    ")
#user1 = input(user)
547/33:
user = input("Please input a number    ")
#user1 = input(user)
547/34:
user = ("Please input a number    ")
user1 = input(user)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/35:
user = ("Please input a number    ")
kind_of_search = input(user)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/36:
user = ("Please input a number?    ")
kind_of_search = input(user)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/37:
user = ("Please input a number""[Trivia, Math, Date, year])
kind_of_search = input(user)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/38:
user = ("Please input a number""[Trivia, Math, Date, year]")
kind_of_search = input(user)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/39:
question = ("Please input a number""[Trivia, Math, Date, year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/40:
import requests
import json
547/41:
url = "http://numbersapi.com"
url
547/42:
question = ("Please input a number""[Trivia, Math, Date, year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
548/1:
# Dependencies
import requests
import json
548/2:
# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com/"
548/3:
# Ask the user what kind of data they would like to search for
question = ("What type of data would you like to search for? "
            "[Trivia, Math, Date, or Year] ")
kind_of_search = input(question)
547/43:
# question = ("Please input a number""[Trivia, Math, Date, year]")
# kind_of_search = input(question)
question = ("What type of data would you like to search for? "
            "[Trivia, Math, Date, or Year] ")
kind_of_search = input(question)
547/44:
import requests
import json
547/45:
url = "http://numbersapi.com"
url
547/46:
# question = ("Please input a number""[Trivia, Math, Date, year]")
# kind_of_search = input(question)
question = ("What type of data would you like to search for? "
            "[Trivia, Math, Date, or Year] ")
kind_of_search = input(question)
547/47:
question = ("Please input a number"
                "[Trivia, Math, Date, year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/48:
import requests
import json
547/49:
url = "http://numbersapi.com"
url
547/50:
question = ("Please input a number"
                "[Trivia, Math, Date, year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/51:
question = ("Please input a number?"
                "[Trivia, Math, Date, year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/52:
question = ("Please input a number?"
                "[Trivia, Math, Date, or year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/53:
import requests
import json
547/54:
url = "http://numbersapi.com"
url
547/55:
question = ("Please input a number?"
                "[Trivia, Math, Date, or year]")
kind_of_search = input(question)
# question = ("What type of data would you like to search for? "
#             "[Trivia, Math, Date, or Year] ")
# kind_of_search = input(question)
547/56:
import requests
import json
547/57: url = "http://numbersapi.com/"
547/58:
url = "http://numbersapi.com/"
url
547/59:
question = ("What is yor choice? "["Trivia,Math,Date,Year"])
input(question)
547/60:
import requests
import json
547/61:
url = "http://numbersapi.com/"
url
547/62:
question = ("What is yor choice? "["Trivia,Math,Date,Year"])
input(question)
547/63:
question = ("What is yor choice? "["Trivia,Math,Date,Year"])
question1 = input(question)
547/64:
import requests
import json
547/65:
url = "http://numbersapi.com/"
url
547/66:
question = ("What is yor choice? "["Trivia,Math,Date,Year"])
question1 = input(question)
550/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
550/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
550/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
553/1:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
#print(f"The weather API responded with: {weather_json}.")
553/2:
# Dependencies
import json
import requests
from config import My_api_key
553/3:
# Save config information
url = "http://api.openweathermap.org/data/2.5/weather?"
city = "London"

# Build query URL
query_url = url + "appid=" + My_api_key + "&q=" + city
553/4:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()

# Get the temperature from the response
#print(f"The weather API responded with: {weather_json}.")
553/5:
# Get weather data
weather_response = requests.get(query_url)
weather_json = weather_response.json()
weather_json

# Get the temperature from the response
#print(f"The weather API responded with: {weather_json}.")
561/1:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
561/2:
import pandas as pd
import scipy.stats as stats
561/3:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
561/4: # Calculate the population mean for Sardine Vertebrae in Alaska
561/5:
import pandas as pd
import scipy.stats as stats
561/6:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
561/7:
# Calculate the population mean for Sardine Vertebrae in Alaska
df.mean()
561/8:
# Calculate the population mean for Sardine Vertebrae in Alaska
df["vertebrae].Smean()
561/9:
# Calculate the population mean for Sardine Vertebrae in Alaska
df["vertebrae"].Smean()
561/10:
# Calculate the population mean for Sardine Vertebrae in Alaska
df["vertebrae"].mean()
561/11:
# Calculate the population mean for Sardine Vertebrae in Alaska
df["vertebrae"][1].mean()
561/12:
# Calculate the population mean for Sardine Vertebrae in San Diego
df["vertebrae"][6].mean()
561/13:
df = pd.read_csv("../Resources/sardines.csv")
df
561/14:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
561/15:
# Calculate Independent (Two Sample) T-Test
def gendata(loc=0):
    sample1 = stats.norm.rvs(size=100, random_state=none)
    sample2 = stats.norm.rvs(loc=loc, size=200, random_state=none)
    plt.subplot(2, 1, 1)
    plt.scatter(range(len(sample1)), sample1, label="sample1")
    plt.scatter(range(len(sample2)), sample2, label="sample2")
    plt.legend()
561/16:
# Calculate Independent (Two Sample) T-Test
def gendata(loc=0):
    sample1 = stats.norm.rvs(size=100, random_state=none)
    sample2 = stats.norm.rvs(loc=loc, size=200, random_state=none)
    plt.subplot(2, 1, 1)
    plt.scatter(range(len(sample1)), sample1, label="sample1")
    plt.scatter(range(len(sample2)), sample2, label="sample2")
    plt.legend()
    return sample1, sample2
561/17:
import pandas as pd
import scipy.stats as stats
561/18:
import pandas as pd
import scipy.stats as stats
from matplotlib import pyplot as plt
561/19:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
561/20:
# Calculate the population mean for Sardine Vertebrae in Alaska
df["vertebrae"][1].mean()
561/21:
# Calculate the population mean for Sardine Vertebrae in San Diego
df["vertebrae"][6].mean()
561/22:
# Calculate Independent (Two Sample) T-Test
def gendata(loc=0):
    sample1 = stats.norm.rvs(size=100, random_state=none)
    sample2 = stats.norm.rvs(loc=loc, size=200, random_state=none)
    plt.subplot(2, 1, 1)
    plt.scatter(range(len(sample1)), sample1, label="sample1")
    plt.scatter(range(len(sample2)), sample2, label="sample2")
    plt.legend()
    return sample1, sample2
561/23:
df = pd.read_csv("../Resources/sardines.csv")
df
561/24:
df = pd.read_csv("../Resources/sardines.csv")
df.head()
562/1:
import warnings
warnings.filterwarnings('ignore')
562/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
562/3:
df = pd.read_csv("../Resources/mosquito.csv")
df.head()
562/4:
# Create a boxplot to compare means
df.boxplot("mosq", by="treatment", figsize=(20, 10))
562/5:
# Extract individual groups
group1 = df[df["treatment"] == 1]["mosq"]
group2 = df[df["treatment"] == 2]["mosq"]
group3 = df[df["treatment"] == 3]["mosq"]
group4 = df[df["treatment"] == 4]["mosq"]
group5 = df[df["treatment"] == 5]["mosq"]
562/6:
# Perform the ANOVA
stats.f_oneway(group1, group2, group3, group4, group5)
stats.f_oneway
562/7:
# Extract individual groups
group1 = df[df["treatment"] == 1]["mosq"]
group2 = df[df["treatment"] == 2]["mosq"]
group3 = df[df["treatment"] == 3]["mosq"]
group4 = df[df["treatment"] == 4]["mosq"]
group5 = df[df["treatment"] == 5]["mosq"]
group1.head()
562/8:
# Perform the ANOVA
stats.f_oneway(group1, group2, group3, group4, group5)
563/1:
import warnings
warnings.filterwarnings('ignore')
563/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
563/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
563/4:
group1 = df[df["Haircolor"]==0]["Pain"]
group1.head()
563/5:
group1 = df[df["LightBlond"]==0]["Pain"]
group1.head()
563/6:
group1 = df["LightBlond"]==0["Pain"]
group1.head()
563/7:
group1 = df[df["LightBlond"]]["Pain"]
group1.head()
563/8:
group1 = df[df["LightBlond"]]["Pain"]
group1.head()
563/9:
group1 = df["LightBlond"]["Pain"]
group1.head()
563/10:
group1 = df["LightBlond"]
group1.head()
563/11:
group1 = df["HairColour"]
group1.head()
563/12:
group1 = df["HairColour"]["Pain"]
group1.head()
563/13:
group1 = df["HairColour"]=="LightBlond"
group1.head()
563/14:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group1.head()
563/15:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group1 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group1 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group1 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
group1.head()
563/16:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group1 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group1 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group1 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
563/17: stats.f_oneway(group1,group2,group3,group4)
563/18:
import warnings
warnings.filterwarnings('ignore')
563/19:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
563/20:
df = pd.read_csv("../Resources/hair.csv")
df.head()
563/21:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group2 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group3 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group4 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
563/22: stats.f_oneway(group1,group2,group3,group4)
563/23:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(20, 10))
563/24:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(10, 105))
df.boxplot
563/25:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(10, 10))
df.boxplot
563/26: stats.f_oneway (group1,group2,group3,group4)
563/27:
import warnings
warnings.filterwarnings('ignore')
563/28:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
563/29:
df = pd.read_csv("../Resources/hair.csv")
df.head()
563/30:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group2 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group3 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group4 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
563/31: stats.f_oneway (group1,group2,group3,group4)
563/32:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(10, 10))
df.boxplot
563/33: stats.f_oneway (group1, group2, group3, group4)
563/34: stats.f_oneway(group1, group2, group3, group4)
564/1:
import warnings
warnings.filterwarnings('ignore')
564/2:
%matplotlib inline
import pandas as pd
import scipy.stats as stats
564/3:
df = pd.read_csv("../Resources/hair.csv")
df.head()
564/4:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group2 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group3 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group4 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
564/5: stats.f_oneway(group1, group2, group3, group4)
564/6:
# Create a boxplot to compare means
df.boxplot("Pain", by="HairColour", figsize=(10, 10))
df.boxplot
564/7:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group2 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group3 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group4 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
stats.f_oneway(group1, group2, group3, group4)
564/8:
group1 = df[df["HairColour"]=="LightBlond"]["Pain"]
group2 = df[df["HairColour"]=="DarlBlond"]["Pain"]
group3 = df[df["HairColour"]=="LightBrunette"]["Pain"]
group4 = df[df["HairColour"]=="DarkBrunette"]["Pain"]
stats.f_oneway(group1, group2, group3, group4)
565/1:
import numpy as np
import pandas as pd
565/2:
# The statistical module used to run chi square test
import scipy.stats as stats
565/3:
# Observed data in a (hypothetical) survey of 6000 people 
observed = pd.Series([1500, 1350, 1600, 1550], index=["1", "2", "3", "4"])
565/4:
# Create a data frame
df = pd.DataFrame([observed]).T
565/5:
# Create a data frame
df = pd.DataFrame([observed]).T
df
565/6:
# Add a column whose default values are the expected values
df[["Expected"]]
565/7: # Add a column whose default values are the expected values
565/8:
# Add a column whose default values are the expected values
df[[columns]]
565/9:
# Add a column whose default values are the expected values
df[[0]]
565/10:
# Add a column whose default values are the expected values
df[[0]]
df[1] = "Expected"
565/11:
# Add a column whose default values are the expected values
df[[0]]
df[1] = "Expected"
df
565/12:
# Add a column whose default values are the expected values
df[[0]]
df[Expected] = "1000"
df
565/13:
# Add a column whose default values are the expected values
df[[0]]
df[1] = "1000"
df
565/14:
# Rename the data frame columns
df[1] = "Expected"
565/15:
# Rename the data frame columns
df[1] = "Expected"
df
565/16:
# Rename the data frame columns
df.columns = [1, Expected]
df
565/17:
# Rename the data frame columns
df.rename =(columns="1:""Expected")
df.rename
565/18:
# Rename the data frame columns
df.rename =(columns="1":""Expected")
df.rename
565/19:
# Rename the data frame columns
df.rename =(columns=1:""Expected")
df.rename
565/20:
# Rename the data frame columns
df.rename =(columns=1:Expected")
df.rename
565/21:
# Rename the data frame columns
df.rename =(columns={1:"Expected"})
df.rename
565/22:
# Rename the data frame columns
df.rename =(columns={1:"Expected"})
565/23:
import numpy as np
import pandas as pd
565/24:
# The statistical module used to run chi square test
import scipy.stats as stats
565/25:
# Observed data in a (hypothetical) survey of 6000 people 
observed = pd.Series([1500, 1350, 1600, 1550], index=["1", "2", "3", "4"])
565/26:
# Create a data frame
df = pd.DataFrame([observed]).T
df
565/27:
# Add a column whose default values are the expected values
df[[0]]
df[1] = "1000"
df
565/28:
# Rename the data frame columns
df.rename =(columns={1:"Expected"})
#df.rename
565/29: # View the data frame
565/30:
# Rename the data frame columns
df.rename =(columns={1:"Expected"})
#df.rename
565/31:
# Rename the data frame columns
df.rename =(index=str,columns={1:"Expected"})
#df.rename
565/32:
# Rename the data frame columns
df.rename =(index=str,columns={1:"Expected"})
565/33:
# Rename the data frame columns
df.rename =(index=str,columns={0:"Observed",1:"Expected"})
565/34:
# Rename the data frame columns
df.rename =(index=str,columns={0:"Observed",1:"Expected"})
565/35:
# Rename the data frame columns
df =pd.DataFrame(index=str,columns={0:"Observed",1:"Expected"})
df.rename
565/36:
# Rename the data frame columns
df =pd.DataFrame(index=str,columns={0:"Observed",1:"Expected"})
#df.rename
567/1:
import numpy as np
import pandas as pd
567/2:
# The statistical module used to run chi square test
import scipy.stats as stats
567/3:
# Observed data in a (hypothetical) survey of 6000 people 
observed = pd.Series([1500, 1350, 1600, 1550], index=["1", "2", "3", "4"])
567/4:
# Create a data frame
df = pd.DataFrame([observed]).T
df
567/5:
# Add a column whose default values are the expected values
df[[0]]
df[1] = "1000"
df
567/6:
# Rename the data frame columns
df =pd.DataFrame(index=str,columns={0:"Observed",1:"Expected"})
#df.rename
568/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
568/4:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/5:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/6:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/7:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/8:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/9:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/10:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/11:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/12:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/13:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/14:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/15:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/16:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/17:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/18:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
571/1:
# Dependencies and Setup
%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import seaborn as sns
import time
from datetime import datetime as dt

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
   city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name

   # If the city is unique, then add it to a our cities list
   if city not in cities:
       cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
temp = []
Humid = []
Cloudy = []
Wind_Speed = []
Lat = []
Lng = []
foundcities = []
Date = []


for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(" finding weather for " +city)
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       temp.append(response['main']['temp_max'])
       Humid.append(response['main']['humidity'])
       Cloudy.append(response['clouds']['all'])
       Wind_Speed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print("City " + city + " not found")
#Build a scatter plot for each data type
plt.scatter([Lat],[temp], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)

# # Save the figure
plt.savefig("TemperatureInWorldCities.png")

# Show plot
plt.show()
# Build a scatter plot for each data type
plt.scatter([Lat],[Humid], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))

# # Save the figure
plt.savefig("HumidityInWorldCities.png")

# Show plot
plt.show()
# Build a scatter plot for each data type
plt.scatter([Lat],[Cloudy], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))

# # Save the figure
plt.savefig("CloudinessInWorldCities.png")

# Show plot
plt.show()
# Build a scatter plot for each data type
plt.scatter([Lat],[Wind_Speed], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)

# # Save the figure
plt.savefig("WindSpeedInWorldCities.png")

# Show plot
plt.show()
568/19:
# Build a scatter plot for each data type
plt.scatter([Lat],[Cloudy], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))

# # Save the figure
plt.savefig("CloudinessInWorldCities.png")

# Show plot
plt.show()
568/20:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print("City " + city + " not found")
568/21:
# Build a scatter plot for each data type
plt.scatter([Lat],[Humid], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))

#Save the figure
#plt.savefig("HumidityInWorldCities.png")

# Show plot
plt.show()
568/22:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue')

plt.title("Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)

# # Save the figure
plt.savefig("TemperatureInWorldCities.png")

# Show plot
plt.show()
568/23:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))

#Save the figure
#plt.savefig("HumidityInWorldCities.png")

# Show plot
plt.show()
568/24:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue')

# # Incorporate the other graph properties
plt.title("Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)

# # Save the figure
plt.savefig("WindSpeedInWorldCities.png")

# Show plot
plt.show()
568/25:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
568/26:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
568/27:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
568/28:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
568/29:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue')

plt.title("Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)

Save the figure
plt.savefig("TemperatureInWorldCities.png")

# Show plot
plt.show()
568/30:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue')

plt.title("Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)
plt.savefig("TemperatureInWorldCities.png")

plt.show()
572/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
572/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
572/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
572/4:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
572/5:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue')

plt.title("Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)
plt.savefig("TemperatureInWorldCities.png")

plt.show()
572/6:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue')

plt.title("Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
#plt.savefig("HumidityInWorldCities.png")

plt.show()
572/7:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue')

plt.title("Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("CloudinessInWorldCities.png")

plt.show()
572/8:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue')

plt.title("Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)
plt.savefig("WindspeedInWorldCities.png")

plt.show()
573/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
573/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
573/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
573/4:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
573/5:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue')

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/6:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue')

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
573/7:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue')

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
573/8:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue')

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
573/9:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True)
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/10:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
573/11:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
573/12:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
573/13:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/14:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = 'black'
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/15:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = 'black'
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/16:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = 'lightblue'
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/17:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
""
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/18:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['figure.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/19:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['figure.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/20:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/21:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/22:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/23:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
573/24:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
574/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
574/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
574/4:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
574/5:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/6:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/7:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/8:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True)
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/9:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w")
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/10:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/11:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.5)
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/12:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lightblue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/13:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/14:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/15:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/16:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "blue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/17:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "blue"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/18:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "silver"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/19:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "silver"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/20:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/21:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/22:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'azure', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/23:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'navy', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/24:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/25:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.yticks(np.arange(-20, 120, 20))
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/26:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/27:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/28:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/29:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
574/30:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
574/31:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key 

url
574/32:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
foundcities = []
#Date = []

for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
574/33:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-40, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/34:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/35:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/36:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/37:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/38:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/39:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 100, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/40:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
574/41:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 50))
plt.xticks(np.arange(-80, 100, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/42:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 100, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/43:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
574/44:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 50))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/45:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
574/46:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 40, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/47:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
574/48:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
574/49:
Random_Cities = pd.DataFrame({"City":["cities"], "Max Temperature":["Temperature"],
                              "Humidity":[Humidity], "Cloudiness":["Cloudiness"],"Date":["Date"],
                              "Lat":["Lat"],"Lng":["Lon"],"Country":["Country"]})
random_Cities.head()
574/50:
Random_Cities = pd.DataFrame({"City":["cities"], "Max Temperature":["Temperature"],
                              "Humidity":[Humidity], "Cloudiness":["Cloudiness"],"Date":["Date"],
                              "Lat":["Lat"],"Lng":["Lon"],"Country":["Country"]})
Random_Cities.head()
574/51:
Random_Cities = pd.DataFrame({"City":["cities"], "Max Temperature":["Temperature"],
                              "Humidity":["Humidity"], "Cloudiness":["Cloudiness"],"Date":["Date"],
                              "Lat":["Lat"],"Lng":["Lon"],"Country":["Country"]})
Random_Cities.head()
574/52:
Random_Cities = pd.DataFrame({"City":["foundcities"], "Max Temperature":["Temperature"],
                              "Humidity":["Humidity"], "Cloudiness":["Cloudiness"],"Date":["Date"],
                              "Lat":["Lat"],"Lng":["Lon"],"Country":["Country"]})
Random_Cities.head()
574/53:
Random_Cities = pd.DataFrame({"City":[foundcities], "Max Temperature":[Temperature],
                              "Humidity":[Humidity], "Cloudiness":[Cloudiness],
                              "Lat":[Lat],"Lng":[Lon]})
Random_Cities.head()
574/54:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []



for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
576/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
576/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
576/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
576/4:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []



for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
       response = requests.get(url, params=params).json()
       Lat.append(response['coord']['lat'])
       Temperature.append(response['main']['temp_max'])
       Humidity.append(response['main']['humidity'])
       Cloudiness.append(response['clouds']['all'])
       Windspeed.append(response['wind']['speed'])
       foundcities.append(response['name'])
       print("found")
   except KeyError:
       print(f"I did not find {city}")
576/5:
Random_Cities = pd.DataFrame({"City":[foundcities], "Max Temperature":[Temperature],
                              "Humidity":[Humidity], "Cloudiness":[Cloudiness],
                              "Lat":[Lat],"Lng":[Lon]})
Random_Cities.head()
576/6:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
576/7:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
576/8:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
576/9:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
576/10:
Random_Cities = pd.DataFrame({"City":[foundcities], "Max Temperature":[Temperature],
                              "Humidity":[Humidity], "Cloudiness":[Cloudiness],
                              "Lat":[Lat],"Lng":[Lon]})
Random_Cities.head()
576/11:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":[Temperature],
                              "Humidity":[Humidity], "Cloudiness":[Cloudiness],
                              "Lat":[Lat],"Lng":[Lon]})
Random_Cities.head()
576/12:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/13:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []



for city in cities:
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
    response = requests.get(url, params=params).json()
    Lat.append(response['coord']['lat'])
    Lon.append(response["coord"]["lon"])
    Temperature.append(response['main']['temp_max'])
    Humidity.append(response['main']['humidity'])
    Cloudiness.append(response['clouds']['all'])
    Windspeed.append(response['wind']['speed'])
    foundcities.append(response['name'])
    print("found")
   except KeyError:
    print(f"I did not find {city}")
576/14:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/15:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/16:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/17:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities
576/18:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []



for city in cities:
    final_url = url + Units + APPID
   params = {
       'appid' : api_key,
       'q' : city,
       'units' : 'imperal'
   }
   print(f"I found {city}")
   try:
    response = requests.get(url, params=params).json()
    Lat.append(response['coord']['lat'])
    Lon.append(response["coord"]["lon"])
    Temperature.append(response['main']['temp_max'])
    Humidity.append(response['main']['humidity'])
    Cloudiness.append(response['clouds']['all'])
    Windspeed.append(response['wind']['speed'])
    foundcities.append(response['name'])
    print("found")
   except KeyError:
    print(f"I did not find {city}")
576/19:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"I found {city}")
    try:
    response = requests.get(url, params=params).json()
    Lat.append(response['coord']['lat'])
    Lon.append(response["coord"]["lon"])
    Temperature.append(response['main']['temp_max'])
    Humidity.append(response['main']['humidity'])
    Cloudiness.append(response['clouds']['all'])
    Windspeed.append(response['wind']['speed'])
    foundcities.append(response['name'])
    print("found")
   except KeyError:
    print(f"I did not find {city}")
576/20:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"I found {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/21:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"I found {city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/22:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print("Beginning Data Retrieval")
    print(--------------------------------------)
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/23:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print("Beginning Data Retrieval")
    print("--------------------------------------")
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/24:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

print("Beginning Data Retrieval")
print("--------------------------------------")
for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
   
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/25:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

print("Beginning Data Retrieval")
print("--------------------------------------")
for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
   
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/26:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

print("Beginning Data Retrieval")
print("--------------------------------------")
for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
   
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/27:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

print("Beginning Data Retrieval")
print("--------------------------------------")
for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
   
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/28:
#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
print("Beginning Data Retrieval")
print("--------------------------------------")
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
   
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/29:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/30:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities
576/31:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/32:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
576/33:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
576/34:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
576/35:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{city}{url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/36:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities
576/37:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
576/38:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
576/39:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
576/40:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
576/41:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities
576/42:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/43:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/44:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
576/45:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}{city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
576/46:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}{city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/47:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"q=" {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/48:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
576/49:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
576/50:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"q=" {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/51:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"&q=" {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/52:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"&q=" {city})
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/53:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"&q=" +{city})
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/54:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url}+"&q=" +{city})
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/55:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url} + "&q=" + {city})
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/56:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url} + "&q=" + {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/57:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    print(f"{url} + '&q=' + {city}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/58:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q=' + [city]
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/59:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q=' + str([city])
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/60:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q=' + str(city)
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/61:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q=' + str(city)
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/62:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/63:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str([city])
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
576/64:
# print("Beginning Data Retrieval")
# print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str([city])
    print(f"{final_url}")
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
579/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
579/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
579/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
579/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
580/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
580/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
580/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
580/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print("found")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
581/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
581/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
581/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
581/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
581/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
581/6:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
581/7:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
581/8:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
581/9:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
582/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
582/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
582/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
582/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
582/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
582/6:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
582/7:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
582/8:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
582/9:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
583/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
583/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
583/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=" + api_key
583/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name']
        Date.append(response['dt'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
583/5:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []
Date = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        Date.append(response['dt'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
583/6:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
583/7:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
583/8:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
583/9:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID=api_key"
583/10:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
584/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
584/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
584/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID="+ api_key
584/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lng"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
584/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
584/6:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
585/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
585/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
585/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID="+ api_key
585/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
585/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
585/6:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
585/7:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
585/8:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
585/9:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
585/10:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
585/11:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
random_Cities.value_counts
585/12:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.value_counts
585/13:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.value_counts()
585/14:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.value_counts()
586/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
586/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
586/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID="+ api_key
586/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
586/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.value_counts()
586/6:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
586/7:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
586/8:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity ")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
586/9:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
586/10:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness " )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
586/11:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed ")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
586/12:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
586/13: Random_Cities.to_csv('Random_Cities_output.csv')
586/14:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature ")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
586/15:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature (08/26/2018)")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
586/16:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity (08/26/2018)")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
586/17:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness (08/26/2018)" )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.3)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
586/18:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed (08/26/2018)")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
587/1:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature (08/26/2018)")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
587/2:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
587/3:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
587/4:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID="+ api_key
587/5:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature (08/26/2018)")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
587/6:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed (08/26/2018)")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
587/7:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed (08/26/2018)")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
587/8:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness (08/26/2018)" )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.4)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
587/9:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed (08/26/2018)")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
588/1:
# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time

# Import API key
import api_keys

# Incorporated citipy to determine city based on latitude and longitude
from citipy import citipy

# Output File (CSV)
output_data_file = "output_data/cities.csv"

# Range of latitudes and longitudes
lat_range = (-90, 90)
lng_range = (-180, 180)
588/2:
# List for holding lat_lngs and cities
lat_lngs = []
cities = []

# Create a set of random lat and lng combinations
lats = np.random.uniform(low=-90.000, high=90.000, size=1500)
lngs = np.random.uniform(low=-180.000, high=180.000, size=1500)
lat_lngs = zip(lats, lngs)

# Identify nearest city for each lat, lng combination
for lat_lng in lat_lngs:
    city = citipy.nearest_city(lat_lng[0], lat_lng[1]).city_name
    
    # If the city is unique, then add it to a our cities list
    if city not in cities:
        cities.append(city)

# Print the city count to confirm sufficient count
len(cities)
588/3:
# OpenWeatherMap API Key
api_key = api_keys.api_key

# Starting URL for Weather Map API Call
url = "http://api.openweathermap.org/data/2.5/weather?units=Imperial&APPID="+ api_key
588/4:
print("Beginning Data Retrieval")
print("--------------------------------------")

#Create empty lists for Temperature, Humidity, Cloudiness, Windspeed and Latitude 
#since we need to find the correlation between each with latitude
Temperature = []
Humidity = []
Cloudiness = []
Windspeed = []
Lat = []
Lon = []
foundcities = []

for city in cities:
    params = {'appid' : api_key,
               'q' : city,
               'units' : 'imperial'
            }
    final_url = url + '&q='+str(city)
    print(final_url)
    try:
        response = requests.get(final_url, params=params).json()
        Lat.append(response['coord']['lat'])
        Lon.append(response["coord"]["lon"])
        Temperature.append(response['main']['temp_max'])
        Humidity.append(response['main']['humidity'])
        Cloudiness.append(response['clouds']['all'])
        Windspeed.append(response['wind']['speed'])
        foundcities.append(response['name'])
        print(f"found {city}")
    except KeyError:
        print(f"I did not find {city}")
    continue
print("--------------------------------------")
print("Data Retrieval Complete")
print("--------------------------------------")
588/5:
Random_Cities = pd.DataFrame({"City":foundcities, "Max Temperature":Temperature,
                              "Humidity":Humidity, "Cloudiness":Cloudiness,"Windspeed":Windspeed,
                              "Lat":Lat,"Lng":Lon})
Random_Cities.head()
588/6: Random_Cities.to_csv('Random_Cities_output.csv')
588/7:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature (08/26/2018)")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
588/8:
#Scatter plot for Humidity (F) vs. Latitude
plt.scatter([Lat],[Humidity], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Humidity (08/26/2018)")
plt.ylabel("Humidity(%)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.3)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesHumidity.png")

plt.show()
588/9:
#Scatter plot for Cloudiness (F) vs. Latitude
plt.scatter([Lat],[Cloudiness], marker= "o", color = 'blue',edgecolors = "black")

plt.title("City Latitude vs Cloudiness (08/26/2018)" )
plt.ylabel("Cloudiness (%)")
plt.xlabel("Latitude")
plt.grid(True, color ="w", alpha = 0.4)
plt.yticks(np.arange(-20, 120, 20))
plt.xticks(np.arange(-80, 120, 20))
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesCloudiness.png")

plt.show()
588/10:
#Scatter plot for Windspeed (F) vs. Latitude
plt.scatter([Lat],[Windspeed], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Wind Speed (08/26/2018)")
plt.ylabel("Wind Speed (mph)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-5, 45, 5))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesWindSpeed.png")

plt.show()
588/11:
#Scatter plot for Temperature (F) vs. Latitude

plt.scatter([Lat],[Temperature], marker="o", color = 'blue', edgecolors = "black")

plt.title("City Latitude vs Max Temperature (08/26/2018)")
plt.ylabel("Max Temperature (F)")
plt.xlabel("Latitude")
plt.yticks(np.arange(-100, 150, 50))
plt.xticks(np.arange(-80, 120, 20))
plt.grid(True, color ="w", alpha = 0.4)
plt.rcParams['axes.facecolor'] = "lavender"
plt.savefig("RandomWorldCitiesTemperature.png")

plt.show()
590/1:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional" +title)
590/2:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " +title)
590/3: print("He has been coding for " +years+"years")
590/4: print("He has been coding for " + str(years) +"years")
590/5: print("He has been coding for " + str(years) +" years")
590/6: print("Expert Status: " expert_status)
590/7: print("Expert Status: " str(expert_status))
590/8: print("Expert Status: " + expert_status)
590/9: print("Expert Status: " + str(expert_status))
590/10:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "who is " + satisfied)
590/11:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "who is " + (strsatisfied)
590/12:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "who is " + str(satisfied)
590/13:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " +title)
590/14: print("He has been coding for " + str(years) +" years")
590/15: print("Expert Status: " + str(expert_status))
590/16:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "who is " + str(satisfied)
590/17:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "who is " + str(satisfied))
590/18:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + " from " + country + "who is " + str(satisfied))
590/19:
name = "shubha"
country = "India"
satisfied = True
hourly_wage = 40
daily_wage = hourly_wage * 8
print("I am " + name + " from " + country + " who is " + str(satisfied))
590/20: print("I get paid " + daily_wage + "Dollars")
590/21: print("I get paid " + str(daily_wage) + "Dollars")
590/22: print("I get paid " + str(daily_wage) + " Dollars")
590/23: print("I get paid " + str(daily_wage) + " Dollars a day")
590/24: name = input("What is your name")
590/25:
name = input("What is your name?  ")
age = input("How old are you?  ")
590/26:
name = input("What is your name?  ")
age = int(input("How old are you?  "))
590/27:
name = input("What is your name?  ")
age = int(input("How old are you?  "))
590/28: truorfal = bool(input("Is he input worthy?"))
590/29: truorfal = bool(input("Is the input worthy?"))
590/30: truorfal = bool(input("Is the input worthy?"))
590/31: truorfal = bool(input("Is the input worthy?"))
590/32:
name = input("What is your name?  ")
age = int(input("How old are you?  "))
truorfal = bool(input("Is the input worthy?"))
590/33:
name = input("What is your name?  ")
age = int(input("How old are you?  "))
truorfal = bool(input("Is the input worthy?"))
590/34:
print("My name is" + name)
print("I am " + str(age) +" years old now. But will be " + str(age +1) +" next year")
print("The input was converted to " + str(truorfal))
590/35:
firstname = input("What is your first name? ")
Neigh_firstname = input("What is your neighbor's first name? ")
firstname_code = input("how many months have you been coding? ")
neigh_firstname_code = input("How many months has been your neighbor coding? ")
print(firstname)
print(Neigh_firstname)
print(firstname_code)
print()
590/36:
firstname = input("What is your first name? ")
Neigh_firstname = input("What is your neighbor's first name? ")
firstname_code = input("how many months have you been coding? ")
neigh_firstname_code = input("How many months has been your neighbor coding? ")
print(firstname)
print(Neigh_firstname)
print(firstname_code)
print(neigh_firstname_code)
592/1:
title = "Frankfurter"
years = 80
expert_status = True
#print("Nick is a professional ")
592/2:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + years + "years")
print("Expert_status" + expert_status)
592/3:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + str(years) + "years")
print("Expert_status" + expert_status)
592/4:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + str(years) + "years")
print("Expert_status" + str(expert_status))
592/5:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + str(years) + " years")
print("Expert_status " + str(expert_status))
592/6:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + str(years) + " years")
print("Expert_status " + "="+ str(expert_status))
592/7:
title = "Frankfurter"
years = 80
expert_status = True
print("Nick is a professional " + title)
print("He has been coding for " + str(years) + " years")
print("Expert_status " + "= "+ str(expert_status))
592/8:
name = "Shubha"
country = "India"
age = 38
hourly_wage = 100
satisfied = True
daily_wage = hourly_wage * 8
print("I am " + name + "from " + country + "and I am " + str(age) + "years old.")
592/9:
name = "Shubha"
country = "India"
age = 38
hourly_wage = 100
satisfied = True
daily_wage = hourly_wage * 8
print("I am " + name + " from " + country + " and I am " + str(age) + " years old.")
592/10: print("I get paid " + str(daily_wage) + " dollars" + "and I am satisfied with it, which is really " + str(satisfied))
592/11: print("I get paid " + str(daily_wage) + " dollars" + " and I am satisfied with it, which is really " + str(satisfied))
592/12:
name = input("What is your name? ")
age = input("How old are you? ")
truorfal = input("Is the info true? ")
592/13:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = input("Is the info true? ")
592/14:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? "))
592/15:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? " Y/N))
592/16:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? Y/N"))
592/17:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? Y/N"))
592/18:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? Y/N"))
592/19:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? "))
592/20:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? "))
print("Mera naam " + name + "hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
592/21:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? "))
print("Mera naam " + name + "hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
592/22:
name = input("What is your name? ")
age = int(input("How old are you? "))
truorfal = bool(input("Is the info true? "))
print("Mera naam " + name + "hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
593/1:
name = input("What is your name? ")
age = int(input("How old are you? "))
trueOrFalse = bool(input("Is the info true? "))
print("Mera naam " + name + "hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
593/2:
name = input("What is your name? ")
age = int(input("How old are you? "))
trueOrFalse = bool(input("Is the info true? "))
print("Mera naam " + name + " hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
593/3:
name = input("What is your name? ")
age = int(input("How old are you? "))
trueOrFalse = bool(input("Is the info true? "))
print("Mera naam " + name + " hai")
print("I will be " + str(age + 1) + " next year.")
print("The input was converted to " + str(trueOrFalse))
593/4:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = input("How many months have you been coding? ")
neigh_code_time = input("How many months has your neighbor been coding? ")
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + "and she has been coding for " + str(neigh_code_time) + " months." + "So we have been coding for " 
     + str(code_time + neigh_code_time) + " months.")
593/5:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = input("How many months have you been coding? ")
neigh_code_time = input("How many months has your neighbor been coding? ")
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + "So we have been coding for " 
     + str(code_time + neigh_code_time) + " months.")
594/1:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = input("How many months have you been coding? ")
neigh_code_time = input("How many months has your neighbor been coding? ")
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + "So we have been coding for " 
     + (str(code_time) + str(neigh_code_time)) + " months.")
594/2:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = int(input("How many months have you been coding? "))
neigh_code_time = int(input("How many months has your neighbor been coding? "))
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + "So we have been coding for " 
     + (str(code_time) + str(neigh_code_time)) + " months.")
594/3:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = int(input("How many months have you been coding? "))
neigh_code_time = int(input("How many months has your neighbor been coding? "))
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + "So we have been coding for " 
     + str(code_time + neigh_code_time) + " months.")
594/4:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = int(input("How many months have you been coding? "))
neigh_code_time = int(input("How many months has your neighbor been coding? "))
print("I am " + first_name + "and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + " So we have been coding for " 
     + str(code_time + neigh_code_time) + " months.")
594/5:
first_name = input("what is your first name? ")
neigh_first_name = input("What is your neighbor's first name? ")
code_time = int(input("How many months have you been coding? "))
neigh_code_time = int(input("How many months has your neighbor been coding? "))
print("I am " + first_name + " and I have been coding for " + str(code_time) + " months." + " My neighbor is " 
     + neigh_first_name + " and she has been coding for " + str(neigh_code_time) + " months." + " So we have been coding for " 
     + str(code_time + neigh_code_time) + " months.")
594/6:
x = 10
y = 15

if(x=1):
    print("x is equal to 1")
594/7:
x = 10
y = 15

if(x==1):
    print("x is equal to 1")
594/8:
x = 10
y = 15

if(x==10):
    print("x is equal to 1")
594/9:
x = 10
y = 15

if(x==y):
    print("")
594/10:
x = 10
y = 15

if(x==y):
    print("True")
594/11:
x = 10
y = 15

if(x< y):
    print("True")
594/12:
x = 10
y = 15

if(x>y):
    print("True")
594/13:
x = 10
y = 15

if(x>y):
    print("True")
else:
    print("False")
594/14:
x = 10
y = 15

if(x>y):
    print("True")
else:
    print("False, x<y")
594/15:
x = 10
y = 15

if(x==10 and y<=15):
    print("True")
else:
    print("False, x<y")
594/16:
x = 10
y = 15

if(x>10 and y<=15):
    print("True")
else:
    print("False")
594/17:
x = 10
y = 15

if(x>=y and y<=15):
    print("True")
else:
    print("False")
594/18:
x = 10
y = 15

if(x>=y and y<=15):
    print("True")
else:
    print("False")
594/19:
x = 10
y = 15

if(x<=y and y<=15):
    print("True")
else:
    print("False")
594/20:
x = 10
y = 15

if(x<=y and y<=15):
    print("True")
    if (True)
        print("success")
    else:
        print("Failure")
594/21:
x = 10
y = 15

if(x<=y):
    if (y<=15 and y>x)
        print("success")
    else:
        print("Failure")
594/22:
x = 10
y = 15

if(x<=y):
    if (y<=15 and y>x):
        print("success")
    else:
        print("Failure")
594/23:
x = 10
y = 15

if(x<=y):
    if (y<=15 or x>y):
        print("success")
    else:
        print("Failure")
594/24:
x = 10
y = 15

if(x<=y):
    if (y>=15 or x>y):
        print("success")
    else:
        print("Failure")
594/25:
x = 10
y = 15

if(x<=y):
    if (y>15 or x>y):
        print("success")
    else:
        print("Failure")
594/26:
x = 5
y = 10
if (2 * x > 10):
    print("Question 1 works!")
else:
    print("oooo needs some work")
594/27:
# 2.
x = 5
y = 10
if (len("Dog") < x):
    print("Question 2 works!")
else:
    print("Still missing out")
594/28:
x = 2
y = 5
if ((x**3 >= y) and (y**2 < 26)):
    print("GOT QUESTION 3!")
else:
    print("Oh good you can count")
594/29:
name = "Dan"
group_one = ["Greg", "Tony", "Susan"]
group_two = ["Gerald", "Paul", "Ryder"]
group_three = ["Carla", "Dan", "Jefferson"]

if (name in group_one):
    print(name + " is in the first group")
elif (name in group_two):
    print(name + " is in group two")
elif (name in group_three):
    print(name + " is in group three")
else:
    print(name + " does not have a group")
594/30:
height = 66
age = 16
adult_permission = True

if ((height > 70) and (age >= 18)):
    print("Can ride all the roller coasters")
elif ((height > 65) and (age >= 18)):
    print("Can ride moderate roller coasters")
elif ((height > 60) and (age >= 18)):
    print("Can ride light roller coasters")
elif (((height > 50) and (age >= 18)) or ((adult_permission) and (height > 50))):
    print("Can ride bumper cars")
else:
    print("Stick to lazy river")
594/31:
hosa_list = ["Ikshu",8,"Cheeku",2]
print hosa_list
594/32:
hosa_list = ["Ikshu",8,"Cheeku",2]
print (hosa_list)
594/33:
hosa_list.append("Prashanth")
print(hosa_list)
594/34:
hosa_list.append("Prashanth", 42)
print(hosa_list)
594/35:
hosa_list.append("Prashanth")
hosa_list.append(42)
print(hosa_list)
594/36:
hosa_list.append("Prashanth")
hosa_list.append(42)
594/37:
hosa_list.append("Prashanth")
hosa_list.append(42)
print(hosa_list)
594/38:
# hosa_list.append("Prashanth")
# hosa_list.append(42)
print(hosa_list)
594/39:
hosa_list = ["Ikshu",8,"Cheeku",2]
print (hosa_list)
594/40:
# hosa_list.append("Prashanth")
# hosa_list.append(42)
print(hosa_list)
594/41:
hosa_list.append("Prashanth")
hosa_list.append(42)
print(hosa_list)
594/42: print(hosa_list.index("Prashanth"))
594/43: print(hosa_list.index("43"))
594/44: print(hosa_list.index(42))
594/45:
hosa_list[2] = "Ishya"
print(hosa_list)
594/46: print(len(hosa_list))
594/47:
hosa_list.remove("Prashanth")
print(hosa_list)
594/48:
hosa_list.pop(0)
print(hosa_list)
594/49:
hosa_list.pop(1)
print(hosa_list)
594/50:
myTuple = ('Python', 100, 'VBA', False)
print(myTuple)
594/51:
myTuple.append("pandas")
print(myTuple)
594/52:
myTuple.pop(1)
print(myTuple)
594/53:

print(myTuple)
594/54: import random
594/55: print("Lets play rock, paper and scissors!!")
594/56:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input(Make your choice: (r)ock, (p)aper, (s)cissors)
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/57:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/58:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/59:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/60:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/61:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/62:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/63:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/64:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/65:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/66:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/67:
options = ["r","p","s"]
computer_choice = random.choice(options)
user_choice = input("Make your choice: (r)ock, (p)aper, (s)cissors")
if(computer_choice == user_choice):
    print("You both are tied!")
elif(user_choice != computer_choice):
    if(user_choice=="r" and computer_choice=="p"):
        print("computer wins")
    elif(user_choice=="p" and computer_choice=="s"):
        print("computer wins")
    elif(user_choice=="s" and computer_choice=="r"):
        print("computer wins")
else:
    print("USER, YOU WIN!!!!!!!!!!!")
594/68:
for x in range(3):
    print(x)
594/69:
for x in range(0,8)
print(x)
594/70:
for x in range(0,8):
    print(x)
594/71:
for x in range(0,8,2):
    print(x)
594/72:
x = "shubha"
for letters in x:
    print(letters)
594/73:
x = "shubha_prashanth"
for letters in x:
    print(letters)
594/74:
x = "shubha_prashanth_123"
for letters in x:
    print(letters)
594/75:
x = "shubha_prashanth  _  123"
for letters in x:
    print(letters)
594/76:
x = " "
for letters in x:
    print(letters)
594/77:
x = ["a","b"]
for letters in x:
    print(letters)
594/78:
x = ["a","b"]
for x in x:
    print(x)
594/79:
x = ["a","b", 123]
for x in x:
    print(x)
594/80:
x = ["a","b", 123]
for x in x:
    print(len(x)
594/81:
x = ["a","b", 123]
for x in x:
    print(len(x))
594/82:
x = ["a","b"]
for x in x:
    print(len(x))
594/83:
x = ["a","b"]
for letters in x:
    print(len(x))
594/84:
x = ["a","b"]
for letters in x:
print(len(x))
594/85:
x = ["a","b"]
#for letters in x:
print(len(x))
594/86:
x = ["a","b"]
#for letters in x:
print(len(letters))
594/87:
letters = ["a","b"]
#for letters in x:
print(len(letters))
594/88:
x = ["a","b"]
for letters in x:
print(len(letters))
594/89:
x = ["a","b"]
for letters in x:
    print(len(letters))
594/90:
x = ["a","b"]
for letters in x:
    print(letters)
594/91: print(len(letters))
594/92:
zoo = ["cow", "dog", "bee", "zebra"]
for animal in zoo:
    print(animal)
594/93: print(len(animal))
594/94:
run = "y"

while run == "y":
    print("Hi!")
    run = input("To run again. Enter 'y'")
594/95:
run = "y"

while run == "y":
    print("Hi!")
    run = input("To run again. Enter 'y'")
594/96:
number = 10
while number == 10:
    number = int(input("How many numbers? ")
594/97:
number = 10
while number == 10:
    number = int(input("How many numbers? "))
594/98:
number = 10
while number == 10:
    number = int(input("How many numbers? "))
594/99:
number = 10
while number == 10:
    number = int(input("How many numbers? "))
595/1: number = int(input("How many numbers? "))
595/2:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
595/3:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
user = input("Would you like to continue? Y/N  ")
595/4:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
user = input("Would you like to continue? y/n  ")
if (user == "y"):
    number = int(input("How many numbers? "))
    for x in range(int(number)):
    print(x)
user = input("Would you like to continue? y/n  ")
else:
    print("sorry,see you again")
595/5:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
user = input("Would you like to continue? y/n  ")
if (user == "y"):
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
user = input("Would you like to continue? y/n  ")
else:
    print("sorry,see you again")
595/6:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
user = input("Would you like to continue? y/n  ")
    if (user == "y"):
        number = int(input("How many numbers? "))
        for x in range(int(number)):
            print(x)
user = input("Would you like to continue? y/n  ")
    else:
        print("sorry,see you again")
595/7:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
    user = input("Would you like to continue? y/n  ")
    if (user == "y"):
        number = int(input("How many numbers? "))
        for x in range(int(number)):
            print(x)
    user = input("Would you like to continue? y/n  ")
    else:
        print("sorry,see you again")
595/8:
number = int(input("How many numbers? "))
for x in range(int(number)):
    print(x)
    user = input("Would you like to continue? y/n  ")
    if (user == "y"):
        number = int(input("How many numbers? "))
        for x in range(int(number)):
            print(x)
            user = input("Would you like to continue? y/n  ")
    else:
        print("sorry,see you again")
596/1:
user_reply == "y"
while user_reply == "y"
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
user = input("Would you like to continue? y/n  ")
596/2:
user_reply == "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
user = input("Would you like to continue? y/n  ")
596/3:
user_reply == "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
    user = input("Would you like to continue? y/n  ")
596/4:
user_reply == "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
    user = input("Would you like to continue? y/n  ")
596/5:
user_reply == "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
    user_reply = input("Would you like to continue? y/n  ")
597/1:
user_reply == "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
    user_reply = input("Would you like to continue? y/n  ")
597/2:
user_reply = "y"
while user_reply == "y":
    number = int(input("How many numbers? "))
    for x in range(int(number)):
        print(x)
    user_reply = input("Would you like to continue? y/n  ")
597/3:
# Initial variable to track game play
user_play = "y"

# Set start and last number
start_number = 0

# While we are still playing...
while user_play == "y":

    # Ask the user how many numbers to loop through
    user_number = input("How many numbers? ")

    # Loop through the numbers. (Be sure to cast the string into an integer.)
    for x in range(start_number, int(user_number) + start_number):

        # Print each number in the range
        print(x)

    # Set the next start number as the last number of the loop
    start_number = start_number + int(user_number)

    # Once complete...
    user_play = input("Continue the chain: (y)es or (n)o? ")
598/1:
In [6]:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
In [7]:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
159464
-------------
38573
-------------
37461
In [8]:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
Out[8]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  2014-04-04 00:00:00 Cleared by Arrest   B   3121345.0   10082705.0  2014.0
9   Auto Theft  2014-04-04 00:00:00 Not cleared C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    2014-05-08 00:00:00 Not cleared H   3125886.0   10047276.0  2014.0
17  Auto Theft  2014-12-18 00:00:00 Not cleared C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    2014-04-16 00:00:00 Not cleared B   3117183.0   10103211.0  2014.0
In [9]:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
Out[9]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  04  Cleared by Arrest   B   3121345.0   10082705.0  2014.0
9   Auto Theft  04  Not cleared C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    05  Not cleared H   3125886.0   10047276.0  2014.0
17  Auto Theft  12  Not cleared C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    04  Not cleared B   3117183.0   10103211.0  2014.0
In [10]:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
Out[10]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  04  C   B   3121345.0   10082705.0  2014.0
9   Auto Theft  04  N   C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    05  N   H   3125886.0   10047276.0  2014.0
17  Auto Theft  12  N   C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    04  N   B   3117183.0   10103211.0  2014.0
In [11]:
# Check length of set
len(crime_data_2014)
Out[11]:
32996
In [12]:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
In [13]:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
Out[13]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
0   Robbery Jan N   E   3130483.0   10102366.0  2015.0
1   Robbery Jan N   I   3124730.0   10090296.0  2015.0
2   Burglary    Jan N   E   3135985.0   10117220.0  2015.0
3   Burglary    Jan N   I   3129896.0   10096032.0  2015.0
4   Burglary    Jan N   F   3110455.0   10039340.0  2015.0
In [14]:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
Out[14]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
0   Agg Assault Jan C   D   3067322.0   10062796.0  2016.0
1   Theft   Jan C   G   3114957.0   10070462.0  2016.0
2   Robbery Jan N   E   3129181.0   10106923.0  2016.0
3   Theft   Jan N   G   3113643.0   10070357.0  2016.0
5   Agg Assault Jan N   C   3146947.0   10077985.0  2016.0
In [15]:
# Check length of set
len(crime_data_2015)
Out[15]:
36125
In [16]:
# Check length of set
len(crime_data_2016)
Out[16]:
34973
In [17]:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
In [ ]:
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
In [ ]:
crime_data_merge_final.head()
In [ ]:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
598/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
In [7]:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
159464
-------------
38573
-------------
37461
In [8]:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
Out[8]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  2014-04-04 00:00:00 Cleared by Arrest   B   3121345.0   10082705.0  2014.0
9   Auto Theft  2014-04-04 00:00:00 Not cleared C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    2014-05-08 00:00:00 Not cleared H   3125886.0   10047276.0  2014.0
17  Auto Theft  2014-12-18 00:00:00 Not cleared C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    2014-04-16 00:00:00 Not cleared B   3117183.0   10103211.0  2014.0
In [9]:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
Out[9]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  04  Cleared by Arrest   B   3121345.0   10082705.0  2014.0
9   Auto Theft  04  Not cleared C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    05  Not cleared H   3125886.0   10047276.0  2014.0
17  Auto Theft  12  Not cleared C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    04  Not cleared B   3117183.0   10103211.0  2014.0
In [10]:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
Out[10]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
4   Theft: Shoplifting  04  C   B   3121345.0   10082705.0  2014.0
9   Auto Theft  04  N   C   3118304.0   10072414.0  2014.0
14  Theft: All Other Larceny    05  N   H   3125886.0   10047276.0  2014.0
17  Auto Theft  12  N   C   3125976.0   10072207.0  2014.0
20  Theft: All Other Larceny    04  N   B   3117183.0   10103211.0  2014.0
In [11]:
# Check length of set
len(crime_data_2014)
Out[11]:
32996
In [12]:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
In [13]:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
Out[13]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
0   Robbery Jan N   E   3130483.0   10102366.0  2015.0
1   Robbery Jan N   I   3124730.0   10090296.0  2015.0
2   Burglary    Jan N   E   3135985.0   10117220.0  2015.0
3   Burglary    Jan N   I   3129896.0   10096032.0  2015.0
4   Burglary    Jan N   F   3110455.0   10039340.0  2015.0
In [14]:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
Out[14]:
primary_type    timestamp   clearance_status    district    x_coordinate    y_coordinate    year
0   Agg Assault Jan C   D   3067322.0   10062796.0  2016.0
1   Theft   Jan C   G   3114957.0   10070462.0  2016.0
2   Robbery Jan N   E   3129181.0   10106923.0  2016.0
3   Theft   Jan N   G   3113643.0   10070357.0  2016.0
5   Agg Assault Jan N   C   3146947.0   10077985.0  2016.0
In [15]:
# Check length of set
len(crime_data_2015)
Out[15]:
36125
In [16]:
# Check length of set
len(crime_data_2016)
Out[16]:
34973
In [17]:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
In [ ]:
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
In [ ]:
crime_data_merge_final.head()
In [ ]:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
598/3:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)

# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
159464
-------------
38573
-------------
37461

# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()

# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()

# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()

# Check length of set
len(crime_data_2014)

# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()

# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()

# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()

# Check length of set
len(crime_data_2015)

# Check length of set
len(crime_data_2016)

# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")

crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")

crime_data_merge_final.head()

# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
598/4:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)

# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))


# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()

# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()

# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()

# Check length of set
len(crime_data_2014)

# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()

# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()

# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()

# Check length of set
len(crime_data_2015)

# Check length of set
len(crime_data_2016)

# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")

crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")

crime_data_merge_final.head()

# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
599/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
599/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
599/3:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
599/4:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
599/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
599/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
599/7:
# Check length of set
len(crime_data_2014)
599/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
599/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
599/10:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
599/11:
# Check length of set
len(crime_data_2015)
599/12:
# Check length of set
len(crime_data_2016)
599/13:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
599/14: crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
599/15:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
599/16:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
599/17:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
599/18: crime_data_merge_final.head()
599/19:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
crime_data_merge_final.head()
599/20:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
crime_data_merge_final.head()
599/21:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
599/22:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, on = "primary_type")
crime_data_merge_final.head()
599/23:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2016, on = "primary_type")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2015, on = "primary_type")
crime_data_merge_final.head()
599/24:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2016, on = "primary_type")
#crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2015, on = "primary_type")
crime_data_merge.head()
599/25: crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2015, on = "primary_type")
599/26: crime_data_merge_final = pd.merge(crime_data_2015,crime_data_merge, on = "primary_type")
599/27:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2015,crime_data_2016, on = "primary_type")
crime_data_merge.head()
602/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
602/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
602/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
602/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
602/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
602/6:
# Check length of set
len(crime_data_2014)
602/7:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
602/8:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
602/9:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
602/10:
# Check length of set
len(crime_data_2015)
602/11:
# Check length of set
len(crime_data_2016)
602/12:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2015,crime_data_2016, on = "primary_type")
crime_data_merge.head()
602/13:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2015,crime_data_2016, on = "primary_type")
crime_data_merge.head()
602/14:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2016, on = "primary_type")
crime_data_merge.head()
602/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, on = "primary_type")
crime_data_merge.head()
603/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
603/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
603/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
603/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
603/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
603/6:
# Check length of set
len(crime_data_2014)
603/7:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
603/8:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
603/9:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
603/10:
# Check length of set
len(crime_data_2015)
603/11:
# Check length of set
len(crime_data_2016)
603/12:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
603/13:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
602/16:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, on = "primary_type",how='inner')
crime_data_merge.head()
602/17:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, on = "primary_type",how='inner')
crime_data_merge.head()
602/18:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, left_on=None, right_on=None ,how='inner')
crime_data_merge.head()
602/19:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, left_on=primary_key, right_on=None ,how='inner')
crime_data_merge.head()
602/20:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, left_on=primary_type, right_on=None ,how='inner')
crime_data_merge.head()
602/21:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, left_on=primary_type, right_on=primary_type, how='inner')
crime_data_merge.head()
602/22: #crime_data_merge_final = pd.merge(crime_data_2015,crime_data_merge, on = "primary_type")
602/23:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
602/24: crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, on = "primary_type")
602/25:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, on = "primary_type")
crime_data_merge_final.head()
602/26:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer', on = "primary_type")
crime_data_merge.head()
602/27:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer', on = "primary_type")
crime_data_merge.head()
602/28:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
602/29:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge()
602/30:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge
602/31:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
602/32:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final
602/33:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
602/34:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer', on='primary_type')
crime_data_merge_final
602/35:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final
602/36: %matplotlib notebook
602/37: %matplotlib notebook
602/38: import matplotlib.pyplot as plt
602/39: crime_data.value_count()
602/40: crime_data.value_counts()
602/41: crime_data_2014.value_counts()
602/42: crime_data_2014["primary_type"].value_counts()
602/43: incident_type = [Theft, Auto Theft, Aggravated Assault, Robbery, Homicide]
602/44: incident_type = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]
602/45: crime_data_2014["primary_type"]["Theft: Shoplifting","Theft: All Other Larceny"].add()
602/46: crime_data_2014["primary_type"].value_counts()
602/47:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
            
                                                                           
crime_data_2014["primary_type"].value_counts()
602/48:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/49:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/50:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
602/51:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
602/52:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
602/53:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
602/54:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/55:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/56:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft',
                                                                           'Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft',
                                                                           'Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft',
                                                                           'Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft',
                                                                           'Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/57:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/58:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014 ["primary_type"].value_counts()
602/59:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014 ["primary_type"].value_counts()
602/60:
crime_data_2014 ["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014 ["primary_type"].value_counts()
602/61:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts();
602/62:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
602/63:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/64:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2015["primary_type"].value_counts()
602/65: crime_data_2015["primary_type"].valuecounts()
602/66: crime_data_2015["primary_type"].valuecounts()
602/67:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
602/68:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
602/69:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
602/70:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
602/71:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/72:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
602/73:
# Check length of set
len(crime_data_2014)
602/74:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
602/75:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
602/76: crime_data_2015["primary_type"].valuecounts()
602/77: crime_data_2016['primary_key'].valuecounts()
602/78: crime_data_2016['primary_type'].valuecounts()
602/79:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
602/80: crime_data_2016['primary_type'].valuecounts()
602/81: type(crime_data_2016)
602/82:
type(crime_data_2016)
crime_data_2016["primary_type"]
602/83:
type(crime_data_2016)
crime_data_2016["primary_type"].count()
602/84:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
602/85: type(crime_data_2015)
602/86:
type(crime_data_2015)
crime_data_2015['primary_type']
602/87:
type(crime_data_2015)
crime_data_2015['primary_type'].count()
602/88:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
602/89:
type(crime_data_2014)
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/90:
type(crime_data_2014)
crime_data_2014['primary_type']
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/91:
type(crime_data_2014)
crime_data_2014['primary_type'].coun()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/92:
type(crime_data_2014)
crime_data_2014['primary_type'].count()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/93:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/94:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/95:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/96:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/97:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/98:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
crime_data_2014["primary_type"].value_counts()
602/99:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"]
602/100:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
602/101:
crime_data_15_16 = plt.plot(crime_data_2015,crime_data_2016,color='red')
plt.show()
602/102:
import matplotlib.pyplot as plt
import numpy as np
602/103:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/104:
# world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
# country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
#                         color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# # Create a legend for our chart
# plt.legend(handles=[world_avg, country_one], loc="best")

# # Show the chart
# plt.show()
602/105:
crime_data_15_16 = plt.plot(crime_data_2015,crime_data_2016,color='red')
plt.show()
602/106:
crime_data_15_16 = plt.plot(crime_data_2015,color='red')
plt.show()
602/107:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
602/108:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
602/109:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
602/110:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
602/111:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
602/112:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
602/113:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
602/114:
# Check length of set
len(crime_data_2014)
602/115:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
602/116:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
602/117:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
602/118:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
602/119:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
602/120:
# Check length of set
len(crime_data_2015)
602/121:
# Check length of set
len(crime_data_2016)
602/122:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge
602/123:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final
602/124:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
602/125: %matplotlib notebook
602/126:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/127:
# world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
# country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014"]], 
#                         color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# # Create a legend for our chart
# plt.legend(handles=[world_avg, country_one], loc="best")

# # Show the chart
# plt.show()
602/128:
crime_data_15_16 = plt.plot(crime_data_2015,color='red')
plt.show()
602/129:
# Set x axis and variables
months = np.arange(1, 12, 1)
months
602/130: plt.plot(months,primary_type)
602/131:
plt.plot(months,primary_type)
plt.show()
602/132:
plt.plot(months,points)
plt.show()
602/133:
# Set x axis and variables
months = np.arange(1, 12, 1)
months
602/134: points= ['primary_type']
602/135:
plt.plot(months,points)
plt.show()
602/136:
# Set x axis and variables
months = np.arange(1, 13, 1)
months
602/137: points= ['primary_type']
602/138:
plt.plot(months,points)
plt.show()
602/139:
# Set x axis and variables
months = np.arange(1, 13, 1)
months
602/140: points= [200,500,1000,5000,10000,25000,30000,50000,60000,70000,75000,80000]
602/141:
plt.plot(months,points)
plt.show()
602/142:
# Set x axis and variables
months = np.arange(1, 13, 1)
months
602/143: points= [Theft, Burglary, AutoTheft, AggravatedAssault, Robbery, Homicide]
602/144:
plt.plot(months,points)
plt.show()
602/145: %matplotlib notebook
602/146:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/147:
# Set x axis and variables
months = np.arange(1, 13, 1)
months
602/148: points= [Theft, Burglary, AutoTheft, AggravatedAssault, Robbery, Homicide]
602/149:
plt.plot(months,points)
plt.show()
602/150: points= ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
602/151:
plt.plot(months,points)
plt.show()
602/152:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
x_axis = np.arange(len(types))
602/153:
plt.plot(months,points)
plt.show()
602/154:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
months = np.arange(1,13,1)
602/155:
plt.bar(types, users, color='r', alpha=0.5, align="center")
plt.show()
602/156:
plt.bar(types, months, color='r', alpha=0.5, align="center")
plt.show()
602/157:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/158:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/159:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/160:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/161:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/162: %matplotlib notebook
602/163:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/164:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/165:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/166:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/167:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/168:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center",tick_label=types)
plt.show()
602/169:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center",tick_label=types)
plt.show()
602/170:
# Create an array that contains the number of users each language has
#for the height of the bars
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(months)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center",tick_label=types)
plt.show()
602/171:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
#default step size will be 1, start value will be 0, any value mentioned will mean the stop value
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.bar(types_x, months, color='r', alpha=0.5, align="center",tick_label=types)
plt.show()
602/172:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/173:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_xypes,t)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/174:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_xypes,t)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/175:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/176:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/177:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types, months, color='r', alpha=0.5, align="center")
plt.show()
602/178:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types)
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/179:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/180: %matplotlib notebook
602/181:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/182:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/183:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="center")
plt.show()
602/184:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 1.2)
plt.show()
602/185:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 0.5)
plt.show()
602/186:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.figure(figsize=(20, 3))
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 0.5)
plt.show()
602/187: %matplotlib notebook
602/188:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/189:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.figure(figsize=(20, 3))
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 0.5)
plt.show()
602/190:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 0.5)
plt.show()
602/191:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="center", width = 0.3)
plt.show()
602/192:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
602/193:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)

plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3,wrap= True)
plt.show()
602/194: %matplotlib notebook
602/195:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/196:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)

plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3,wrap= True)
plt.show()
602/197:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3,wrap= True)
plt.show()
602/198: %matplotlib notebook
602/199:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
602/200:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3,wrap= True)
plt.show()
602/201:
types = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
602/202:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
602/203:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
620/1: print("Hello User")
620/2: input("Hey, what's your name?")
621/1: print("Hello User")
621/2: a = input("Hey, what's your name?")
621/3: print("Hi"+name+"!")
621/4: print("Hi"+a+"!")
621/5: print("Hi "+ a+" !")
621/6: a = input("Hey, what's your name?")
622/1: print("Hello user")
622/2: name = input("What is your name?")
622/3: name = input("What is your name?")
623/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
623/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
623/3: print("Hello user")
623/4: name = input("What is your name?")
624/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
624/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
624/3: print("Hello user")
624/4: name = input("What is your name?")
624/5: print("Hi  " + name + "!")
624/6: age = input("What is your age?")
624/7:
if (age <20):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/8:
if (age <= 20):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/9: age = input("What is your age?")
624/10:
if (age <= 20):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/11:
if (int(age) <= 20):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/12:
if (age <= str(20)):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/13: age = input("What is your age?")
624/14:
if (age <= str(20)):
    print("Awwwwww, you are just a baby!")
else:
    print("Ah, a well travelled soul are ya!")
624/15:
for x in range(20):
    print(x)
624/16:
for x in range(5,4):
    print(x)
624/17:
for x in range(5,4):
    print(x)
624/18:
for x in range(4,5):
    print(x)
624/19:
grains = [blackgram,greengram, chickpea, ragi, rice,wheat]
for element in grains:
    print(element)
624/20:
grains = ["blackgram","greengram", "chickpea", "ragi", "rice", "wheat"]
for element in grains:
    print(element)
624/21:
x = 'y'
while x == 'y':
    print("You will do it!!!")
    x = input("Type y to increase confidence")
624/22:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print(each_candy)
624/23:
candies = ["m&ms [0]", "snickers[1]", "dairy_milk[2]", "5_star[3]", "sour_patch[4]", "dove[5]", "reese's[6]"]
for each_candy in candies:
    print(each_candy)
624/24:
allowance = 3
for x in range(allowance):
    print(x)
624/25:
allowance = 3
for x in range(allowance):
    print(x+1)
624/26:
allowance = 3
for x in range(allowance):
    print(x)
    a = int(input("How many candies do you need? "))
625/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
625/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
625/3:
candies = ["m&ms [0]", "snickers[1]", "dairy_milk[2]", "5_star[3]", "sour_patch[4]", "dove[5]", "reese's[6]"]
for each_candy in candies:
    print(each_candy)
625/4:
allowance = 3
for x in range(allowance):
    print(x)
a = int(input("How many candies do you need? "))
625/5:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print(each_candy)
625/6:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + candies.index(candy) + "]" + each_candy)
625/7:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + candies.index(each_candy) + "]" + each_candy)
625/8:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + str(candies.index(each_candy)) + "]" + each_candy)
625/9:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
626/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
626/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
626/3:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + str(candies.index(each_candy)) + "]" + each_candy)
626/4:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
for candy in candycart:
    print("I bought home " + candy)
627/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
627/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
627/3:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + str(candies.index(each_candy)) + "]" + each_candy)
627/4:
allowance = 3
candycart = []
for x in range(allowance):
a = int(input("which candies do you need? "))
    candycart.append(candies[a])
for candy in candycart:
    print("I bought home " + candy)
627/5:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
for candy in candycart:
    print("I bought home " + candy)
628/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
628/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
628/3:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + str(candies.index(each_candy)) + "]" + each_candy)
628/4:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
628/5:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
629/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
629/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
629/3:
candies = ["m&ms", "snickers", "dairy_milk", "5_star", "sour_patch", "dove", "reese's"]
for each_candy in candies:
    print("[" + str(candies.index(each_candy)) + "]" + each_candy)
629/4:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
    print(candycart)
629/5:
for candy in candycart:
    print("I bought home " + candy)
629/6:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
    print("I bought home" + candycart)
629/7:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
    print("I bought home" + str(candycart)
629/8:
allowance = 3
candycart = []
for x in range(allowance):
    a = int(input("which candies do you need? "))
    candycart.append(candies[a])
    print("I bought home" + str(candycart))
629/9:
print("Welcome to the House of Pies! Here are our pies: ")
print("---------------------------------------------------------------------")
print("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee,  (5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek,  (9) Tamale, (10) Steak")
629/10: pie_choice = int(input("choose any pie you want!"))
629/11: print("Great! We'll have that" + pie_choice + "right out for you.")
629/12: print("Great! We'll have that" + str(pie_choice) + "right out for you.")
629/13: print("Great! We'll have that " + str(pie_choice) + " right out for you.")
629/14:
print("Great! We'll have that " + str(pie_choice) + " right out for you.")
new_order = input("would like to make another order?")
631/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
631/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
631/3:
print("Welcome to the House of Pies! Here are our pies: ")
print("---------------------------------------------------------------------")
print("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee,  (5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek,  (9) Tamale, (10) Steak")
631/4:
x = 'y'
while x == 'y'
pie_choice = int(input("choose any pie you want!"))
print("Great! We'll have that " + str(pie_choice) + " right out for you.")
new_order = input("would like to make another order?")
631/5:
x = 'y'
while x == 'y':
pie_choice = int(input("choose any pie you want!"))
print("Great! We'll have that " + str(pie_choice) + " right out for you.")
new_order = input("would like to make another order?")
631/6:
x = 'y'
while x == 'y':
    pie_choice = int(input("choose any pie you want!"))
print("Great! We'll have that " + str(pie_choice) + " right out for you.")
new_order = input("would like to make another order?")
632/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
632/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
632/3:
print("Welcome to the House of Pies! Here are our pies: ")
print("---------------------------------------------------------------------")
print("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee,  (5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek,  (9) Tamale, (10) Steak")
632/4:
x = 'y'
while x == 'y':
    pie_choice = int(input("choose any pie you want!"))
    print("Great! We'll have that " + str(pie_choice) + " right out for you.")
    new_order = input("would like to make another order?")
633/1:
import pandas as pd

df = pd.DataFrame([
  [1, '3 inch screw', 0.5, 0.75],
  [2, '2 inch nail', 0.10, 0.25],
  [3, 'hammer', 3.00, 5.50],
  [4, 'screwdriver', 2.50, 3.00]],columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price'])
#df = df[["Product ID","Product Description","Cost to Manufacture","Price"]]
df = df.rename(columns={"Description":"Product Description"})
#df["Sold in Bulk?"]
df
df['Is taxed?'] = True
df
#help(df.rename)
633/2:
from string import upper
df["Product Description"]=df.Product Description.apply(upper)
df
633/3:
print("Welcome to the House of Pies! Here are our pies: ")
print("---------------------------------------------------------------------")
print("(1) Pecan, (2) Apple Crisp, (3) Bean, (4) Banoffee,  (5) Black Bun, (6) Blueberry, (7) Buko, (8) Burek,  (9) Tamale, (10) Steak")
633/4:
x = 'y'
while x == 'y':
    pie_choice = int(input("choose any pie you want!"))
    print("Great! We'll have that " + str(pie_choice) + " right out for you.")
    new_order = int(input("would like to make another order?"))
634/1:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
635/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
635/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
635/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
635/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
635/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
635/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
635/7:
# Check length of set
len(crime_data_2014)
635/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
635/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
635/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
635/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
635/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
635/13:
# Check length of set
len(crime_data_2015)
635/14:
# Check length of set
len(crime_data_2016)
635/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
635/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
635/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
635/18: %matplotlib notebook
635/19:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
635/20:
types_2014 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
635/21:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
635/22:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
635/23:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
635/24:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
635/25:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
635/26:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
636/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
636/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
636/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
636/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
636/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
636/7:
# Check length of set
len(crime_data_2014)
636/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
636/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
636/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
636/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
636/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
636/13:
# Check length of set
len(crime_data_2015)
636/14:
# Check length of set
len(crime_data_2016)
636/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
636/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
636/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
636/18: %matplotlib notebook
636/19:
# incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

# plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
#         autopct="%1.1f%%", shadow=True, startangle=140)
636/20:
types_2014 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/21:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/22:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/23:
types_2014 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/24:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/25:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/26:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/27:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'AggravatedAssault', 'Robbery', 'Homicide']

types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)

plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/28:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']

types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)

plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/29:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/30:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/31:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/32:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/33:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/34:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/35:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
636/36:
incident_type_2014 = [Theft, AutoTheft, AggravatedAssault, Robbery, Homicide]

plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/37:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']

plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/38:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0)

plt.pie(incident_type_2014, explode=explode, labels=labels, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/39:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0)

plt.pie(incident_type_2014, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/40:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0)

plt.pie(values, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/41:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0,0)

plt.pie(values, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
636/42:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0,0)

plt.pie(values, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
638/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
638/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
638/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
638/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
638/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
638/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
638/7:
# Check length of set
len(crime_data_2014)
638/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
638/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
638/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
638/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
638/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
638/13:
# Check length of set
len(crime_data_2015)
638/14:
# Check length of set
len(crime_data_2016)
638/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
638/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
638/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
638/18: %matplotlib notebook
638/19:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.1, 0, 0, 0,0)

plt.pie(values, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
638/20:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
638/21:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
638/22:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
638/23:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.1, 0, 0, 0,0)

plt.pie(values, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=140)
638/24:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.1, 0, 0, 0,0)

plt.pie(values, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=90)
638/25:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.4, 0, 0, 0,0)

plt.pie(values, explode=explodelabels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=90)
638/26:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
explode = (0.4, 0, 0, 0,0)

plt.pie(values, explode=explode, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=90)
638/27:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values, explode=none, labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=90)
638/28:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%1.1f%%", shadow=True, startangle=90)
638/29:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%1.8f%%", shadow=True, startangle=90)
638/30:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.2f%%", shadow=True, startangle=90)
638/31:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90)
638/32:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.8)
638/33:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=1.8)
638/34:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.6)
638/35:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5)
638/36:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.8)
638/37:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5)
638/38:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.5)
638/39:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.3)
638/40:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.2)
638/41:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.2,radius=0.8 )
638/42:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.2,radius=1.8 )
638/43:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,labeldistance=1.2,radius=0.8 )
638/44:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,rotatelabels=True )
638/45:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3,rotatelabels=True)
plt.show()
638/46:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
638/47:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
638/48:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8)
638/49:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="r"))
638/50:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["red", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/51:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "orange", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/52:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "lightcoral", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/53:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "darkgreen", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/54:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "lightskyblue"]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/55:
incident_type_2014 = ['Theft', '   Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
638/56:
incident_type_2014 = ['Theft', ' Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
639/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
639/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
639/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
639/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
639/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
639/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
639/7:
# Check length of set
len(crime_data_2014)
639/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
639/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
639/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
639/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
639/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
639/13:
# Check length of set
len(crime_data_2015)
639/14:
# Check length of set
len(crime_data_2016)
639/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
639/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
639/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
639/18: %matplotlib notebook
639/19:
incident_type_2014 = ['Theft', ' Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
639/20:
incident_type_2014 = ['Theft', ' Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
639/21:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
640/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
640/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
640/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
640/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
640/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
640/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
640/7:
# Check length of set
len(crime_data_2014)
640/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
640/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
640/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
640/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
640/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
640/13:
# Check length of set
len(crime_data_2015)
640/14:
# Check length of set
len(crime_data_2016)
640/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
640/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
640/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
640/18: %matplotlib notebook
640/19:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
640/20:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
#explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
640/21:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
641/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
641/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
641/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
641/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
641/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
641/7:
# Check length of set
len(crime_data_2014)
641/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
641/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
641/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
641/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
641/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
641/13:
# Check length of set
len(crime_data_2015)
641/14:
# Check length of set
len(crime_data_2016)
641/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
641/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
641/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
641/18: %matplotlib notebook
641/19:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/20:
incident_type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", ""]
explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/21:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "White"]
explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/22:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
explode = (0.4, 0, 0, 0,0)

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/23:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=90, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/24:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=110, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]

plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/26:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="INCIDENTS IN AUSTIN IN THE YEAR 2014",
          loc="center left",
          bbox_to_anchor=(1, 0, 0.5, 1))
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/27:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="INCIDENTS IN AUSTIN IN THE YEAR 2014",
          loc="center left",
          bbox_to_anchor=(0.5, 0, 0.5, 0.5))
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="center left",
          bbox_to_anchor=(0.5, 0, 0.5, 0.5))
plt_title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="center left",
          bbox_to_anchor=(0.5, 0, 0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/30:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0, 0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/31:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/32:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.setp(autotexts, size=8, weight="bold")
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/33:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.setp(incident_type_2014, size=8, weight="bold")
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/34:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.setp(values, size=8, weight="bold")
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/35:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014")
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/36:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'0.8', 'pad':5})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/37:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'1.8', 'pad':5})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/38:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'1.8', 'pad':9})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/39:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'1.8', 'pad':1})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/40:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'1.8', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/41:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'1', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/42:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'5', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/43:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=35, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/44:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=140, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/45:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(values, incident_type_2014,
          title="",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/46:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(pie_plot, incident_type_2014,
          title="INCIDENT TYPES",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie_plot=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/47:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(pie_plot, incident_type_2014,
          title="INCIDENT TYPES",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/48:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014,
          title="INCIDENT TYPES",
          loc="upper right",
          bbox_to_anchor=(0.5, 0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/49:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.5, 0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/50:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.5, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/51:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(1.5, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/52:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.2, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/53:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.9, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/54:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.6, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/55:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.7, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/56:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right",bbox_to_anchor=(0.6, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/57:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower right",bbox_to_anchor=(0.6, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/58:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.6, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/59:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(1.6, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/60:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.8, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/61:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(1.0, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/62:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 0.5,0.1,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/63:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 0.5,0.8,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/64:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 0.5,1.8,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/65:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 0.5,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/66:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 0.9,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/67:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left",bbox_to_anchor=(0.5, 1.9,0.5,0.5))
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/68:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="lower left")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/69:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"))
641/70:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"k" })
641/71:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"g" })
641/72:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"b" })
641/73:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"y" })
641/74:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"p" })
641/75:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 2, "edgecolor" :"k" })
641/76:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 5, "edgecolor" :"k" })
641/77:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops=dict(color="b"),wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/78:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={color:"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/79:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={color:"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/80:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={colors:"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/81:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/82:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/83:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black",'linewidth':1},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/84:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/85:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.legend(pie[0],labels, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/86:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.legend(pie[0],labels, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/87:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
plt.legend(pie[0],labels, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
641/88:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})

pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],labels, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
641/89:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})

pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
641/90:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="middle right", 
                          bbox_transform=plt.gcf().transFigure)
641/91:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="upper right", 
                          bbox_transform=plt.gcf().transFigure)
641/92:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"black"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
641/93:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
641/94:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
641/95:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
642/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
642/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
642/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
642/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
642/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
642/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
642/7:
# Check length of set
len(crime_data_2014)
642/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
642/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
642/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
642/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
642/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
642/13:
# Check length of set
len(crime_data_2015)
642/14:
# Check length of set
len(crime_data_2016)
642/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
642/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
642/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
642/18: %matplotlib notebook
642/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
642/20:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
642/21:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
642/22:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
642/23:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
642/24:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="lower right", 
                          bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="center right", 
                          bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/26:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="center", 
                          bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/27:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0), loc="center right", 
                          bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(,0), loc="center right", 
                     0.5     bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.5,0), loc="center right", 
                     0.5     bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/30:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.5,0), loc="center right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/31:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.1,0.5), loc="center right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/32:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.1,0.2), loc="center right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/33:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.1,0.2), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/34:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.5,0.2), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/35:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.5,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/36:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/37:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.8,0.7), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/38:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/39:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(0.9,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/40:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/41:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/42:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1.05,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/43:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1.03,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/44:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1.01,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/45:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/46:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 0, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/47:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/48:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.Figure(facecolor="lightblue")
642/49:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.Figure(facecolor="lightblue")
642/50:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/51:
incident_type_2015 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/52:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
642/53:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
643/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
643/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
643/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
643/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
643/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
643/7:
# Check length of set
len(crime_data_2014)
643/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
643/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
643/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
643/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
643/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
643/13:
# Check length of set
len(crime_data_2015)
643/14:
# Check length of set
len(crime_data_2016)
643/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
643/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
643/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
643/18: %matplotlib notebook
643/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/21:
incident_type_2016 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/22:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
643/23:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
643/24:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
643/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/26:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
643/27:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
643/28:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
644/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
644/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
644/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
644/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
644/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
644/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
644/7:
# Check length of set
len(crime_data_2014)
644/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
644/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
644/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
644/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
644/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
644/13:
# Check length of set
len(crime_data_2015)
644/14:
# Check length of set
len(crime_data_2016)
644/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
644/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
644/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
644/18: %matplotlib notebook
644/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
644/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
644/21:
incident_type_2016 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
644/22:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
644/23:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
644/24:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
644/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
644/26:
incident_type_2016 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
644/27:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624,4846,1982,1829,826,18]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
644/28:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
645/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
645/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
645/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
645/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
645/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
645/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
645/7:
# Check length of set
len(crime_data_2014)
645/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
645/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
645/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
645/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
645/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
crime_data_2016["district"].value_counts()
645/13:
# Check length of set
len(crime_data_2015)
645/14:
# Check length of set
len(crime_data_2016)
645/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
645/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
645/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
645/18: %matplotlib notebook
645/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
645/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
645/21:
incident_type_2016 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
645/22:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
645/23:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
645/24:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
645/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
645/26:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
645/27:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
645/28:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
645/29:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
645/30:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
645/31:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/1: %matplotlib notebook
646/2:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/3:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
646/4:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
646/5:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
646/6:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
646/7:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
646/8:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
646/9:
# Check length of set
len(crime_data_2014)
646/10:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
646/11:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
646/12:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
646/13:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
646/14:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
646/15:
# Check length of set
len(crime_data_2015)
646/16:
# Check length of set
len(crime_data_2016)
646/17:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
646/18:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
646/19:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
646/20: %matplotlib notebook
646/21:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/22:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/23:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/24:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/25:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/26:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/27:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/28:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
646/30:
types_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
646/31:
type_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
649/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
649/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
649/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
649/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
649/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
649/7:
# Check length of set
len(crime_data_2014)
649/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
649/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
649/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
649/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
649/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
649/13:
# Check length of set
len(crime_data_2015)
649/14:
# Check length of set
len(crime_data_2016)
649/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
649/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
649/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
649/18: %matplotlib notebook
649/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827,839,25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/21:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/22:
type_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/23:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [28143, 2162, 1827,839,25]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/24:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/25:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/26:
type_2014 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/27:
types_2015 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/28:
types_2016 = ['Theft', 'Burglary', 'AutoTheft', 'AggravatedAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
649/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/30:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), bbox={'facecolor': 15}, loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
649/31:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
650/1:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
651/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
651/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
651/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
651/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
651/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
651/7:
# Check length of set
len(crime_data_2014)
651/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
651/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
651/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
651/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
651/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
651/13:
# Check length of set
len(crime_data_2015)
651/14:
# Check length of set
len(crime_data_2016)
651/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
651/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
651/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
651/18: %matplotlib notebook
651/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/21:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/22:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/23:
types_2015 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/24:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/26:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/27:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/28:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/29:
types_2015 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/30:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/31:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/32:
types_2015 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/33:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
651/34: %matplotlib notebook
651/35:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/36:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
651/37:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
652/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
652/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
652/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
652/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
652/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
652/7:
# Check length of set
len(crime_data_2014)
652/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
652/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
652/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
652/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
652/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
652/13:
# Check length of set
len(crime_data_2015)
652/14:
# Check length of set
len(crime_data_2016)
652/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
652/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
652/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
652/18: %matplotlib notebook
652/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/21:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/22:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
652/23:
types_2015 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
652/24:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
652/25:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/26:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/27:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
652/28:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(x, bins=bins)
652/29:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(x, bins=bins)
652/30:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(x, bins=bins)
plt.show()
652/31:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=bins)
plt.show()
652/32:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=bins)
plt.show()
653/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
653/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
653/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
653/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
653/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
653/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
653/7:
# Check length of set
len(crime_data_2014)
653/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
653/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
653/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
653/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
653/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
653/13:
# Check length of set
len(crime_data_2015)
653/14:
# Check length of set
len(crime_data_2016)
653/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
653/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
653/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
653/18: %matplotlib notebook
653/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
653/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
653/21:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
653/22:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
months = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
653/23:
types_2015 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2015))
months = [26624, 4846, 1982, 1829, 826, 18]
plt.xticks(types_x, types_2015)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
653/24:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(types_2016))
months = [24911, 5039, 2092, 1990, 911, 30]
plt.xticks(types_x, types)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
653/25:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=bins)
plt.show()
653/26:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.barstacked(types_2016, bins=bins)
plt.show()
653/27:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.barstacked(types_2016, bins=bins)
plt.show()
653/28:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.barstacked(types_2016, bins=months)
plt.show()
654/1: import matplotlib as plt
654/2:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.barstacked(types_2016, bins=months)
plt.show()
654/3:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=months)
plt.show()
654/4: import matplotlib as plt
654/5:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=months)
plt.show()
654/6: import matplotlib from pyplot as plt
654/7:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=months)
plt.show()
654/8: import matplotlib from pyplot as plt
654/9: import matplotlib.pyplot as plt
654/10:
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
bins = [50,1000,2500,5000,6000,25000]
months = [24911, 5039, 2092, 1990, 911, 30]
plt.hist(types_2016, bins=months)
plt.show()
653/29:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('month')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 30000, 1000))
plt.legend((p1[0], p2[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/30:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('month')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 30000, 1000))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/31:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 30000, 1000))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/32:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 12, 1))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/33:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 12, 1))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/34:
N = 5
values_2014 = (28143, 2162, 1827, 839, 25)
values_2015 = (26624, 4846, 1982, 1829, 826, 18)
values_2016 = (24911, 5039, 2092, 1990, 911, 30)
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 12, 1))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/35:
N = 5
values_2014 = (\[28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 12, 1))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/36:
N = 5
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence
p1 = plt.bar(ind, values_2014,)
p2 = plt.bar(ind, values_2015)
p3 = plt.bar(ind, values_2016)
plt.ylabel('months')
plt.title('Incident in Austin in 2014')
plt.xticks(ind, ('Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'))
plt.yticks(np.arange(0, 12, 1))
plt.legend((p1[0], p2[0], p3[0]), ('incidents_2014', 'incident_2015','incidents_2016'))
plt.show()
653/37:
#2014 crime rate by months
x_axis = np.arange(1, 13, 1)
x_axis
653/38:
#2014 crime rate by months
x_axis = np.arange(1, 13, 1)
values = [26624, 4846, 1982, 1829, 826, 18]
types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
plt.plot(x_axis,values)
plt.show()
653/39:
#2014 crime rate by months
x = np.arange(1, 13, 1)
values = [26624, 4846, 1982, 1829, 826, 18]
#types_2016 = ['Theft', 'Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
plt.plot(x,values)
plt.show()
653/40:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016
660/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
660/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
660/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
660/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
660/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
660/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
660/7:
# Check length of set
len(crime_data_2014)
660/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
660/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
660/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
660/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016
660/12:
type(crime_data_2016)
crime_data_2016["primary_type"].value_counts()
#crime_data_2016["district"].value_counts()
660/13:
# Check length of set
len(crime_data_2015)
660/14:
# Check length of set
len(crime_data_2016)
660/15:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2016,crime_data_2015, how='outer')
crime_data_merge.head()
660/16:
crime_data_merge_final = pd.merge(crime_data_2014,crime_data_merge, how = 'outer')
crime_data_merge_final.head()
660/17:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
660/18: %matplotlib notebook
660/19:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
660/20:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
660/21:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
660/22:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
values = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
660/23:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.value_count(s)
660/24:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.value_counts()
661/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
661/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
661/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
661/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
661/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
661/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
661/7:
# Check length of set
len(crime_data_2014)
661/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
661/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
661/10:
type(crime_data_2015)
crime_data_2015['primary_type'].value_counts()
661/11:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.value_counts()
661/12:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
661/13:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
661/14:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
661/15:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
661/16:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
661/17:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
661/18:
# Check length of set
len(crime_data_2014)
661/19:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
661/20:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
661/21:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
661/22:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
661/23: crime_data_2016.value_counts()
661/24: crime_data_2016["timestamp"].value_counts()
661/25: crime_data_2015["timestamp"].value_counts()
661/26: crime_data_2014["timestamp"].value_counts()
661/27:
type(crime_data_2015)
crime_data_2015['time_stamp'].value_counts()
662/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
662/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
662/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
662/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
662/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
662/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
662/7:
# Check length of set
len(crime_data_2014)
662/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
662/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
662/10:
type(crime_data_2015)
crime_data_2015['time_stamp'].value_counts()
662/11: crime_data_2014["timestamp"].value_counts()
662/12: crime_data_2015["timestamp"].value_counts()
662/13: crime_data_2016["timestamp"].value_counts()
662/14: crime_data_2016["timestamp"].value_counts()
662/15: crime_data_2016['timestamp'].value_counts()
662/16: crime_data_2016["primary_type"].value_counta()
662/17: crime_data_2016["primary_type"].value_counts()
663/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
663/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
663/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
663/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
663/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
663/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
663/7:
# Check length of set
len(crime_data_2014)
663/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
663/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
663/10:
type(crime_data_2015)
crime_data_2015['time_stamp'].value_counts()
663/11: crime_data_2016["primary_type"].value_counts()
663/12:
#2014 crime rate by district
x1 = 01 = crime_data_2014.loc[crime_data_2014["timestamp"] =="01"]
x2 = 02 = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = 03 = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = 04 = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = 05 = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = 06 = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = 07 = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = 08 = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = 09 = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = 10 = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = 11 = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = 12 = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["primary_type"].value_counts()
primary_value_counts.plot(kind="bar")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/13:
#2014 crime rate by district
x1 = "01" = crime_data_2014.loc[crime_data_2014["timestamp"] =="01"]
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["primary_type"].value_counts()
primary_value_counts.plot(kind="bar")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/14:
#2014 crime rate by district
x1 = 01 = crime_data_2014.loc[crime_data_2014["timestamp"] == 01]
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["primary_type"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/15:
#2014 crime rate by district
x1 = '01' = crime_data_2014.loc[crime_data_2014["timestamp"] == '01']
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["primary_type"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/16:
#2014 crime rate by district
x1 = '01' = crime_data_2014.loc[crime_data_2014["timestamp"] == '01']
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/17:
#2014 crime rate by district
x1 = '01' = crime_data_2014.loc[crime_data_2014["timestamp"] == '01']
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/18:
#2014 crime rate by district
x1 = "01" = crime_data_2014.loc[crime_data_2014["timestamp"] == 01]
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/19:
#2014 crime rate by district
x1 = "01" = crime_data_2014.loc[crime_data_2014["timestamp"] == "01"]
x2 = "02" = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = "03" = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = "04" = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = "05" = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = "06" = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = "07" = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = "08" = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = "09" = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = "10" = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = "11" = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = "12" = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/20:
#2014 crime rate by district
x1 = crime_data_2014.loc[crime_data_2014["timestamp"] == "01"]
x2 = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = x1["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/21:
#2014 crime rate by district
x1 = crime_data_2014.loc[crime_data_2014["timestamp"] == "01"]
x2 = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = crime_data_2014["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/22:
#2014 crime rate by district
x1 = crime_data_2014.loc[crime_data_2014["timestamp"] == "01"]
x2 = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = crime_data_2014["timestamp"].value_counts()
primary_value_counts.plot(kind="bar")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/23:
#2014 crime rate by district
x1 = crime_data_2014.loc[crime_data_2014["timestamp"] == "01"]
x2 = crime_data_2014.loc[crime_data_2014["timestamp"] =="02"]
x3 = crime_data_2014.loc[crime_data_2014["timestamp"] =="03"]
x4 = crime_data_2014.loc[crime_data_2014["timestamp"] =="04"]
x5 = crime_data_2014.loc[crime_data_2014["timestamp"] =="05"]
x6 = crime_data_2014.loc[crime_data_2014["timestamp"] =="06"]
x7 = crime_data_2014.loc[crime_data_2014["timestamp"] =="07"]
x8 = crime_data_2014.loc[crime_data_2014["timestamp"] =="08"]
x9 = crime_data_2014.loc[crime_data_2014["timestamp"] =="09"]
x10 = crime_data_2014.loc[crime_data_2014["timestamp"] =="10"]
x11 = crime_data_2014.loc[crime_data_2014["timestamp"] =="11"]
x12 = crime_data_2014.loc[crime_data_2014["timestamp"] =="12"]
primary_value_counts = crime_data_2014["timestamp"].value_counts()
primary_value_counts.plot(kind="line")
plt.title("Crime data 2014")
plt.xlabel("Crime Type")
plt.ylabel("Crime count")
663/24: crime_data_2014
663/25: x10
663/26: oct_crimes = x10.groupby('primary_type').count()
663/27: oct_crimes
663/28: oct_crimes.value_counts()
663/29: oct_crimes['timestamp'].value_counts()
663/30: oct_crimes
663/31: crime_data_2014
663/32: crime_data_2014.groupby('timestamp').count()
663/33: pd.get_dummies(crime_data_2014)
663/34:
test = pd.get_dummies(crime_data_2014)
test['timestamp']
663/35:
test = pd.get_dummies(crime_data_2014)
test['timestamp_01']
663/36:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
test['timestamp']
663/37:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
test
663/38:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
test.value_count()
663/39:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
test.value_counts()
663/40:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
test['timestamp'].value_counts()
663/41:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/42: grouped
663/43: crime_data_2014.head()
663/44: test.head()
663/45: grouped
663/46: slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
663/47:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
slimmed_data.head()
663/48:
test = pd.get_dummies(crime_data_2014, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/49: grouped
663/50:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/51: grouped
663/52: grouped.plot(kind='line')
663/53:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
663/54:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/55:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
slimmed_data.head()
663/56:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/57:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data.head()
663/58:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
663/59:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
664/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
664/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
664/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
664/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
664/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
664/6:
type(crime_data_2014)
crime_data_2014['primary_type'].value_counts()
# crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Theft: All Other Larceny':'Theft','Theft: BOV':'Theft','Theft: Shoplifting':'Theft','Theft: Pocket Picking':'Theft','Theft: Auto Parts': 'Theft', 'Theft: from Building':'Theft','Theft: from Building':'Theft','Theft: Coin Op Machine':'Theft','Theft: Purse Snatching':'Theft'}
# crime_data_2014["primary_type"].value_counts()
664/7:
# Check length of set
len(crime_data_2014)
664/8:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
664/9:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
664/10:
type(crime_data_2015)
crime_data_2015['time_stamp'].value_counts()
668/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
668/2:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
668/3:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
668/4:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
668/5:
# Check length of set
len(crime_data_2014)
668/6:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
668/7:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
668/8:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
668/9:
# Check length of set
len(crime_data_2015)
668/10:
# Check length of set
len(crime_data_2016)
668/11:
# Merge all data sets into one
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer",on = "primary_type")
668/12: crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on = "primary_type")
668/13: crime_data_merge_final.head()
668/14:
# Print value counts for review
print(crime_data_merge_final["primary_type"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["timestamp"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["clearance_status"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["district"].value_counts())
print("-------------------------------\n")
print(crime_data_merge_final["year"].value_counts())
669/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
669/2:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
669/3:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
669/4:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
669/5:
# Check length of set
len(crime_data_2014)
669/6:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
669/7:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0

# Edit timestamp column to show only month
crime_data_2015['timestamp'] = crime_data_2015['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2015.head()
669/8:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0

# Edit timestamp column to show only month
crime_data_2016['timestamp'] = crime_data_2016['timestamp'].map(lambda x: str(x)[2:5])
crime_data_2016.head()
669/9:
# Check length of set
len(crime_data_2015)
669/10:
# Check length of set
len(crime_data_2016)
669/11:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
669/12:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
670/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/2:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
670/3:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
670/4:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/5:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
670/6:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
671/1:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
671/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/7:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/8:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/9:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
670/10:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
670/11:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
670/12:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
670/13: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
670/14:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
670/15: crime_data_2014["district"].value_counts()
670/16: crime_data_2014["timestamp"].value_counts()
670/17: crime_data_2014["primary_type"].value_counts()
670/18:
# Check length of set
len(crime_data_2014)
670/19:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
670/20:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
670/21:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
670/22: crime_data_2015["primary_type"].value_counts()
670/23:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
670/24: crime_data_2015["timestamp"].value_counts()
670/25:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
670/26:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
670/27: crime_data_2016["timestamp"].value_counts()
670/28:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
670/29: crime_data_2016["primary_type"].value_counts()
670/30:
# Check length of set
len(crime_data_2015)
670/31:
# Check length of set
len(crime_data_2016)
670/32:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
670/33:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
670/34:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
670/35:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
670/36:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
670/37:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
670/38:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
670/39:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
670/40: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
670/41:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
670/42: crime_data_2014["district"].value_counts()
670/43: crime_data_2014["timestamp"].value_counts()
670/44: crime_data_2014["primary_type"].value_counts()
670/45:
# Check length of set
len(crime_data_2014)
670/46:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
670/47:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
670/48:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
670/49: crime_data_2015["primary_type"].value_counts()
670/50:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
670/51: crime_data_2015["timestamp"].value_counts()
670/52:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
670/53:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
670/54: crime_data_2016["timestamp"].value_counts()
670/55:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
670/56: crime_data_2016["primary_type"].value_counts()
670/57:
# Check length of set
len(crime_data_2015)
670/58:
# Check length of set
len(crime_data_2016)
670/59:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
670/60:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
670/61:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
670/62:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
670/63:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
670/64:
type_2014 = ['Theft', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
types_x = np.arange(len(type_2014))
values = [28143, 2162, 1827, 839, 25]
plt.xticks(types_x, type_2014)
plt.bar(types_x, months, color='r', alpha=0.5, align="edge", width = 0.3)
plt.show()
670/65:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
slimmed_data.head()
670/66:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
670/67:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
670/68: grouped.plot(kind='line')
670/69:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
slimmed_data.head()
670/70:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
670/71:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data.head()
670/72:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
670/73:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
670/74: grouped.plot(kind='line')
670/75:
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
grouped.plot(kind='line')
670/76:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
670/77:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
670/78:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
670/79:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='stackedbar')
670/80:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
670/81:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar',yerr=test)
670/82:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar',yerr="primary_type")
670/83:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
670/84:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
670/85:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
670/86:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('Scores')
plt.title('Scores by group and gender')
plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
plt.yticks(np.arange(0, 81, 10))
plt.legend((p1[0], p2[0]), ('Men', 'Women'))
670/87:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))
672/1:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))
672/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
672/3:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
672/4:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
672/5:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))
672/6:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))
672/7:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
672/8:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
672/9:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
672/10:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
672/11:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
672/12:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
672/13:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
672/14:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
672/15:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
672/16: crime_data_2015.head()
673/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
673/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
673/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
673/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
673/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
673/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
673/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
673/8: crime_data_2014["district"].value_counts()
673/9: crime_data_2014["timestamp"].value_counts()
673/10: crime_data_2014["primary_type"].value_counts()
673/11:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
673/12:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
673/13: crime_data_2015.head()
673/14:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
673/15: crime_data_2015.head()
673/16: crime_data_2015["primary_type"].value_counts()
673/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
673/18: crime_data_2015["timestamp"].value_counts()
673/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
673/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
673/21: crime_data_2016["timestamp"].value_counts()
673/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
673/23: crime_data_2016["primary_type"].value_counts()
673/24:
# Check length of set
len(crime_data_2015)
673/25:
# Check length of set
len(crime_data_2016)
673/26:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
673/27:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
673/28:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
673/29:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/30: crime_data_2015.head()
673/31:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
673/32:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
grouped.plot(kind='line')
673/33:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/34:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
673/35:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
grouped.plot(kind='line')
673/36:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')

plt.ylabel('values')
# plt.title('Scores by group and gender')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/37:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')

plt.ylabel('values')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/38:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')

plt.ylabel('values')
plt.title('CRIME DATA 2014',)
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/39:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/40:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('TIMESTAMP').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/41:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/42:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
673/43:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
673/44:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
Austin_Population = 931,830
crime_data_2016["primary_type"].value_counts()
popl_primary_type =
673/45:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
Austin_Population = 931,830
crime_data_2016["primary_type"].value_counts()
#popl_primary_type =
673/46: grouped
673/47:
Austin_Population = 931,830
t = grouped / Austin_Population
t
673/48:
Austin_Population = 931,830
t = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'] / Austin_Population
t
673/49: grouped
673/50: grouped.dtypes
673/51:
Austin_Population = 931,830
t = grouped /Austin_Population
673/52:
Austin_Population = 931830
t = grouped /Austin_Population
673/53:
Austin_Population = 931830
t = grouped /Austin_Population
t
673/54:
Austin_Population = 931830
t = (grouped /Austin_Population)*100000
t
673/55:
Austin_Population = 931830 / 100000
t = (grouped /Austin_Population)
t
673/56:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
Austin_Population = 931,830
crime_data_2016["primary_type"].value_counts()
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
673/57:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
Austin_Population = 931,830
crime_data_2016["primary_type"].value_counts()
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
673/58:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
Austin_Population = 931,830
crime_data_2016["primary_type"].value_counts()
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/59:
values = [24911, 5039, 2092, 1990, 911, 30]
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
slimmed_data = crimes_popl_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
crimes_popl_2014.plot(kind='line')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
673/60:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.py

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
674/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.py

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
674/3:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
674/4:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
674/5:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
674/6:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
674/7:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
674/8:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
674/9: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
674/10:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
674/11: crime_data_2014["district"].value_counts()
674/12: crime_data_2014["timestamp"].value_counts()
674/13: crime_data_2014["primary_type"].value_counts()
674/14:
# Check length of set
len(crime_data_2014)
674/15:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
674/16:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
674/17: crime_data_2015.head()
674/18:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
674/19: crime_data_2015["primary_type"].value_counts()
674/20:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
674/21: crime_data_2015["timestamp"].value_counts()
674/22:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
674/23:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
674/24: crime_data_2016["timestamp"].value_counts()
674/25:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
674/26: crime_data_2016["primary_type"].value_counts()
674/27:
# Check length of set
len(crime_data_2015)
674/28:
# Check length of set
len(crime_data_2016)
674/29:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
674/30:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
674/31:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
674/32:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
674/33:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
674/34:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/35:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
#slimmed_data.head()
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('VALUES')
plt.title('CRIME DATA 2014')
674/36:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/37:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA Based on AUSTIN POPULATION 2014')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/38:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('NO. OF CRIMES')
plt.title('CRIME DATA BASED ON AUSTIN POPULATION 2014\n(per 10000)')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/39:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA BASED ON AUSTIN POPULATION 2014\n(per 10000)')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/40:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
674/41:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
# plt.xticks(test, ('G1', 'G2', 'G3', 'G4', 'G5'))
plt.xticks(np.arange(1, 13, 1))
674/42:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
plt.xticks(np.arange(1, 13, 1))
plt.grid
674/43:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
plt.xticks(np.arange(1, 13, 1))
674/44:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
plt.xticks(np.arange(1, 13, 1))
674/45:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
plt.xticks(np.arange(1, 13, 1))
plt.legend(line[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/46:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)')
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/47:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/48:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.5,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/49:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/50:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox_to_anchor=(1.2,0.5)bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/51:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox_to_anchor=(1.2,0.5),bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/52:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 10000 INHABITANTS')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/53:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(0.5,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/54:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/55:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/56:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/57:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/58:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
# plt.xtickslabels(np.arange(1, 13, 1))
plt.xticks(np.arange(12), calendar.month_name[1:13], rotation=20)
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/59:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xtickslabels(np.arange(1, 13, 1))

plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/60:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xtick(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/61:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
674/62:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.xaxis.grid(True)
674/63:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.grid(major)
674/64:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.grid(color='r', linestyle='-', linewidth=2)
674/65:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.grid(color='r', linestyle='-', linewidth=0.2)
674/66:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.grid(color='k', linestyle='-', linewidth=0.2)
674/67:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.xaxis.grid(color='k', linestyle='-', linewidth=0.2)
674/68:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
#plt.xaxis.grid(color='k', linestyle='-', linewidth=0.2)
plt.gca().xaxis.grid(True)
674/69:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
#plt.xaxis.grid(color='k', linestyle='-', linewidth=0.2)
plt.gca().yaxis.grid(True)
674/70:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.2)
674/71:
values = [24911, 5039, 2092, 1990, 911, 30]
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/72:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/73:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/74:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/75:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/76:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels)
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/77:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/78:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels)
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/79:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/80:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/81:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/82:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/83:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/84:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fig)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/85:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/86:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/87:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/88:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/89:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/90:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(30,10))
674/91:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(30,20))
674/92:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(50,20))
674/93:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(50,50))
674/94:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(100,50))
674/95:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/96:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/97:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/98:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/99:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/100:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line',marker='o')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/101:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line',marker='*')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
674/102:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
#plt.figure(figsize=(20,10))
674/103:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/104:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
674/105:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
675/1:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
675/2:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8}
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
675/3:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
675/4:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
675/5:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
675/6:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
675/7:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
675/8:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(100,50))
676/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
676/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
676/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
676/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
676/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
676/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
676/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
676/8: crime_data_2014["district"].value_counts()
676/9: crime_data_2014["timestamp"].value_counts()
676/10: crime_data_2014["primary_type"].value_counts()
676/11:
# Check length of set
len(crime_data_2014)
676/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
676/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
676/14: crime_data_2015.head()
676/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
676/16: crime_data_2015["primary_type"].value_counts()
676/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
676/18: crime_data_2015["timestamp"].value_counts()
676/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
676/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
676/21: crime_data_2016["timestamp"].value_counts()
676/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
676/23: crime_data_2016["primary_type"].value_counts()
676/24:
# Check length of set
len(crime_data_2015)
676/25:
# Check length of set
len(crime_data_2016)
676/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
676/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
676/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
676/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
676/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
676/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped /Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line',marker='*')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
676/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(100,50))
676/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/34:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/35:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.figure(figsize=(100,50))
676/36:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line',marker='*')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
676/37:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line',marker='o')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.figure(figsize=(100,50))
676/38:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/39:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line',marker='o')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/40:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line',marker='o')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
676/41:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line',marker='o')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
676/42:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/43:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
676/44:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
676/45:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/46:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
676/47:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(3)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,'Theft', width=0.3, color='k')
p2 = plt.bar(x_loc, 'AutoTheft', bottom='Theft', width=0.3,color='r')
p3 = plt.bar(x_loc, 'AggravatedAssault', bottom='AutoTheft', width=0.3,color='g')
p4 = plt.bar(x_loc, 'Robbery', bottom='AggravatedAssault', width=0.3,color='b')
p4 = plt.bar(x_loc, 'Homicide', bottom='Robbery', width=0.3,color='y')
plt.xticks(x_loc,'2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/48:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(2)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,'Theft', width=0.3, color='k')
p2 = plt.bar(x_loc, 'AutoTheft', bottom='Theft', width=0.3,color='r')
p3 = plt.bar(x_loc, 'AggravatedAssault', bottom='AutoTheft', width=0.3,color='g')
p4 = plt.bar(x_loc, 'Robbery', bottom='AggravatedAssault', width=0.3,color='b')
p4 = plt.bar(x_loc, 'Homicide', bottom='Robbery', width=0.3,color='y')
plt.xticks(x_loc,'2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/49:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(2)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,'Theft', width=0.3, color='k')
p2 = plt.bar(x_loc, 'AutoTheft', bottom='Theft', width=0.3,color='r')
p3 = plt.bar(x_loc, 'AggravatedAssault', bottom='AutoTheft', width=0.3,color='g')
p4 = plt.bar(x_loc, 'Robbery', bottom='AggravatedAssault', width=0.3,color='b')
p4 = plt.bar(x_loc, 'Homicide', bottom='Robbery', width=0.3,color='y')
plt.xticks(x_loc,'2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/50:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(3)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,'Theft', width=0.3, color='k')
p2 = plt.bar(x_loc, 'AutoTheft', bottom='Theft', width=0.3,color='r')
p3 = plt.bar(x_loc, 'AggravatedAssault', bottom='AutoTheft', width=0.3,color='g')
p4 = plt.bar(x_loc, 'Robbery', bottom='AggravatedAssault', width=0.3,color='b')
p4 = plt.bar(x_loc, 'Homicide', bottom='Robbery', width=0.3,color='y')
plt.xticks(x_loc,'2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/51:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(3)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,'Theft', width=0.3, color='k')
p2 = plt.bar(x_loc, 'AutoTheft', bottom='Theft', width=0.3,color='r')
p3 = plt.bar(x_loc, 'AggravatedAssault', bottom='AutoTheft', width=0.3,color='g')
p4 = plt.bar(x_loc, 'Robbery', bottom='AggravatedAssault', width=0.3,color='b')
p4 = plt.bar(x_loc, 'Homicide', bottom='Robbery', width=0.3,color='y')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/52:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=np.arange(3)
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,values[0], width=0.3, color='k')
p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/53:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=('2014','2015','2016')
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
p1 = plt.bar(x_loc,values[0], width=0.3, color='k')
p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/54:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=('2014','2015','2016')
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1 = plt.bar(x_loc,values[1], bottom=values[0]width=0.3, color='k')
# p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/55:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=('2014','2015','2016')
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1 = plt.bar(x_loc,values[1], bottom=values[0], width=0.3, color='k')
# p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/56:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=[]'2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc,values[1], bottom=values[0], width=0.3, color='k')
# p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/57:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc,values[1], bottom=values[0], width=0.3, color='k')
# p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/58:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values[0], width=0.3, color='k')
# p2 = plt.bar(x_loc, values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/59:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values[1], bottom=values[0], width=0.3,color='r')
# p3 = plt.bar(x_loc, values[2], bottom=values[1], width=0.3,color='g')
# p4 = plt.bar(x_loc, values[3], bottom=values[2], width=0.3,color='b')
# p4 = plt.bar(x_loc, values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/60:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values[2], bottom=values[1], width=0.3,color='g')
p4_2014 = plt.bar(x_loc[0], values[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/61:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values[2], bottom=values[1], width=0.3,color='g')
p4_2014 = plt.bar(x_loc[0], values[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values[2], bottom=values[1], width=0.3,color='g')
p4_2015 = plt.bar(x_loc[1], values[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/62:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='g')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='g')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/63:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='g')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='o')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/64:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='o')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='o')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/65:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='p')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='p')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/66:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='y')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='y')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/67:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/68:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
#p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
#p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/69:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/70:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='r')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='lightblue')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/71:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='r')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='lightblue')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/72:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='r')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='lightblue')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[4], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/73:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p6_2014 = plt.bar(x_loc[2], values_2014[4], width=0.3,color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p6_2015 = plt.bar(x_loc[2], values_2015[4], width=0.3,color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='r')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='lightblue')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[4], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/74:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='k')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='r')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='lightblue')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3,color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='k')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='r')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='lightblue')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3,color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='r')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='lightblue')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/75:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3,color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3,color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='k')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/76:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3,color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3,color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3,color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/77:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.6,color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3,color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3,color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3,color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3,color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3,color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3,color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3,color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3,color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3,color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3,color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3,color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3,color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3,color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/78:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, cowidth, lor='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, cowidth, lor='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/79:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, cowidth, lor='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/80:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width,color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
676/81:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
677/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
677/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
677/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
677/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
677/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
677/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
677/8: crime_data_2014["district"].value_counts()
677/9: crime_data_2014["timestamp"].value_counts()
677/10: crime_data_2014["primary_type"].value_counts()
677/11:
# Check length of set
len(crime_data_2014)
677/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
677/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
677/14: crime_data_2015.head()
677/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
677/16: crime_data_2015["primary_type"].value_counts()
677/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
677/18: crime_data_2015["timestamp"].value_counts()
677/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
677/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
677/21: crime_data_2016["timestamp"].value_counts()
677/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
677/23: crime_data_2016["primary_type"].value_counts()
677/24:
# Check length of set
len(crime_data_2015)
677/25:
# Check length of set
len(crime_data_2016)
677/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
677/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
677/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
677/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
677/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
677/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/35:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
677/36:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
677/37:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
677/38:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
677/39:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/40:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
width = 0.6
p1_2014 = plt.bar(x_loc[0],values_2014[0], width, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/41:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/42:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/43:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
savefig('crime_months_2014.png')
677/44:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
crime_month_2014 = savefig('crime_months_2014.png')
677/45:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png')
677/46:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
677/47:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
677/48:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
677/49:
crimetype=crime_data_2014.groupby("primary_type")
crimetype
677/50:
crimetype=crime_data_2014.groupby("primary_type")
crimetype.head()
677/51:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
677/52:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
677/53:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
677/54:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
677/55:
crimetype=crime_data_2014.groupby("primary_type")
crimetype.head()
crime_data_2014.head()
677/56:
crimetype=crime_data_2014.groupby("primary_type")
crimetype.head()
crime_data_2014.groupby("primary_type")
677/57:
crimetype=crime_data_2014.groupby("primary_type")
crimetype.head()
crimetype = crime_data_2014.groupby("primary_type")
677/58:
crimetype=crime_data_2014.groupby("primary_type")
crimetype.head()
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
677/59:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
677/60:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
677/61:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
677/62:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
677/63:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
677/64:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
677/65: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
677/66:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
677/67: crime_data_2014["district"].value_counts()
677/68: crime_data_2014["timestamp"].value_counts()
677/69: crime_data_2014["primary_type"].value_counts()
677/70:
# Check length of set
len(crime_data_2014)
677/71:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
677/72:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
677/73: crime_data_2015.head()
677/74:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
677/75: crime_data_2015["primary_type"].value_counts()
677/76:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
677/77: crime_data_2015["timestamp"].value_counts()
677/78:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
677/79:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
677/80: crime_data_2016["timestamp"].value_counts()
677/81:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
677/82: crime_data_2016["primary_type"].value_counts()
677/83:
# Check length of set
len(crime_data_2015)
677/84:
# Check length of set
len(crime_data_2016)
677/85:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
677/86:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
677/87:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/88:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/89:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
677/90:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
677/91:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
677/92:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
677/93:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
677/94:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
677/95:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
total_crimetype = crimetype.sum()
677/96:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
total_crimetype = crimetype.sum()
total_crimetype
677/97:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
total_crimetype = crimetype("primary_type").sum()
total_crimetype
677/98:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.head()
total_crimetype = crimetype("primary_type").sum()
#total_crimetype
677/99:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.sum()
total_crimetype = crimetype.sum()
#total_crimetype
677/100:
crimetype = crime_data_2014.groupby("primary_type")
crimetype.sum()
#total_crimetype = crimetype.sum()
#total_crimetype
677/101:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_2014.groupby("primary_type")
# crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
677/102:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final
# crimetype = crime_data_2014.groupby("primary_type")
# crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
677/103:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
677/104:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
677/105:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer", on='primary_type')
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer",on='primary_type')
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
679/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
679/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
679/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
679/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
679/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
679/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
679/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014.head()
679/8: crime_data_2014["district"].value_counts()
679/9: crime_data_2014["timestamp"].value_counts()
679/10: crime_data_2014["primary_type"].value_counts()
679/11:
# Check length of set
len(crime_data_2014)
679/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
679/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
679/14: crime_data_2015.head()
679/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
679/16: crime_data_2015["primary_type"].value_counts()
679/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
679/18: crime_data_2015["timestamp"].value_counts()
679/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
679/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
679/21: crime_data_2016["timestamp"].value_counts()
679/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
679/23: crime_data_2016["primary_type"].value_counts()
679/24:
# Check length of set
len(crime_data_2015)
679/25:
# Check length of set
len(crime_data_2016)
679/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
679/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
679/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
679/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
679/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
679/35:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype.sum()
# #total_crimetype = crimetype.sum()
# #total_crimetype
679/36:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
# #total_crimetype
679/37:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype
679/38:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
#crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype.sort_values(by='primary_type').head()
679/39:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
679/40:
ax1 = plt.subplot2grid((3,3),(1,0))
ax2.plot(theft.groupby('timestamp').size(),'o-')
ax2.set title('Theft')
679/41:
ax1 = plt.subplot2grid((3,3),(1,0))
ax2.plot(theft.groupby('timestamp').size(),'o-')
#ax2.set title('Theft')
679/42:
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
#ax2.set title('Theft')
679/43:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
#ax2.set title('Theft')
679/44:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set title('Theft')
679/45:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
679/46:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-',figsize=(20,30))
ax1.set_title('Theft')
679/47:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
679/48:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((3,3),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/49:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((3,4),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/50:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((9,9),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((9,9),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/51:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((1,1),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((9,9),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/52:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((1,1),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((1,1),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/53:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((2,2),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((2,2),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/54:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((3,3),(2,0))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/55:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax1 = plt.subplot2grid((3,3),(2,1))
ax1.plot(autotheft.groupby('timestamp').size(),'o-')
ax1.set_title('AutoTheft')
679/56:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(2,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
679/57:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
679/58:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1.5))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
679/59:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
679/60:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(Robbery.groupby('timestamp').size(),'o-')
ax2.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(homicide.groupby('timestamp').size(),'o-')
ax2.set_title('Homicide')
679/61:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(1,2))
ax4.plot(Robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(1,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
679/62:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,2))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(1,2))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(1,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
679/63:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(1,2))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(1,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
679/64:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(1,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
679/65:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
679/66:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.tight_layout(2)
plt.show()
679/67:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
679/68:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
679/69:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type_cleaned'].value_counts()
679/70:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
679/71: crime_data_2014["district"].value_counts()
679/72: crime_data_2014["timestamp"].value_counts()
679/73: crime_data_2014["primary_type"].value_counts()
679/74:
# Check length of set
len(crime_data_2014)
679/75:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
679/76:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
679/77: crime_data_2015.head()
679/78:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
679/79: crime_data_2015["primary_type"].value_counts()
679/80:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
679/81: crime_data_2015["timestamp"].value_counts()
679/82:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
679/83:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
679/84: crime_data_2016["timestamp"].value_counts()
679/85:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
679/86: crime_data_2016["primary_type"].value_counts()
679/87:
# Check length of set
len(crime_data_2015)
679/88:
# Check length of set
len(crime_data_2016)
679/89:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
679/90:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
679/91:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/92:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/93:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/94:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
679/95:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
679/96:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/97:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
679/98:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
679/99:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
679/100:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
679/101:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
679/102: crime_data_2016["timestamp"].value_counts()
679/103:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
679/104: crime_data_2016["primary_type"].value_counts()
679/105:
# Check length of set
len(crime_data_2015)
679/106:
# Check length of set
len(crime_data_2016)
679/107:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
679/108:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
679/109:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/110:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/111:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
679/112:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
679/113:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
679/114:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/115:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
679/116:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
679/117:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
679/118:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
679/119:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped
679/120:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('timestamp').sum()
grouped
679/121:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped
679/122:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/123:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped["primary_type"]["Theft"].plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/124:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped-theft = grouped["primary_type"]["Theft"]
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/125:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped_theft = grouped["primary_type"]["Theft"]
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
679/126:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped_theft = grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
679/127:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
679/128:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
grouped
#grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
679/129:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped
#grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
680/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
680/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
680/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
680/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
680/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
680/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
680/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
680/8: crime_data_2014["district"].value_counts()
680/9: crime_data_2014["timestamp"].value_counts()
680/10: crime_data_2014["primary_type"].value_counts()
680/11:
# Check length of set
len(crime_data_2014)
680/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
680/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
680/14: crime_data_2015.head()
680/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
680/16: crime_data_2015["primary_type"].value_counts()
680/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
680/18: crime_data_2015["timestamp"].value_counts()
680/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
680/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
680/21: crime_data_2016["timestamp"].value_counts()
680/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
680/23: crime_data_2016["primary_type"].value_counts()
680/24:
# Check length of set
len(crime_data_2015)
680/25:
# Check length of set
len(crime_data_2016)
680/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
680/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
680/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
680/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
680/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
680/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
680/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
680/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
680/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
680/35:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
680/36:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
680/37:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped
#grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
681/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
681/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
681/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
681/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
681/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
681/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
681/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/8: crime_data_2014["district"].value_counts()
681/9: crime_data_2014["timestamp"].value_counts()
681/10: crime_data_2014["primary_type"].value_counts()
681/11:
# Check length of set
len(crime_data_2014)
681/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
681/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
681/14: crime_data_2015.head()
681/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
681/16: crime_data_2015["primary_type"].value_counts()
681/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
681/18: crime_data_2015["timestamp"].value_counts()
681/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
681/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
681/21: crime_data_2016["timestamp"].value_counts()
681/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
681/23: crime_data_2016["primary_type"].value_counts()
681/24:
# Check length of set
len(crime_data_2015)
681/25:
# Check length of set
len(crime_data_2016)
681/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
681/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
681/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
681/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
681/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/35:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
681/36:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/37:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped
#grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
681/38:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
681/39:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
681/40:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
681/41:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
681/42:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
681/43: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
681/44:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/45: crime_data_2014["district"].value_counts()
681/46: crime_data_2014["timestamp"].value_counts()
681/47: crime_data_2014["primary_type"].value_counts()
681/48:
# Check length of set
len(crime_data_2014)
681/49:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
681/50:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
681/51: crime_data_2015.head()
681/52:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
681/53: crime_data_2015["primary_type"].value_counts()
681/54:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
681/55: crime_data_2015["timestamp"].value_counts()
681/56:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
681/57:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
681/58: crime_data_2016["timestamp"].value_counts()
681/59:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
681/60: crime_data_2016["primary_type"].value_counts()
681/61:
# Check length of set
len(crime_data_2015)
681/62:
# Check length of set
len(crime_data_2016)
681/63:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
681/64:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
681/65:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/66:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/67:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/68:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
681/69:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/70:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
681/71:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/72:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
681/73:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/74:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped
#grouped["primary_type"]["Theft"].count()
# grouped.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,10))
681/75:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
681/76:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
plt.grid()
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/77:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax.plt.grid()
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/78:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax.grid(color='r', linewidth=1,alpha=0.5)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/79:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax5.grid(color='r', linewidth=1,alpha=0.5)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/80:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.5)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/81:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/82:
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),'o-')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),'o-')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),'o-')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),'o-')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),'o-')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/83:
pylabs.rcParams['figure.figsize']=(14.0,8.0)
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/84:
from pylab import rcParams
rcParams['figure.figsize'] = 14, 10
rcParams['figure.figsize']=(14.0,8.0)
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/85:
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
rcParams['figure.figsize']=(14.0,8.0)
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/86:
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/87:
# This makes the figure's width 15 inches, and its height 12 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/88:
# This makes the figure's width 15 inches, and its height 12 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams
rcParams['figure.figsize'] = 15, 10
theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)
autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)
aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)
robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)
homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)
plt.figure(figsize=(10,10))
plt.tight_layout(2)
plt.show()
681/89:
# This makes the figure's width 15 inches, and its height 10 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams

rcParams['figure.figsize'] = 15, 10

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.figure(figsize=(10,10))
plt.tight_layout
plt.show()
681/90:
# This makes the figure's width 15 inches, and its height 10 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams

rcParams['figure.figsize'] = 15, 10

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.figure(figsize=(10,10))

plt.show()
681/91:
# This makes the figure's width 15 inches, and its height 10 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams

rcParams['figure.figsize'] = 15, 10

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)


plt.show()
681/92:
# This makes the figure's width 15 inches, and its height 10 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams

rcParams['figure.figsize'] = 15, 10

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/93:
# This makes the figure's width 15 inches, and its height 10 inches.
# The Figure class then uses this as the default value for one of its arguments.
from pylab import rcParams

rcParams['figure.figsize'] = 15, 10

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/94:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6 = plot(crime_data_2015.groupby["timestamp"].size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.get_xlim()=start,end
ax6.xaxis.set_ticks(np.arange(start,end,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/95:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6 = plot(crime_data_2015.groupby["timestamp"].size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(start,end,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/96:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby["timestamp"].size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(start,end,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/97:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(start,end,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/98:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(start,end,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/99:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax1.grid(linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/100:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/101:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax.set_xticklabels(ax.get_xticklabels(),rotation=90)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/102:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax.set_xticklabels(ax.get_xticklabels(),rotation=90)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
#ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/103:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax.set_xticklabels(ax.get_xticklabels(),rotation=90)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
#ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle='...', linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/104:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.set_xticklabels(ax.get_xticklabels(),rotation=90)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
#ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle='...', linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/105:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1),rotation=90)
ax6.grid(linestyle='...', linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/106:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle='...', linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/107:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle='---', linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/108:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=---, linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/109:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/110:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o',linecolor='k')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/111:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('All thefts in 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/112:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('Theft')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AutoTheft')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AggravatedAssault')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('Robbery')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('Homicide')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/113:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/114:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(0.5, 0.5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
...      verticalalignment='center', transform=ax.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/115:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(0.5, 0.5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/116:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(0.5, 0.5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax6.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/117:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(center, center, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax6.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/118:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(lower-left,upper-right center, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax6.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/119:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(lower-left,upper-right, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax6.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/120:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', transform=ax6.transAxes)
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/121:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='red', alpha=0.5))
#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/122:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/123:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o',facecolor='lightgrey')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o',facecolor='lightgrey')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o',facecolor='lightgrey')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o',facecolor='lightgrey')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o',facecolor='lightgrey')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o',facecolor='lightgrey')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/124:
#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/125:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/126:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/127:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/128:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
#p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
#p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
#p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/129:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
681/130:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(10,8))
681/131:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(10,8))
681/132:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(15,12))
681/133:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
681/134:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/135:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/136:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(0.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/137:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
681/138:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
681/139:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
681/140:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/141:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/142:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
681/143:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
681/144:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
681/145:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
681/146:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
681/147:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
681/148: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
681/149:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/150: crime_data_2014["district"].value_counts()
681/151:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/152:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/153:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/154:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/155:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
681/156: #crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
681/157:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/158: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
681/159:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
681/160: crime_data_2014["primary_type"].value_counts()
681/161:
# Check length of set
len(crime_data_2014)
681/162:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
681/163:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
681/164: crime_data_2015.head()
681/165:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
681/166: crime_data_2015["primary_type"].value_counts()
681/167:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
#p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
#p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/168:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='yellow')
#p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
#p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/169:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='blue')
#p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
#p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/170:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='yellow')
#p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/171:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='b')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='b')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='b')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/172:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/173:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/174:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/175:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.5, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.5, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.5, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.5, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.5, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/176:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.5, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.5, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.5, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.5, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.5, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.5, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.5, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.5, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.5, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.5, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.5, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/177:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
681/178:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='AggravatedAssault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/179:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='AutoTheft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/180:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/181:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/182:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/183:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/184:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/185:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/186:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/187:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
681/188:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
682/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
682/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
682/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
682/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
682/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
682/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
682/8: crime_data_2014["district"].value_counts()
682/9: crime_data_2014["timestamp"].value_counts()
682/10: crime_data_2014["primary_type"].value_counts()
682/11:
# Check length of set
len(crime_data_2014)
682/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
682/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
682/14: crime_data_2015.head()
682/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
682/16: crime_data_2015["primary_type"].value_counts()
682/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
682/18: crime_data_2015["timestamp"].value_counts()
682/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
682/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
682/21: crime_data_2016["timestamp"].value_counts()
682/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
682/23: crime_data_2016["primary_type"].value_counts()
682/24:
# Check length of set
len(crime_data_2015)
682/25:
# Check length of set
len(crime_data_2016)
682/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
682/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
682/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
682/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
682/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
682/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
682/35:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
682/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/37:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/39:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/40:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
682/41:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
682/42:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
682/43:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/44:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/45:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/46:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/47:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         #verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/48:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
682/49:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
682/50:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
682/51:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
682/52:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
682/53: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
682/54:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
682/55: crime_data_2014["district"].value_counts()
682/56: crime_data_2014["timestamp"].value_counts()
682/57: crime_data_2014["primary_type"].value_counts()
682/58:
# Check length of set
len(crime_data_2014)
682/59:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
682/60:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
682/61: crime_data_2015.head()
682/62:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
682/63: crime_data_2015["primary_type"].value_counts()
682/64:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
682/65: crime_data_2015["timestamp"].value_counts()
682/66:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
682/67:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
682/68: crime_data_2016["timestamp"].value_counts()
682/69:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
682/70: crime_data_2016["primary_type"].value_counts()
682/71:
# Check length of set
len(crime_data_2015)
682/72:
# Check length of set
len(crime_data_2016)
682/73:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
682/74:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
682/75:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/76:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/77:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/78:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
682/79:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
682/80:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
682/81:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
682/82:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
682/83:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/84:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/85:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         #verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/86:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/87:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
682/88:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
682/89:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
682/90:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
682/91:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
682/92: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
682/93:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
682/94:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
682/95:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
682/96: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
682/97:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
682/98: crime_data_2014["district"].value_counts()
682/99: crime_data_2014["timestamp"].value_counts()
682/100: crime_data_2014["primary_type"].value_counts()
682/101:
# Check length of set
len(crime_data_2014)
682/102:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
682/103:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
682/104: crime_data_2015.head()
682/105:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
682/106: crime_data_2015["primary_type"].value_counts()
682/107:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
682/108: crime_data_2015["timestamp"].value_counts()
682/109:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
682/110:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
682/111: crime_data_2016["timestamp"].value_counts()
682/112:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
682/113: crime_data_2016["primary_type"].value_counts()
682/114:
# Check length of set
len(crime_data_2015)
682/115:
# Check length of set
len(crime_data_2016)
682/116:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
682/117:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
682/118:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/119:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/120:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
682/121:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
682/122:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
682/123:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
682/124:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
682/125:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
682/126:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/127:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/128:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

#plt.text(5,5, 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2016', horizontalalignment='center',
         #verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/129:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/130:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

pylab.gcf.text((5,5), 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/131:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

pylab.gcf.text((5,5), 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/132:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
682/133:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

pylab.gcf.text((5,5), 'AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/134:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/135:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS OF 2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/136:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS OF 2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/137:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10


plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/138:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/139:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.show()
682/140:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
fig.tight_layout()
plt.show()
682/141:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
fig.tight_layout()
fig.subplots_adjust(top=0.88)
plt.show()
682/142:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5,fontsize=16))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
fig.tight_layout()
fig.subplots_adjust(top=0.88)
plt.show()
682/143:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
fig.tight_layout()
fig.subplots_adjust(top=0.88)
plt.show()
682/144:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()
682/145:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10


plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5))

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()
682/146:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10


plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()
682/147:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),figsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()
682/148:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()
682/149:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=2)
plt.show()
682/150:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=1)
plt.show()
682/151:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/152:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/153:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10


plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results
# but requires the title to be spaced accordingly
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/154:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.xaxis.set_labels(1,2,3,4,5,6,7,8,9,10,11,12)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/155:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.xaxis_labels(1,2,3,4,5,6,7,8,9,10,11,12)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/156:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/157:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/158:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/159:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
ax6.xaxis.set_ticks(np.arange(1,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/160:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10


plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/161:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
x_ticks = np.arange(0,13,1)
ax6.set_xticks(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/162:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
x_ticks = np.arange(0,13,1)
ax6.set_xticks(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(0,13,1)
ax1.set_xticks(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(0,13,1)
ax7.set_xticks(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(0,13,1)
ax2.set_xticks(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(0,13,1)
ax3.set_xticks(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(0,13,1)
ax4.set_xticks(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(0,13,1)
ax5.set_xticks(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/163:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
x_ticks = np.arange(0,13,1)
ax6.set_xticks(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(0,13,1)
ax1.set_xticks(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(0,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(0,13,1)
ax2.set_xticks(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(0,13,1)
ax3.set_xticks(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(0,13,1)
ax4.set_xticks(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(0,13,1)
ax5.set_xticks(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/164:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
x_ticks = np.arange(0,13,1)
ax6.set_xticklabels(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(0,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(0,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(0,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(0,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(0,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(0,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/165:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
x_ticks = np.arange(1,13,1)
ax6.set_xticklabels(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/166:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
ax6.set_xticklabels(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/167:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
ax6.set_xticklabels(x_ticks)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

ax.set_yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/168:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
ax6.xaxis.set_ticks(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/169:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
#ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/170:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xticks.set_ticks(np.arange(1, 13, 1),labels,rotation='vertical')
#ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/171:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(1, 13, 1),labels,rotation='vertical')
#ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/172:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(1, 13, 1),labels)
#ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/173:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(labels)
#ax6.xaxis.set_ticks(np.arange(0,13,1))
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/174:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(1, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/175:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/176:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/177:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

ax.set_yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/178:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.set_yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/179:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/180:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
682/181:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/182:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/183:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/184:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/185:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.2)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/186:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/187:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1),rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/188:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/189:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
682/190:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/191:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/192:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/193:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/194:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/195:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/196:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/197:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/198:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1))
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/199:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8,'fontsize'=15})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/200:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8,'fontsize':15})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/201:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/202:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/203:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/204:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
682/205:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
684/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
684/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
684/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
684/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
684/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
684/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
684/8: crime_data_2014["district"].value_counts()
684/9: crime_data_2014["timestamp"].value_counts()
684/10: crime_data_2014["primary_type"].value_counts()
684/11:
# Check length of set
len(crime_data_2014)
684/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
684/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
684/14: crime_data_2015.head()
684/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
684/16: crime_data_2015["primary_type"].value_counts()
684/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
684/18: crime_data_2015["timestamp"].value_counts()
684/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
684/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
684/21: crime_data_2016["timestamp"].value_counts()
684/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
684/23: crime_data_2016["primary_type"].value_counts()
684/24:
# Check length of set
len(crime_data_2015)
684/25:
# Check length of set
len(crime_data_2016)
684/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
684/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
684/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
684/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
684/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
684/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
684/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
684/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
684/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
684/35:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
684/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
684/37:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
684/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
684/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
684/40:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/41:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/42:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/43:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/44:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
684/45:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
684/46:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='barh')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
684/47:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='scatter')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
684/48:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
686/1:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.4)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
686/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
686/3:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
686/4:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
686/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
686/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
686/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
686/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
686/9: crime_data_2014["district"].value_counts()
686/10: crime_data_2014["timestamp"].value_counts()
686/11:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.4)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
687/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
687/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
687/4:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
687/5:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
687/6: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
687/7:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
687/8: crime_data_2014["district"].value_counts()
687/9: crime_data_2014["timestamp"].value_counts()
687/10: crime_data_2014["primary_type"].value_counts()
687/11:
# Check length of set
len(crime_data_2014)
687/12:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
687/13:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
687/14: crime_data_2015.head()
687/15:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
687/16: crime_data_2015["primary_type"].value_counts()
687/17:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
687/18: crime_data_2015["timestamp"].value_counts()
687/19:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
687/20:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
687/21: crime_data_2016["timestamp"].value_counts()
687/22:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
687/23: crime_data_2016["primary_type"].value_counts()
687/24:
# Check length of set
len(crime_data_2015)
687/25:
# Check length of set
len(crime_data_2016)
687/26:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
687/27:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
687/28:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
687/29:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
687/30:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
687/31:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
687/32:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
687/33:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
687/34:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
687/35:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# #crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
687/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
687/37:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.4)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/40:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
687/41:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
687/42:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
687/43:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.5)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/44:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=2,alpha=0.5)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/45:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=2,alpha=0.8)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/46:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.8)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/47:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/48:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.2)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/49:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.4)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/50:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/51:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/52:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
687/53:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
687/54:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
687/55:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
# total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
687/56:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
# total_crimetype.sort_values(by='primary_type').head()
687/57:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype.sort_values(by='primary_type').head()
687/58:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype()
#total_crimetype.sort_values(by='primary_type').head()
687/59:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
687/60:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
crimetype = crime_data_merge_final.groupby("primary_type")
crimetype
total_crimetype = crimetype.sum()
total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
687/61:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
687/62:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
687/63:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].count()
687/64:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts(()
687/65:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
687/66: crime_data_2014["primary_type"].value_counts()
687/67: crime_data_2015["primary_type"].value_counts()
687/68: crime_data_2014["primary_type"].value_counts()
687/69:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
crime_data_2014.value_counts()
687/70:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
crime_data_2014['primary_type].value_counts()
687/71:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
crime_data_2014['primary_type].value_counts()
687/72:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
crime_data_2014['primary_type'].value_counts()
687/73:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
687/74:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
687/75:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
687/76:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
687/77:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
687/78: crime_data_2014['primary_type'].value_counts()
687/79:
crime_data_2014["primary_type"].value_counts()
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
687/80:
crime_data_2014["primary_type"].value_counts()
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
687/81: crime_data_2015["primary_type"].value_counts()
687/82:
crime_data_2014["primary_type"].value_counts()
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
687/83:
crime_data_2014["primary_type"].value_counts()
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
688/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
688/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
688/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
688/4: crime_data_2014['primary_type'].value_counts()
688/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
688/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
688/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
688/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
688/9: crime_data_2014["district"].value_counts()
688/10: crime_data_2014["timestamp"].value_counts()
688/11: crime_data_2014["primary_type"].value_counts()
688/12:
# Check length of set
len(crime_data_2014)
688/13:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
688/14:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
688/15: crime_data_2015.head()
688/16:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
688/17: crime_data_2015["primary_type"].value_counts()
688/18:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
688/19: crime_data_2015["timestamp"].value_counts()
688/20:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
688/21:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
688/22: crime_data_2016["timestamp"].value_counts()
688/23:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
688/24: crime_data_2016["primary_type"].value_counts()
688/25:
# Check length of set
len(crime_data_2015)
688/26:
# Check length of set
len(crime_data_2016)
688/27:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
688/28:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
688/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
688/30:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
688/31:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
688/32:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
688/33:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
688/34:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
688/35:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
688/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
688/37:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
688/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
688/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
688/40:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
688/41:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
688/42:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
688/43:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
688/44:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
688/45:

df = df.groupby('Name').agg({'Sid':'first', 
                             'Use_Case': ', '.join, 
                             'Revenue':'first' }).reset_index()

crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
688/46:
crime_data_2014["primary_type"].value_counts()
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
688/47: crime_data_2015.["primary_type"].value_counts()
688/48: crime_data_2015["primary_type"].value_counts()
688/49: crime_data_2014["primary_type"].value_counts()
688/50:

#df = df.groupby('Name').agg({'Sid':'first', 
                             'Use_Case': ', '.join, 
                             'Revenue':'first' }).reset_index()

crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
688/51:

#df = df.groupby('Name').agg({'Sid':'first', 'Use_Case': ', '.join, 'Revenue':'first' }).reset_index()

crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
688/52:
crime_data_2014 = crime_data_2014["primary_type"]
help(replace)
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
688/53: crime_data_2014["primary_type"].value_counts()
690/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
690/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
690/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
690/4: crime_data_2014['primary_type'].value_counts()
690/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
690/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
690/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
690/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
690/9: crime_data_2014["district"].value_counts()
690/10: crime_data_2014["timestamp"].value_counts()
690/11: crime_data_2014["primary_type"].value_counts()
690/12:
# Check length of set
len(crime_data_2014)
690/13:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
690/14:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
690/15: crime_data_2015.head()
690/16:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
690/17: crime_data_2015["primary_type"].value_counts()
690/18:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
690/19: crime_data_2015["timestamp"].value_counts()
690/20:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
690/21:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
690/22: crime_data_2016["timestamp"].value_counts()
690/23:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
690/24: crime_data_2016["primary_type"].value_counts()
690/25:
# Check length of set
len(crime_data_2015)
690/26:
# Check length of set
len(crime_data_2016)
690/27:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
690/28:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
690/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
690/30:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
690/31:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
690/32:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
690/33:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
690/34:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
690/35:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
690/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
690/37:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
690/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
690/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
690/40:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
690/41:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
690/42:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
690/43:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
690/44:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
690/45:

#df = df.groupby('Name').agg({'Sid':'first', 'Use_Case': ', '.join, 'Revenue':'first' }).reset_index()

crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
690/46:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({})
df['Employer'] = df['Employer'].replace(
    {'Self Employed': 'Self-Employed', 'Self': 'Self-Employed'})
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
690/47: crime_data_2014["primary_type"].value_counts()
690/48: crime_data_2015["primary_type"].value_counts()
690/49:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
# crime_data_2015.to_csv(output_data_file_2015)
# crime_data_2016.to_csv(output_data_file_2016)
690/50: crime_data_2014["primary_type"].value_counts()
690/51: crime_data_2014["primary_type"].value_counts()
690/52: crime_data_2016["primary_type"].value_counts()
690/53:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
690/54:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
690/55:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
690/56:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder: 'Homicide'})
crime_data_2015["primary_type"].value_counts()
690/57:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
690/58:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
690/59:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.head()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
690/60:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
690/61:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
crime_data_merge_final(kind='bar', width=0.7)
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
690/62:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
crime_data_merge_final.plot(kind='bar', width=0.7)
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
691/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
691/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
691/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
691/4: crime_data_2014['primary_type'].value_counts()
691/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
691/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
691/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
691/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
691/9: crime_data_2014["district"].value_counts()
691/10: crime_data_2014["timestamp"].value_counts()
691/11: crime_data_2014["primary_type"].value_counts()
691/12:
# Check length of set
len(crime_data_2014)
691/13:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
691/14:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
691/15: crime_data_2015.head()
691/16:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
691/17: crime_data_2015["primary_type"].value_counts()
691/18:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
691/19: crime_data_2015["timestamp"].value_counts()
691/20:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
691/21:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
691/22: crime_data_2016["timestamp"].value_counts()
691/23:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
691/24: crime_data_2016["primary_type"].value_counts()
691/25:
# Check length of set
len(crime_data_2015)
691/26:
# Check length of set
len(crime_data_2016)
691/27:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
691/28:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
691/29:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
691/30:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
691/31:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
691/32:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
691/33:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
691/34:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
691/35:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
691/36:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
691/37:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
691/38:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
691/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
691/40:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
691/41:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
691/42:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
691/43:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
691/44:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
#grouped
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
691/45:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
691/46:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
691/47:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
691/48:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
crime_data_merge_final.plot(kind='bar', width=0.7)
plt.show()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
692/1:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
692/2:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
692/3:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
692/4:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
692/5: crime_data_2014['primary_type'].value_counts()
692/6:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
692/7:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
692/8: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
692/9:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
692/10: crime_data_2014["district"].value_counts()
692/11: crime_data_2014["timestamp"].value_counts()
692/12: crime_data_2014["primary_type"].value_counts()
692/13:
# Check length of set
len(crime_data_2014)
692/14:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
692/15:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
692/16: crime_data_2015.head()
692/17:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
692/18: crime_data_2015["primary_type"].value_counts()
692/19:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
692/20: crime_data_2015["timestamp"].value_counts()
692/21:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
692/22:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
692/23: crime_data_2016["timestamp"].value_counts()
692/24:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
692/25: crime_data_2016["primary_type"].value_counts()
692/26:
# Check length of set
len(crime_data_2015)
692/27:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
692/28:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
692/29:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
crime_data_2015["primary_type"].value_counts()
692/30:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
crime_data_2016["primary_type"].value_counts()
692/31:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
692/32:
# Check length of set
len(crime_data_2015)
692/33:
# Check length of set
len(crime_data_2016)
692/34:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
692/35:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
692/36:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
692/37:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
692/38:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
692/39:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
692/40:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
692/41:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
692/42:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
692/43:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
692/44:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
692/45:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
692/46:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
692/47:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
692/48:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
692/49:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
692/50:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
692/51:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
692/52:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final.count()
crime_data_merge_final.plot(kind='bar')
plt.show()
# crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
693/1:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
693/2:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# crime_data_merge_final.count()
# crime_data_merge_final.plot(kind='bar')
# plt.show()
# # crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
693/3:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
693/4:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
693/5:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
693/6: crime_data_2014['primary_type'].value_counts()
693/7:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
693/8:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
693/9: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
693/10:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
693/11: crime_data_2014["district"].value_counts()
693/12: crime_data_2014["timestamp"].value_counts()
693/13: crime_data_2014["primary_type"].value_counts()
693/14:
# Check length of set
len(crime_data_2014)
693/15:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
693/16:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
693/17:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
693/18: crime_data_2015.head()
693/19:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
693/20:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
693/21:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
693/22: crime_data_2015["timestamp"].value_counts()
693/23:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
693/24:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
693/25: crime_data_2016["timestamp"].value_counts()
693/26:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
693/27:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
693/28:
# Check length of set
len(crime_data_2015)
693/29:
# Check length of set
len(crime_data_2016)
693/30:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
693/31:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
693/32:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
693/33:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
693/34:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
693/35:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='hist')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
693/36:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
693/37:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
693/38:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
693/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
693/40:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
693/41:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
693/42:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
693/43:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
693/44:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
693/45:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
693/46:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
693/47:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
693/48:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# crime_data_merge_final.count()
# crime_data_merge_final.plot(kind='bar')
# plt.show()
# # crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
693/49:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('year').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
693/50:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
693/51:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
693/52:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
693/53:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
693/54:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.rcParams['figure.facecolor'] = 'black'
plt.figure(figsize=(100,50))
693/55:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
693/56:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
693/57:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
693/58:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
693/59:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
693/60:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.
plt.figure(figsize=(100,50))
693/61:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
694/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
694/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
694/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
694/4: crime_data_2014['primary_type'].value_counts()
694/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
694/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
694/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
694/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
694/9: crime_data_2014["district"].value_counts()
694/10: crime_data_2014["timestamp"].value_counts()
694/11: crime_data_2014["primary_type"].value_counts()
694/12:
# Check length of set
len(crime_data_2014)
694/13:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
694/14:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
694/15:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
694/16: crime_data_2015.head()
694/17:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
694/18:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
694/19:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
694/20: crime_data_2015["timestamp"].value_counts()
694/21:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
694/22:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
694/23: crime_data_2016["timestamp"].value_counts()
694/24:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
694/25:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
694/26:
# Check length of set
len(crime_data_2015)
694/27:
# Check length of set
len(crime_data_2016)
694/28:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
694/29:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
694/30:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
694/31:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
694/32:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
694/33:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/34:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
694/35:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
694/36:
# N = 5
# menMeans = (20, 35, 30, 35, 27)
# womenMeans = (25, 32, 34, 20, 25)
# menStd = (2, 3, 4, 1, 2)
# womenStd = (3, 5, 2, 3, 3)
# ind = np.arange(N)    # the x locations for the groups
# width = 0.35       # the width of the bars: can also be len(x) sequence

# p1 = plt.bar(timestamp,'Theft', width, color='#d62728', yerr=menStd)
# p2 = plt.bar(ind, womenMeans, width,
#              bottom=menMeans, yerr=womenStd)

# plt.ylabel('Scores')
# plt.title('Scores by group and gender')
# plt.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
# plt.yticks(np.arange(0, 81, 10))
# plt.legend((p1[0], p2[0]), ('Men', 'Women'))

# plt.show()

x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/37:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
694/38:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide: Murder & Nonnegligent Manslaughter']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/39:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/40:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/41:
crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
crime_data_merge_final["primary_type"].value_counts()
694/42:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Murder']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/43:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/44:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/45:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='bar', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
694/46:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# crime_data_merge_final.count()
# crime_data_merge_final.plot(kind='bar')
# plt.show()
# # crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
694/47:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/48:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/49:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby(''primary_type'').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/50:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/51:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
#grouped
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/52:
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/53:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/54:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/55:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/56:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2014, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout()
plt.show()
694/57:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line', width=0.7)
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
694/58:
# crime_data_merge = pd.merge(crime_data_2014,crime_data_2015, how ="outer")
# crime_data_merge_final = pd.merge(crime_data_merge,crime_data_2016, how ="outer")
# crime_data_merge_final.count()
# crime_data_merge_final.plot(kind='bar')
# plt.show()
# # crimetype = crime_data_merge_final.groupby("primary_type")
# crimetype
# total_crimetype = crimetype.sum()
# total_crimetype
#total_crimetype.sort_values(by='primary_type').head()
694/59:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8},fontsize = 15)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(0.8,0.5), loc="center",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,10))
plt.tight_layout(0.8)
plt.show()
694/60:
from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/61:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2016 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2016', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid

ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2016.groupby('timestamp').size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2016')
x_ticks = np.arange(1,13,1)
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2016[crime_data_2016['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2016[crime_data_2016['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2016[crime_data_2016['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2016[crime_data_2016['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2016[crime_data_2016['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2016[crime_data_2016['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

#plt.yticks("NO. OF CRIMES")

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/62:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2015 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

from pylab import rcParams

# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2015', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2015.groupby("timestamp").size(),marker='o', color="r")
ax6.set_title('ALL CRIMES IN 2015')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2015[crime_data_2015['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
x_ticks = np.arange(1,13,1)
ax1.set_xticklabels(x_ticks)
ax1.grid(linewidth=1,alpha=0.2)

burglary = crime_data_2015[crime_data_2015['primary_type']=='Burglary']
ax7 = plt.subplot2grid((3,3),(2,0))
ax7.plot(burglary.groupby('timestamp').size(),marker='o')
ax7.set_title('BURGLARY')
x_ticks = np.arange(1,13,1)
ax7.set_xticklabels(x_ticks)
ax7.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2015[crime_data_2015['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
x_ticks = np.arange(1,13,1)
ax2.set_xticklabels(x_ticks)
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2015[crime_data_2015['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
x_ticks = np.arange(1,13,1)
ax3.set_xticklabels(x_ticks)
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2015[crime_data_2015['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
x_ticks = np.arange(1,13,1)
ax4.set_xticklabels(x_ticks)
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2015[crime_data_2015['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
x_ticks = np.arange(1,13,1)
ax5.set_xticklabels(x_ticks)
ax5.grid(linewidth=1,alpha=0.2)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/63:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Aggravated Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/64:
# This makes the figure's width 15 inches, and its height 10 inches.
rcParams['figure.figsize'] = 15, 10

plt.suptitle('AUSTIN CRIME INCIDENTS BY MONTHS\n2014', horizontalalignment='center',
         verticalalignment='center', bbox=dict(facecolor='lightgrey', alpha=0.5),fontsize=16)

#plt.subplot2grid - Creates an axis at specific location inside a regular grid.
#Used here in order to place several different plots in specific locations within a grid
ax6 = plt.subplot2grid((3,3),(0,0),colspan=3)
ax6.plot(crime_data_2014.groupby("timestamp").size(),marker='o',color='r')
ax6.set_title('ALL CRIMES IN 2014')
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
ax6.xaxis.set_ticks(np.arange(0, 13, 1))
ax6.set_xticklabels(labels)
ax6.grid(linestyle=":", linewidth=1,alpha=0.5)

theft = crime_data_2014[crime_data_2014['primary_type']=='Theft']
ax1 = plt.subplot2grid((3,3),(1,0))
ax1.plot(theft.groupby('timestamp').size(),marker='o')
ax1.set_title('THEFT')
ax1.grid(linewidth=1,alpha=0.2)

autotheft = crime_data_2014[crime_data_2014['primary_type']=='Auto Theft']
ax2 = plt.subplot2grid((3,3),(1,1))
ax2.plot(autotheft.groupby('timestamp').size(),marker='o')
ax2.set_title('AUTOTHEFT')
ax2.grid(linewidth=1,alpha=0.2)

aggravatedassault = crime_data_2014[crime_data_2014['primary_type']=='Agg Assault']
ax3 = plt.subplot2grid((3,3),(1,2))
ax3.plot(aggravatedassault.groupby('timestamp').size(),marker='o')
ax3.set_title('AGGRAVATED ASSAULT')
ax3.grid(linewidth=1,alpha=0.2)

robbery = crime_data_2014[crime_data_2014['primary_type']=='Robbery']
ax4 = plt.subplot2grid((3,3),(2,1))
ax4.plot(robbery.groupby('timestamp').size(),marker='o')
ax4.set_title('ROBBERY')
ax4.grid(linewidth=1,alpha=0.2)

homicide = crime_data_2014[crime_data_2014['primary_type']=='Homicide']
ax5 = plt.subplot2grid((3,3),(2,2))
ax5.plot(homicide.groupby('timestamp').size(),marker='o')
ax5.set_title('HOMICIDE')
ax5.grid(linewidth=1,alpha=0.2)

# Tight layout often produces nice results but requires the title to be spaced accordingly
# and in to avoid overlapping labels
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/65:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/66:
slimmed_data = crime_data_2016[['primary_type', 'year']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('year').sum()
694/67:
slimmed_data = crime_data_2016[['primary_type', 'year']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('year').sum()
grouped
694/68:
slimmed_data = crime_data_2016[['primary_type', 'year']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby(primary_type').sum()
grouped
694/69:
slimmed_data = crime_data_2016[['primary_type', 'year']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('primary_type').sum()
grouped
694/70:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'year']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/71:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/72:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='yellow')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/73:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [28143, 0, 2162, 1827, 839, 25]
values_2015 = [26624, 4846, 1982, 1829, 826, 18]
values_2016 = [24911, 5039, 2092, 1990, 911, 30]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/74:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/75:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort()
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/76:
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort()
694/77:
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=True)
694/78:
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=True)
values_2014
694/79:
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=False)
values_2014
694/80:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=False)
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2014.sort(reverse=False)
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2014.sort(reverse=False)
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/81:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Theft','Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=False)
values_2014
694/82:
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2014.sort(reverse=False)
values_2015
694/83:
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2015.sort(reverse=False)
values_2015
694/84:
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2014.sort(reverse=False)
values_2015
694/85:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/86:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=False)
values_2014
694/87:
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2015.sort(reverse=False)
values_2015
694/88:
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2014.sort(reverse=False)
values_2015
694/89:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/90:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/91:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/92:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/93:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/94:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/95:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/96:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
#p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/97:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='orange')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], width=0.3, color='orange')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
#plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/98:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], width=0.3, color='orange')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/99:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='orange')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/100:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='blue')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='orange')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='blue')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='orange')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='blue')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='orange')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/101:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/102:
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2016.sort(reverse=False)
values_2016
694/103:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/104:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
694/105:
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2015.sort(reverse=False)
values_2015
694/106:
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2016.sort(reverse=False)
values_2016
694/107:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/108:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
694/109:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2014.sort(reverse=False)
values_2014
694/110:
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2015.sort(reverse=False)
values_2015
694/111:
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
values_2016.sort(reverse=False)
values_2016
694/112:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/113:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Burglary', 'Auto\nTheft', 'Aggravated Assault', 'Robbery', 'Homicide','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [0, 2162, 1827, 839, 25, 28143]
values_2015 = [4846, 1982, 1829, 826, 18, 26624]
values_2016 = [5039, 2092, 1990, 911, 30, 24911]
694/114:
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/115:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/116:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/117:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/118:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/119:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/120:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/121:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/122:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/123:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks('2014','2015','2016')
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/124:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks(x_loc)
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/125:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.value_counts()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# # p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# # p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# # p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# # p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# # p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# # p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# # p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# # p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# # p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# # p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# # p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# # p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/126:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['primary_type].value_counts()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# # p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# # p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# # p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# # p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# # p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# # p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# # p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# # p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# # p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# # p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# # p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# # p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/127:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['primary_type'].value_counts()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# # p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# # p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# # p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# # p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# # p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# # p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# # p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# # p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# # p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# # p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# # p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# # p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/128:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['timestamp'].value_counts()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# # p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# # p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# # p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# # p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# # p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# # p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# # p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# # p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# # p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# # p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# # p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# # p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/129:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# # p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# # p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# # p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# # p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# # p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# # p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# # p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# # p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# # p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# # p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# # p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# # p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/130:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/131:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.sort_index('timestamp')
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/132:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.sort_values(by=['timestamp'])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/133:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/134:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.sort('timestamp')
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/135:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 = crimes_popl_2016.sort('timestamp')
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/136:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
ref['index'] = ref['timestamp'].apply(lambda x: int(x.split('\t')[0]))
ref.sort_values(by='index')
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/137:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
ref['index'] = ref['timestamp'].apply(lambda x: int(x.split('\t')[0]))
ref.sort_values(by='index')
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/138:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[True])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/139:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/140:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
694/141:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
# test = pd.get_dummies(slimmed_data, columns=['primary_type'])
# grouped = test.groupby('timestamp').sum()
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/142:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
# grouped = test.groupby('timestamp').sum()
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/143:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/144:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('primary_type').sum()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/145:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['timestamp'])
test
grouped = test.groupby('primary_type').sum()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/146:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/147:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/148:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_values(by=['timestamp'], ascending=[False])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/149:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_values(by=['timestamp'],kind='quicksort')
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/150:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_values(by=['timestamp'],kind='mergesort')
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/151:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
# grouped = test.groupby('timestamp').sum()
# grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/152:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp')
grouped
# grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/153:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp')
grouped.sum()
# grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/154:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp')
grouped.sum()
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/155:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp')
grouped
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/156:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/157:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum().reset_index()
grouped
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/158:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum().set_index()
grouped
grouped.sort_values(by=['timestamp'])
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/159:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum().set_index()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/160:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum().reset_index()
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/161:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum().reset_index()
grouped['timestamp']
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/162:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped['timestamp']
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/163:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped['timestamp']
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/164:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
#grouped['timestamp']
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/165:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped['timestamp'].sort_values
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/166:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped['timestamp'].sort_values
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/167:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped['timestamp']
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/168:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped[['timestamp']]
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/169:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_values(by=SAME_KEY)
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/170:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_index(axis='timestamp')
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/171:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_index(axis=timestamp)
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/172:
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.sort_index(axis='timestamp')
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/173:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped..reindex(labels= months)
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/174:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# # crimes_popl_2016.plot(kind='line')
# # plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# # plt.xlabel('TIMESTAMP')
# # plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# # labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
# # plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
# # plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# # plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# # plt.figure(figsize=(20,30))
694/175:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
694/176:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
694/177:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
694/178:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(1, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
694/179:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(100,50))
694/180:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
694/181:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
694/182:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()

plt.show()
694/183:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
694/184:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1.8,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/185:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/186:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/187:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/188:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/189:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/190:
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/191:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/192:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2015 = (grouped /Austin_Population)
# crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/193:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2015 = (grouped /Austin_Population)
# crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/194:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2016 = (grouped /Austin_Population)
# crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/195:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/196:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016 
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/197:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
slimmed_data
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
test
grouped = test.groupby('timestamp').sum()
grouped
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/198:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/199:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
# crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/200:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
crimes_popl_2016.sort_values(by=['timestamp'], ascending=[False])
# crimes_popl_2016.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/201:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(incident_type_2016, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/202:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/203:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/204:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(1, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/205:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/206:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/207:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
grouped
# Austin_Population = 931,830
# Austin_Population = 931830 / 100000
# crimes_popl_2015 = (grouped /Austin_Population)
# crimes_popl_2015
# crimes_popl_2015.plot(kind='line')
# plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
# plt.xlabel('TIMESTAMP')
# plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
# plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
# plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], 
#            bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
# plt.figure(figsize=(20,30))
# plt.tight_layout()
# plt.show()
694/208:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
grouped
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], 
           bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/209:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
#plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/210:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1.2,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
694/211:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
694/212:
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
694/213:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
694/214:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016

values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks(x_loc)
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/215:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016

values_2014 = [25,839,1827,2162,0,28143]
values_2015 = [18,826,1829,1982,4846,26624]
values_2016 = [30,911,1990,2092,5039,24911]
p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
plt.xticks(x_loc)
plt.ylabel('CRIME RATE')
plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.show()
694/216:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/217:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['pimary_type'].value_counts(s)
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/218:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['pimary_type'].value_counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
694/219:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['primary_type'].value_counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/1:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
695/2:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
695/3:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
695/4: crime_data_2014['primary_type'].value_counts()
695/5:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
695/6:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
695/7: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
695/8:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
695/9: crime_data_2014["district"].value_counts()
695/10: crime_data_2014["timestamp"].value_counts()
695/11: crime_data_2014["primary_type"].value_counts()
695/12:
# Check length of set
len(crime_data_2014)
695/13:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
695/14:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
695/15:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
695/16: crime_data_2015.head()
695/17:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
695/18:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
695/19:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
695/20: crime_data_2015["timestamp"].value_counts()
695/21:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
695/22:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
695/23: crime_data_2016["timestamp"].value_counts()
695/24:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
695/25:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
695/26:
# Check length of set
len(crime_data_2015)
695/27:
# Check length of set
len(crime_data_2016)
695/28:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
695/29:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
695/30:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/31:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/32:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/33:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/34:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
695/35:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
695/36:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
695/37: crime_data_2014['primary_type'].value_counts()
695/38:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
695/39:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
695/40: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
695/41:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
695/42: crime_data_2014["district"].value_counts()
695/43: crime_data_2014["timestamp"].value_counts()
695/44: crime_data_2014["primary_type"].value_counts()
695/45:
# Check length of set
len(crime_data_2014)
695/46:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['primary_type'].value_counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/47:
# Dependencies and Setup
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

# Raw data file
file_2014 = os.path.join('austin_crime.csv')
file_2015 = os.path.join('Annual_Crime_Dataset_2015.csv')
file_2016 = os.path.join('2016_Annual_Crime_Data.csv')

# Read crime report files and store into pandas data frame
crime_data_2014 = pd.read_csv(file_2014)
crime_data_2015 = pd.read_csv(file_2015)
crime_data_2016 = pd.read_csv(file_2016)
695/48:
# Check original length of csv file rows
print(len(crime_data_2014))
print("-------------")
print(len(crime_data_2015))
print("-------------")
print(len(crime_data_2016))
695/49:
# Turn 2014 crime csv into dataframe and drop unneccessary columns
crime_data_2014 = pd.DataFrame(crime_data_2014)
crime_data_2014 = crime_data_2014.drop("latitude",axis=1)
crime_data_2014 = crime_data_2014.drop("location",axis=1)
crime_data_2014 = crime_data_2014.drop("longitude",axis=1)
crime_data_2014 = crime_data_2014.drop("census_tract",axis=1)
crime_data_2014 = crime_data_2014.drop("unique_key",axis=1)
crime_data_2014 = crime_data_2014.drop("zipcode",axis=1)
crime_data_2014 = crime_data_2014.drop("clearance_date",axis=1)
crime_data_2014 = crime_data_2014.drop("council_district_code",axis=1)
crime_data_2014 = crime_data_2014.drop("description",axis=1)
crime_data_2014 = crime_data_2014.drop("location_description",axis=1)
crime_data_2014 = crime_data_2014.drop("address",axis=1)

# Drop all rows containing no values
crime_data_2014 = crime_data_2014.dropna()

# Extract only 2014 data
crime_data_2014 = crime_data_2014.drop(crime_data_2014[crime_data_2014.year == 2015.0].index)

# Reorder columns
crime_data_2014 = crime_data_2014[['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate', 'year']]
crime_data_2014.head()
695/50: crime_data_2014['primary_type'].value_counts()
695/51:
# Edit timestamp column to show only month
crime_data_2014['timestamp'] = crime_data_2014['timestamp'].map(lambda x: str(x)[5:7])
crime_data_2014.head()
695/52:
# Edit clearance status to display only first character
crime_data_2014['clearance_status'] = crime_data_2014['clearance_status'].map(lambda x: str(x)[0:1])
crime_data_2014.head()
695/53: crime_data_2014['primary_type_cleaned'] = crime_data_2014['primary_type']
695/54:
import re
regex_pat = re.compile(r'^Theft:.*$', flags=re.IGNORECASE)
crime_data_2014["primary_type_cleaned"].replace(regex_pat, "Theft", inplace=True)

crime_data_2014["primary_type"] = crime_data_2014["primary_type_cleaned"]
crime_data_2014 = crime_data_2014.drop("primary_type_cleaned",axis=1)
crime_data_2014['primary_type'].value_counts()
695/55: crime_data_2014["district"].value_counts()
695/56: crime_data_2014["timestamp"].value_counts()
695/57: crime_data_2014["primary_type"].value_counts()
695/58:
# Check length of set
len(crime_data_2014)
695/59:
crime_data_2014["primary_type"] = crime_data_2014["primary_type"].replace({'Aggravated Assault': 'Agg Assault', 
                                         'Homicide: Murder & Nonnegligent Manslaughter': 'Homicide'})
crime_data_2014["primary_type"].value_counts()
695/60:
# Turn 2015 and 2016 crime csv into dataframes and drop unneccessary columns
crime_data_2015 = pd.DataFrame(crime_data_2015)
crime_data_2016 = pd.DataFrame(crime_data_2016)

crime_data_2015 = crime_data_2015.drop("GO Primary Key",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Primary Key",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Highest Offense Desc",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Highest Offense Desc",axis=1)

crime_data_2015 = crime_data_2015.drop("Clearance Date",axis=1)
crime_data_2016 = crime_data_2016.drop("Clearance Date",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location Zip",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location Zip",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Location",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Location",axis=1)

crime_data_2015 = crime_data_2015.drop("GO Census Tract",axis=1)
crime_data_2016 = crime_data_2016.drop("GO Census Tract",axis=1)

crime_data_2015 = crime_data_2015.drop("Council District",axis=1)
crime_data_2016 = crime_data_2016.drop("Council District",axis=1)

# Drop all rows containing no values
crime_data_2015 = crime_data_2015.dropna()
crime_data_2016 = crime_data_2016.dropna()
695/61:
# Rename columns for merge
crime_data_2015.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2015['year'] = 2015.0
695/62: crime_data_2015.head()
695/63:
# Edit timestamp column to show only month
for index, row in crime_data_2015.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2015.set_value(index, 'timestamp', split)
695/64:
crime_data_2015["primary_type"] = crime_data_2015["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2015["primary_type"].value_counts()
695/65:
crime_data_2015 = crime_data_2015[crime_data_2015.district != "UK"]
crime_data_2015["district"].value_counts()
695/66: crime_data_2015["timestamp"].value_counts()
695/67:
# Rename columns for merge
crime_data_2016.columns = ['primary_type', 'timestamp', 'clearance_status', 'district', 'x_coordinate', 'y_coordinate']

# Add year column
crime_data_2016["year"] = 2016.0
695/68:
# Edit timestamp column to show only month
for index, row in crime_data_2016.iterrows():
    split = row['timestamp']
    split = split.split('-')[1]
    crime_data_2016.set_value(index, 'timestamp', split)
695/69: crime_data_2016["timestamp"].value_counts()
695/70:
crime_data_2016 = crime_data_2016[crime_data_2016.district != "88"]
crime_data_2016 = crime_data_2016[crime_data_2016.district != "UK"]
crime_data_2016["district"].value_counts()
695/71:
crime_data_2016["primary_type"] = crime_data_2016["primary_type"].replace({'Murder': 'Homicide'})
crime_data_2016["primary_type"].value_counts()
695/72:
# Check length of set
len(crime_data_2015)
695/73:
# Check length of set
len(crime_data_2016)
695/74:
output_data_file_2014 = "clean_csvs/crime_2014.csv"
output_data_file_2015 = "clean_csvs/crime_2015.csv"
output_data_file_2016 = "clean_csvs/crime_2016.csv"
695/75:
crime_data_2014.to_csv(output_data_file_2014)
crime_data_2015.to_csv(output_data_file_2015)
crime_data_2016.to_csv(output_data_file_2016)
695/76:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/77:
incident_type_2014 = ['Theft','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [28143, 2162, 1827, 839, 25]
colors = ["Black", "red", "yellow", "purple","orange"]
#plt.legend(incident_type_2014, title="INCIDENT TYPES",loc="upper right")
plt.title("AUSTIN INCIDENTS IN 2014",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2014, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2015 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [26624, 4846, 1982, 1829, 826, 18]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2015",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2015, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2015, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/78:
incident_type_2016 = ['Theft','Burglary','Auto\nTheft', 'Aggravated\nAssault', 'Robbery', 'Homicide']
values = [24911, 5039, 2092, 1990, 911, 30]
colors = ["Black", "red", "yellow", "purple","orange","lavender"]
plt.title("AUSTIN INCIDENTS IN 2016",bbox={'facecolor':'15', 'pad':8})
pie=plt.pie(values,labels=incident_type_2016, colors=colors,
         autopct="%0.1f%%", shadow=True, startangle=175, pctdistance=0.5,
        labeldistance=1.2,radius=0.8,textprops={"color":"b"},wedgeprops = { 'linewidth': 1, "edgecolor" :"k" })
plt.legend(pie[0],incident_type_2014, bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.axis("equal")
plt.show()
695/79:
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/80:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2015[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2015 = (grouped /Austin_Population)
crimes_popl_2015.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2015',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], 
           bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1)
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/81:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2016)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Burglary','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/82:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['primary_type'].value_counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/83:
# LINE GRAPH PLOTTING OF ALL CRIMES BY MONTHS IN 2014 IN AUSTIN AND PLOTS OF EACH CRIME BY MONTH

#With this dictionary, we can control the defaults of almost every property in matplotlib: figure size and dpi, 
#line width, color and style, axes, axis and grid properties, text and font properties and so on.
from pylab import rcParams
695/84:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/85:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['timestamp'].value-counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/86:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['timestamp'].value_counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/87:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.counts()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/88:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.count()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/89:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016['timestamp'].sum()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/90:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/91:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/92:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['timestamp'].sum()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/93:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/94:
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/95:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),labels,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/96:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/97:
months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
slimmed_data = crime_data_2014[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
grouped.reindex(labels= months)
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2014 = (grouped/Austin_Population)
crimes_popl_2014.plot(kind='line')
plt.ylabel('CRIME RATE PER 100,000 INHABITANTS')
plt.xlabel('TIMESTAMP')
plt.title('CRIME DATA 2014)',bbox={'facecolor':'15', 'pad':8})
plt.xticks(np.arange(0, 13, 1),months,rotation='vertical')
plt.legend(['Agg Assault','Auto Theft','Homicide','Robbery','Theft'], bbox_to_anchor=(1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
plt.gca().yaxis.grid(True,color='k', linestyle='-', linewidth=0.1) 
# plt.savefig('crime_months_2014.png',bbox_inches='tight')
plt.figure(figsize=(20,30))
plt.tight_layout()
plt.show()
695/98:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
# crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/99:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/100:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']

slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
# crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/101:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reindex(labels= months)
# crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/102:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016
# crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/103:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
# crimes_popl_2016['timestamp'].count()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/104:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['primary_type_Agg Assault'].sum()

# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/105:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['primary_type_Agg Assault'].sum()
crimes_popl_2016['primary_type_Agg Assault'].sum()
crimes_popl_2016['primary_type_Agg Assault'].sum()
crimes_popl_2016['primary_type_Agg Assault'].sum()
# values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/106:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# crimes_popl_2016['primary_type_Agg Assault'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/107:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['primary_type_Agg Assault'].sum()
crimes_popl_2016['primary_type_Auto Theft'].sum()
crimes_popl_2016['primary_type_Burglary'].sum()
crimes_popl_2016['primary_type_Homicide'].sum()
crimes_popl_2016['primary_type_Robbery'].sum()
crimes_popl_2016['primary_type_Theft'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/108:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
crimes_popl_2016['primary_type_Agg Assault','primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Burglary'].sum()
# crimes_popl_2016['primary_type_Homicide'].sum()
# crimes_popl_2016['primary_type_Robbery'].sum()
# crimes_popl_2016['primary_type_Theft'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/109: mylist = [1,2,3,4,5,6,7,8,9,10]
695/110:
for item in mylist:
    print("LA LA LA LA")
695/111:
for item in mylist:
    if item %2 == 0:
        print(item)
695/112:
for item in mylist:
    if item %2 == 0:
        print(item)
    else:
        print("ODD")
695/113:
for _ in "Hello owrld":
    print("asdsdsad")
695/114:
for _ in "Hello owrld":
    print("asdsdsad")
695/115: len(mylist)
695/116: mylist[10] = 'new item'
695/117: mylist[19] = 'new item'
695/118: mylist[9] = 'new item'
695/119:
mylisttuples = [(1,2),(3,4),(4,5)]
for item in mylisttuples:
    print(item)
695/120:
for (x,y) in mylisttuples:
    print(x,y)
695/121:
for (_,_) in mylisttuples:
    print(xs,y)
695/122:
for (_,_) in mylisttuples:
    print(x,y)
695/123:
mydict = {'k1':1,'k2':2}
for item in mydict:
    print(item)
695/124:
mydict = {'k1':1,'k2':2}
for item in mydict.item:
    print(item)
695/125:
mydict = {'k1':1,'k2':2}
for item in mydict.items:
    print(item)
695/126:
mydict = {'k1':1,'k2':2}
for item in mydict.items(:
    print(item)
695/127:
mydict = {'k1':1,'k2':2}
for item in mydict.items():
    print(item)
695/128:
for i in range(10):
    print(i)
695/129:
for i in range(10):
    print(str(i)* i)
695/130:
count = 1
for i in range(10):
    print(str(i)* i)
    for j in range(0,i):
        count = count+1
695/131:
for i in range(1,3):
    for j in range(1,2):
        print("*")
695/132:
x = 0
while x<=5:
    print(valueof x is: x)
    x = x+1
695/133:
x = 0
while x<=5:
    print('valueof x is:'' x)
    x = x+1
695/134:
x = 0
while x<=5:
    print('valueof x is:' x)
    x = x+1
695/135:
x = 0
while x<=5:
    print('valueof x is:', x)
    x = x+1
695/136:
x = 0
while x<5:
    print('valueof x is:', x)
    x = x+1
695/137:
x = 0
while x<5:
    print('valueof x is:', x)
    x = x+1
else:
    print('oops!!')
695/138:
x=[1,2,3]
for item in x:
695/139:
x=[1,2,3]
for item in x:
    pass
print("end of for loop")
695/140:
mystring = "python"
for letter in mystring:
    print(letter)
695/141:
for letter in mystring:
    if letter =='n':
        continue
    print(letter)
695/142:
for letter in mystring:
    if letter =='n':
        continue
    print(letter)
print("YAYY")
695/143:
for letter in mystring:
    if letter =='n':
    print(letter)
print("YAYY")
695/144:
for letter in mystring:
    if letter =='n':
        continue
    print(letter)
print("YAYY")
695/145:
for letter in mystring:
    if letter =='p':
        continue
    print(letter)
print("YAYY")
695/146:
for letter in mystring:
    if letter == 'o':
        break
    print(letter)
695/147:
for letter in mystring:
    if letter == 'o':
        break
    print(letter)
print("YOYOH")
695/148:
x=[1,2,3]
for item in x:
    pass
    print("DJHJHJ")
print("end of for loop")
695/149:
x=[1,2,3]
for item in x:
    pass
    print("
          items")
print("end of for loop")
695/150:
x=[1,2,3]
for item in x:
    pass
    print("item")
print("end of for loop")
695/151:
numitem = 1
numitem/0
695/152:
num1 = 10
num2=input("pleas gimme a num:")
695/153:
try:
    result = 10 +'50'
except:
    print("Please correct")
else:
    print("result")
695/154:
try:
    result = 10 +'50'
except:
    print("Please correct")
else:
    print("result")
finally:
    print("HEy")
695/155: import numpy as np
695/156: np.zeros(2)
695/157: np.zeros((2,3))
695/158: np.one(5)
695/159: np.ones(5)
695/160: np.zeros((3,3,3))
695/161: np.zeros((3,3,33,4,45))
695/162: np.twos((3,3,3))
695/163: np.twos((3,3,3))
695/164: np.arange((3,3,3))
695/165: np.arange((3,3,3)
695/166: np.arange((3,3,3))
695/167: np.arange(10)
695/168: np.ones((2,3))
695/169: np.ones((2,3))*5
695/170: x = [[1,2],[3,4],[5,6]]
695/171: np(x)
695/172: np.x
695/173: np.linspace(4,15)
695/174: np.linspace(1,3)
695/175: np.linspace(1,3,2)
695/176: np.linspace(1,50)
695/177: np.eye(3)
695/178: np.rand.rand(4)
695/179: np.rand.rand(4)
695/180: np.random.rand(4)
695/181: np.random.rand(4,3)
695/182: np.random.randint(4,3)
695/183: np.random.randint(4,30)
695/184: np.random.randn(10)
695/185: np.random.randn(1,10)
695/186: np.random.randint(4,30)
695/187: np.random.randint(30)
695/188: np.arange(20)
695/189: arr = np.arange(20)
695/190: arr.ndim
695/191: len(arr)
695/192: arr.size
695/193: arr.shape
695/194: arr.reshape(5,5)
695/195: arr.reshape(4,4)
695/196: arr.reshape(3,3)
695/197:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_popl_2016.reset_index()
#crimes_popl_2016['primary_type_Agg Assault','primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Burglary'].sum()
# crimes_popl_2016['primary_type_Homicide'].sum()
# crimes_popl_2016['primary_type_Robbery'].sum()
# crimes_popl_2016['primary_type_Theft'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/198:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_2016scrimes_popl_2016.reset_index()
#crimes_popl_2016['primary_type_Agg Assault','primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Burglary'].sum()
# crimes_popl_2016['primary_type_Homicide'].sum()
# crimes_popl_2016['primary_type_Robbery'].sum()
# crimes_popl_2016['primary_type_Theft'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/199:
months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
x_loc=['2014','2015','2016']
incident_type_2014 = ['Homicide','Robbery','Aggravated Assault','Auto\nTheft','Burglary','Theft']
slimmed_data = crime_data_2016[['primary_type', 'timestamp']]
test = pd.get_dummies(slimmed_data, columns=['primary_type'])
grouped = test.groupby('timestamp').sum()
Austin_Population = 931,830
Austin_Population = 931830 / 100000
crimes_popl_2016 = (grouped /Austin_Population)
crimes_2016 = crimes_popl_2016.reset_index()
crimes_2016
#crimes_popl_2016['primary_type_Agg Assault','primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Auto Theft'].sum()
# crimes_popl_2016['primary_type_Burglary'].sum()
# crimes_popl_2016['primary_type_Homicide'].sum()
# crimes_popl_2016['primary_type_Robbery'].sum()
# crimes_popl_2016['primary_type_Theft'].sum()
# # values_2014 = [25,839,1827,2162,0,28143]
# values_2015 = [18,826,1829,1982,4846,26624]
# values_2016 = [30,911,1990,2092,5039,24911]
# p1_2014 = plt.bar(x_loc[0],values_2014[0], width=0.3, color='orange')
# p2_2014 = plt.bar(x_loc[0], values_2014[1], bottom=values_2014[0], width=0.3, color='lightblue')
# p3_2014 = plt.bar(x_loc[0], values_2014[2], bottom=values_2014[1], width=0.3, color='red')
# p4_2014 = plt.bar(x_loc[0], values_2014[3], bottom=values_2014[2], width=0.3, color='lightgreen')
# p5_2014 = plt.bar(x_loc[0], values_2014[4], bottom=values_2014[3], width=0.3, color='purple')
# p6_2014 = plt.bar(x_loc[0], values_2014[5], bottom=values_2014[4], width=0.3, color='blue')
# p1_2015 = plt.bar(x_loc[1],values_2015[0], width=0.3, color='orange')
# p2_2015 = plt.bar(x_loc[1], values_2015[1], bottom=values_2015[0], width=0.3, color='lightblue')
# p3_2015 = plt.bar(x_loc[1], values_2015[2], bottom=values_2015[1], width=0.3, color='red')
# p4_2015 = plt.bar(x_loc[1], values_2015[3], bottom=values_2015[2], width=0.3, color='lightgreen')
# p5_2015 = plt.bar(x_loc[1], values_2015[4], bottom=values_2015[3], width=0.3, color='purple')
# p6_2015 = plt.bar(x_loc[1], values_2015[5], bottom=values_2015[4], width=0.3, color='blue')
# p1_2016 = plt.bar(x_loc[2],values_2016[0], width=0.3, color='orange')
# p2_2016 = plt.bar(x_loc[2], values_2016[1], bottom=values_2016[0], width=0.3, color='lightblue')
# p3_2016 = plt.bar(x_loc[2], values_2016[2], bottom=values_2016[1], width=0.3, color='red')
# p4_2016 = plt.bar(x_loc[2], values_2016[3], bottom=values_2016[2], width=0.3, color='lightgreen')
# p5_2016 = plt.bar(x_loc[2], values_2016[4], bottom=values_2016[3], width=0.3, color='purple')
# p6_2016 = plt.bar(x_loc[2], values_2016[5], bottom=values_2016[4], width=0.3, color='blue')
# plt.xticks(x_loc)
# plt.ylabel('CRIME RATE')
# plt.title('CRIME RATE FROM 2014-2016 IN CITY OF AUSTIN')
# plt.legend(incident_type_2014, bbox_to_anchor=(1.1,0.5), loc="right",bbox_transform=plt.gcf().transFigure)
# plt.show()
695/200: arr1.max()
695/201: arr1 = np.random.rand(4,3)
695/202: arr1.max()
695/203: arr1.argmax()
695/204:
arr = [1,2,3,4,5,6,7,8,9,10]
arr[0:5]=100
arr
695/205:
arr = np.arange(1,10)
arr[0:5]=100
arr
695/206: arrcond>5
695/207: arr >5
695/208:
arrcond[arr >5]
arrcond
695/209:
arrcond = arr[arr >5]
arrcond
695/210: my_new_data = np.arange(25)
695/211: my_new_data
695/212: my_new_data94,4
695/213: my_new_data(4,4)
695/214: import pandas as pd
695/215:
labels = ['a','b','c','d','e']
my_data = [10,20,30,40,50]
arr = np.array(my_data)
d = {'key': 10,'key2':20,'key2':30,'key3':40}
696/1:
import csv
import os
696/2: user = input("which movie are you lookinf for?")
696/3: user = input("which movie are you looking for?")
696/4:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
696/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        print(row)
696/6:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        
        print(rating)
696/7:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        print(row)
696/8:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
696/9:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
696/10:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
696/11:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
696/12:
# Set path for file
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)

# Set variable to check if we found the video
found = False
696/13:
# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] +
                  " with a rating of " + row[5])

            # Set variable to confirm we have found the video
            found = True

    # If the video is never found, alert the user
    if found == False:
        print("We don't seem to have what you are looking for!")
696/14:
# Modules
import os
import csv
696/15:
# Prompt user for video lookup
video = input("What show or movie are you looking for? ")
696/16:
# Set path for file
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)

# Set variable to check if we found the video
found = False
696/17:
# Open the CSV
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # Loop through looking for the video
    for row in csvreader:
        if row[0] == video:
            print(row[0] + " is rated " + row[1] +
                  " with a rating of " + row[5])

            # Set variable to confirm we have found the video
            found = True

    # If the video is never found, alert the user
    if found == False:
        print("We don't seem to have what you are looking for!")
696/18:
import csv
import os
696/19: user = input("which movie are you looking for?")
696/20:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
696/21:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
697/1:
import csv
import os
697/2: user = input("which movie are you looking for?")
697/3:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/4:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
697/5:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
697/6:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==video:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    if found == False:
        print("Not found")
697/7:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    if found == False:
        print("Not found")
697/8:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    if found == False:
        print("Not found")
697/9:
import csv
import os
697/10: user_movie_name = input("which movie are you looking for?")
697/11:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/12:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    if found == False:
        print("Not found")
697/13:
import csv
import os
697/14: user_movie_name = input("which movie are you looking for?")
697/15:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/16:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    if found == False:
        print("Not found")
697/17:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
        else:
            print("Not found")
697/18:
import csv
import os
697/19: user_movie_name = input("which movie are you looking for?")
697/20: user_movie_name = input("which movie are you looking for?")
697/21:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/22:
import csv
import os
697/23: user_movie_name = input("which movie are you looking for?")
697/24:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/25:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
        else:
            print("Not found")
697/26:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    else:
            print("Not found")
697/27:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    else:
        print("Not found")
697/28:
import csv
import os
697/29: user_movie_name = input("which movie are you looking for?")
697/30:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/31:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
    else:
        print("Not found")
697/32:
import csv
import os
697/33: user_movie_name = input("which movie are you looking for?")
697/34:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/35:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
697/36:
import csv
import os
697/37: user_movie_name = input("which movie are you looking for?")
697/38:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/39:
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
        else:
            print("Not Found")
697/40:
movie = "not found"
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5]
            movie = "found"
697/41:
import csv
import os
697/42: user_movie_name = input("which movie are you looking for?")
697/43:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/44:
movie = "not found"
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5]
            movie = "found"
697/45:
import csv
import os
697/46: user_movie_name = input("which movie are you looking for?")
697/47:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/48:
moviefound = False
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5]
            moviefound = True
697/49:
import csv
import os
697/50: user_movie_name = input("which movie are you looking for?")
697/51:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/52:
moviefound = False
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
            moviefound = True
697/53:
import csv
import os
697/54: user_movie_name = input("which movie are you looking for?")
697/55:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/56:
moviefound = False
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
            moviefound = True
            break
697/57:
import csv
import os
697/58: user_movie_name = input("which movie are you looking for?")
697/59:
csvpath = os.path.join("Resources", "netflix_ratings.csv")
print(csvpath)
697/60:
moviefound = False
with open(csvpath) as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',')
    for row in csvreader:
        if row[0]==user_movie_name:
            print("Movie Found")
            print(row[0] + 'is rated' + row[1] + 'with a rating of' + row[5])
            moviefound = True
            break
    if moviefound == False:
        print("OOPS, MOVIE NOT FOUND!!")
697/61: import pandas as pd
697/62:
pd.series("UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University")
type(pd.series)
697/63:
sample = pd.series("UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University")
sample
697/64: import pandas as pd
697/65:
sample = pd.series("UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University")
sample
697/66:
sample = pd.series("UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University")
type(sample)
697/67: import pandas as pd
697/68:
sample = pd.series("UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University")
type(sample)
697/69:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
type(sample)
697/70: import pandas as pd
697/71:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
type(sample)
699/1: import pandas as pd
699/2:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
type(sample)
699/3: import pandas as pd
699/4:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
type(sample)
699/5:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
sample
699/6: import pandas as pd
699/7:
sample = pd.series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
sample
699/8:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
sample
699/9:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
sample
699/10:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
699/11: import pandas as pd
699/12:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
699/13:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
sample
699/14:
sample = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
type(sample)
699/15:
sample1 = pd.DataFrame{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5}
sample1
699/16:
sample1 = pd.DataFrame({"key1":1,"key2":2,"key3":3,"key4":4,"key5":5})
sample1
699/17:
sample1 = pd.DataFrame[{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5}]
sample1
699/18:
sample1 = pd.DataFrame({"key1":1,"key2":2,"key3":3,"key4":4,"key5":5})
sample1
699/19:
sample1 = pd.DataFrame([{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5}])
sample1
699/20:
sample1 = pd.DataFrame([{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},
                       {"key1":1,"key2":2,"key3":3,"key4":4,"key5":5}])
sample1
699/21:
sample1 = pd.DataFrame([{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},
                       {"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5},{"key1":1,"key2":2,"key3":3,"key4":4,"key5":5}])
sample1
699/22:
sample2 = pd.DataFrame({"name":["a","b","c","d"], "subject":["Math","Science","Physics","Chemistry"]})
sample2
699/23:
frame_shop = pd.DataFrame({"Frame":["Ornate", "Classical","Modern","Wood","Cardboard"], "Price":[15,12.5,10,5,1],"Sales":[100,200,150,300,"N/A"]})
frame_shop
699/24:
art_gallery = pd.DataFrame({"painting":"monalisa","price":10000,"popularity":"famous"},
                          {"painting":"Love","price":1000,"popularity":"notsofamous"},
                          {"painting":"friendship","price":15000,"popularity":"famous"})
art_gallery
699/25:
art_gallery = pd.DataFrame([{"painting":"monalisa","price":10000,"popularity":"famous"},
                          {"painting":"Love","price":1000,"popularity":"notsofamous"},
                          {"painting":"friendship","price":15000,"popularity":"famous"}])
art_gallery
701/1:
# Dependencies
import pandas as pd
701/2: data_file = "Resources/dataSet.csv"
701/3:
# Dependencies
import pandas as pd
701/4: data_file = "Resources/dataSet.csv"
701/5: data_file
701/6:
data_file = pd.read_csv(data_file)
data_file
701/7:
data_file = pd.read_csv(data_file)
data_file.head()
701/8:
data_file = pd.read_csv(data_file)
data_file
701/9:
data_file_df = pd.read_csv(data_file)
data_file_df
701/10:
# Dependencies
import pandas as pd
701/11: data_file = "Resources/dataSet.csv"
701/12: data_file
701/13:
data_file_df = pd.read_csv(data_file)
data_file_df
701/14:
data_file_df = pd.read_csv(data_file)
data_file_df.head()
701/15: data_file_df.decsribe()
701/16: data_file_df.describe()
701/17: data_file_df["Amount"]
701/18: data_file_df["Amount"].head()
701/19: type(data_file_df["Amount"])
701/20: type(data_file_df.describe())
701/21: data_file_df["Amount"]["Gender"]
701/22:
data_file_df["Amount"]["Gender"]
data_file_df
701/23:
data_file_df["Amount","Gender"]
data_file_df
701/24:
data_file_df[["Amount","Gender"]]
data_file_df
701/25:
data_file_df[["Amount","Gender"]]
data_file_df.head()
701/26: data_file_df[["Amount","Gender"]]
701/27: type(data_file_df[["Amount","Gender"]])
701/28: data_file_df["Amount"].mean()
701/29: data_file_df["Amount"].sum()
701/30: data_file_df["Amount"].unique()
701/31: data_file_df["Gender"].unique()
701/32: data_file_df["Last Name"].unique()
701/33: data_file_df["First Name"].unique()
701/34: data_file_df["Last Name"].value_counts()
701/35: data_file_df["First Name"].value_counts()
701/36: data_file_df["Amount"]/100
701/37:
perc = data_file_df["Amount"]/100
data_file_df["perc"]
701/38:
perc = data_file_df["Amount"]/100
data_file_df["perc"] = perc
701/39:
perc = data_file_df["Amount"]/100
data_file_df["perc"] = perc
data_file_df
701/40:
perc = data_file_df["Amount"]/100
data_file_df["perc"] = perc
data_file_df.head()
702/1:
# Import Dependencies
import pandas as pd
import random
702/2:
# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_data = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership(Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_data.head()
702/3:
# Collecting a summary of all numeric data
training_data.describe()
702/4:
# Finding the names of the trainers
training_data["Trainer"].unique()
702/5:
# Finding how many students each trainer has
training_data["Trainer"].value_counts()
702/6:
# Converting the membership days into weeks and then adding a column to the DataFrame
weeks = training_data["Membership(Days)"]/7
training_data["Weeks"] = weeks
training_data.head()
702/7: training_data.columns()
702/8: training_data.columns
702/9: training_data.columns
702/10:
new = training_data[["Weeks","Weight","Name","Trainer","Membership(Days)"]]
new
702/11:
new = training_data[["Weeks","Weight","Name","Trainer","Membership(Days)"]]
new.head()
702/12:
# Collecting a summary of all numeric data
training_data.describe()
702/13:
new_name = new.rename(columns={"Membership(Days)":"DailyMembership","Weight":"Weight in Pounds"})
new_name
704/1:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character","color-of_hair":"Hair Color",
                                           "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/2:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character","color_of_hair":"Hair Color",
                                           "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/3:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character","color_of_hair":"Hair Color","Height"="Height"
                                           "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/4:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character","color_of_hair":"Hair Color","Height"="Height",
                                           "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/5:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character",
                                            "color_of_hair":"Hair Color",
                                            "Height"="Height",
                                            "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/6:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character",
                                            "color_of_hair":"Hair Color",
                                            "Height":"Height",
                                            "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/7:
# import dependencies
import pandas as pd
704/8:
# Create a data frame with given columns and value
hey_arnold = pd.DataFrame(
    {"Character_in_show": ["Arnold", "Gerald", "Helga", "Phoebe", "Harold", "Eugene"],
     "color_of_hair": ["blonde", "black", "blonde", "black", "unknown", "red"],
     "Height": ["average", "tallish", "tallish", "short", "tall", "short"],
     "Football_Shaped_Head": [True, False, False, False, False, False]
     })

hey_arnold
704/9:
# Rename columns to clean up the look
hey_arnold_new = hey_arnold.rename(columns={"Character_in_show":"Character",
                                            "color_of_hair":"Hair Color",
                                            "Height":"Height",
                                            "Football_Shaped_Head":"Football Head"})
hey_arnold_new
704/10:
# Organize columns into a more logical order
hey_arnold_new[["Character","Football Head","Hair Color","Height"]]
704/11: file_one = "Resources/DataOne.csv"
704/12:
file_d = pd.read(file_one)
file_d
704/13:
file_d = pd.read_csv(file_one)
file_d
   1: %history -g -f Project_1_DataClean2
